{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJWDCdw-DXK-"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/SachsLab/MonkeyPFCSaccadeStudies/blob/master/StudyLocationRule/Analysis/04_analyze_target_CNN.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/SachsLab/IMonkeyPFCSaccadeStudies/blob/master/StudyLocationRule/Analysis/04_analyze_target_CNN.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lnt_1_ellnIY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Decode Intended Saccade Direction from Macaque PFC Microelectrode Recordings With Convolutional Neural Networks - Using Spike Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Environment Setup\n",
    "Run the cells in this section to configure the local or Google Colab environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "id": "yGrxqTomlnIc",
    "outputId": "2137da9c-943e-4878-f2e9-c07e983a35ee",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    %tensorflow_version 2.x  # Only on colab\n",
    "    os.chdir('..')\n",
    "    \n",
    "    if not (Path.home() / '.kaggle').is_dir():\n",
    "        # Configure kaggle\n",
    "        uploaded = files.upload()  # Find the kaggle.json file in your ~/.kaggle directory.\n",
    "        if 'kaggle.json' in uploaded.keys():\n",
    "            !mkdir -p ~/.kaggle\n",
    "            !mv kaggle.json ~/.kaggle/\n",
    "            !chmod 600 ~/.kaggle/kaggle.json\n",
    "    \n",
    "    !pip install git+https://github.com/SachsLab/indl.git\n",
    "    \n",
    "    if Path.cwd().stem == 'MonkeyPFCSaccadeStudies':\n",
    "        os.chdir(Path.cwd().parent)\n",
    "\n",
    "    if not (Path.cwd() / 'MonkeyPFCSaccadeStudies').is_dir():\n",
    "        !git clone --single-branch --recursive https://github.com/SachsLab/MonkeyPFCSaccadeStudies.git\n",
    "        sys.path.append(str(Path.cwd() / 'MonkeyPFCSaccadeStudies'))\n",
    "    os.chdir('MonkeyPFCSaccadeStudies')\n",
    "    \n",
    "    !pip install -q kaggle\n",
    "    !pip install -U scikit-learn\n",
    "    IN_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    \n",
    "    # chdir to MonkeyPFCSaccadeStudies\n",
    "    if Path.cwd().stem == 'Analysis':\n",
    "        os.chdir(Path.cwd().parent.parent)\n",
    "    \n",
    "    # Add indl repository to path.\n",
    "    # Eventually this should already be pip installed, but it's still under heavy development so this is easier for now.\n",
    "    check_dir = Path.cwd()\n",
    "    while not (check_dir / 'Tools').is_dir():\n",
    "        check_dir = check_dir / '..'\n",
    "    indl_path = check_dir / 'Tools' / 'Neurophys' / 'indl'\n",
    "    sys.path.append(str(indl_path))\n",
    "    \n",
    "    # Make sure the kaggle executable is on the PATH\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ';' + str(Path(sys.executable).parent / 'Scripts')\n",
    "    \n",
    "    IN_COLAB = False\n",
    "\n",
    "# Try to clear any logs from previous runs\n",
    "if (Path.cwd() / 'logs').is_dir():\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(str(Path.cwd() / 'logs'))\n",
    "    except PermissionError:\n",
    "        print(\"Unable to remove logs directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from indl.display import turbo_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Configure plotting defaults.\n",
    "if IN_COLAB:\n",
    "    plt.style.use('dark_background')\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'lines.linewidth': 2,\n",
    "    'lines.markersize': 5,\n",
    "    'xtick.labelsize': 16,\n",
    "    'ytick.labelsize': 16,\n",
    "    'legend.fontsize': 18,\n",
    "    'figure.figsize': (6.4, 6.4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download data if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ltlcRrJLlnIh",
    "outputId": "50951495-4454-41e1-c452-3217b95f5674",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    data_path = Path.cwd() / 'data' / 'monkey_pfc' / 'converted'\n",
    "else:\n",
    "    data_path = Path.cwd() / 'StudyLocationRule' / 'Data' / 'Preprocessed'\n",
    "\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare to load data\n",
    "\n",
    "We will use a custom function load_macaque_pfc to load the data into memory.\n",
    "\n",
    "There are 4 different strings to be passed to the import x_chunk argument:\n",
    "* 'analogsignals' - if present. Returns 1 kHz LFPs\n",
    "* 'gaze' - Returns 2-channel gaze data.\n",
    "* 'spikerates' - Returns smoothed spikerates\n",
    "* 'spiketrains'\n",
    "\n",
    "The y_type argument can be\n",
    "* 'pair and choice' - returns Y as np.array of (target_pair, choice_within_pair)\n",
    "* 'encoded input' - returns Y as np.array of shape (n_samples, 10) (explained below)\n",
    "* 'replace with column name' - returns Y as a vector of per-trial values. e.g., 'sacClass'\n",
    "\n",
    "The actual data we load depends on the particular analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from misc.misc import load_macaque_pfc, sess_infos\n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': True,\n",
    "    'y_type': 'sacClass',  # Integer 0:7 for 8 different targets.\n",
    "    'samples_last': True  # Our EEGNet blocks expect time-samples in the last dimension.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Decoding trial class (0:7) from spike rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Load the per-trial spikerates and saccadeClass (0:7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_sess_ix = 1\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "print(f\"\\nImporting session {sess_id}\")\n",
    "X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "Y_class = tf.keras.utils.to_categorical(Y_class, num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MkP-59rTyK42",
    "outputId": "d49772ea-d2b5-4a65-a20b-ae41882ed584"
   },
   "source": [
    "### Create Model\n",
    "\n",
    "Our model objective is to transform timeseries segments into probabilities of each class.\n",
    "\n",
    "Let's start with a function to make the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from indl.model import parts\n",
    "from indl.model.helper import check_inputs\n",
    "from indl.regularizers import KernelLengthRegularizer\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "@check_inputs\n",
    "def make_model(\n",
    "        _input,\n",
    "        F1=8, kernLength=25, F1_kern_reg=None,\n",
    "        D=2, D_pooling=4,\n",
    "        F2=8, F2_kernLength=16,\n",
    "        F2_pooling=8,\n",
    "        dropoutRate=0.25,\n",
    "        activation='elu',\n",
    "        l1_reg=0.000, l2_reg=0.000,\n",
    "        norm_rate=0.25,\n",
    "        latent_dim=16,\n",
    "        return_model=True\n",
    "    ):\n",
    "    \n",
    "    if F1_kern_reg is None:\n",
    "        F1_kern_reg = tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg)\n",
    "    elif isinstance(F1_kern_reg, str) and F1_kern_reg == 'kern_length_regu':\n",
    "        F1_kern_reg = KernelLengthRegularizer((1, kernLength),\n",
    "                                              window_scale=1e-4,\n",
    "                                              window_func='poly',\n",
    "                                              poly_exp=2,\n",
    "                                              threshold=0.0015)\n",
    "        \n",
    "    # EEGNetEnc \n",
    "    _y = parts.EEGNetEnc(_input,\n",
    "                         F1=F1,\n",
    "                         F1_kernLength=kernLength,\n",
    "                         F1_kern_reg=F1_kern_reg,\n",
    "                         D=D,\n",
    "                         D_pooling=D_pooling,\n",
    "                         F2=F2,\n",
    "                         F2_pooling=F2_pooling,\n",
    "                         F2_kernLength=F2_kernLength,\n",
    "                         dropoutRate=dropoutRate)\n",
    "    \n",
    "    # Restore time-dimension that was stripped out by EEGNetEnc\n",
    "    _y = layers.Reshape((1, _input.shape.as_list()[2] // D_pooling // F2_pooling, F2))(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    _y = parts.Classify(_y, n_classes=8, norm_rate=norm_rate)\n",
    "    \n",
    "    if return_model:\n",
    "        return tf.keras.models.Model(inputs=[_input], outputs=[_y])\n",
    "    else:\n",
    "        return _y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Parameterize the model and preview the model.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indl.data.augmentations import random_slice\n",
    "from functools import partial\n",
    "\n",
    "MAX_OFFSET = 0\n",
    "\n",
    "p_random_slice_trn = partial(random_slice, max_offset=MAX_OFFSET)\n",
    "p_random_slice_vld = partial(random_slice, training=False, max_offset=MAX_OFFSET)\n",
    "\n",
    "def rates2dataset(X, y, is_train=True, batch_size=16):\n",
    "    temp_ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    \n",
    "    # cast data types to GPU-friendly types.\n",
    "    temp_ds = temp_ds.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "    \n",
    "    # augmentations (random slicing)\n",
    "    if MAX_OFFSET > 0:\n",
    "        temp_ds = temp_ds.map(p_random_slice_trn if is_train else p_random_slice_vld)\n",
    "    \n",
    "    if is_train:\n",
    "        temp_ds = temp_ds.shuffle(X.shape[0] + 1)\n",
    "        \n",
    "    temp_ds = temp_ds.batch(batch_size, drop_remainder=is_train)\n",
    "    \n",
    "    return temp_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "LABEL_SMOOTHING = 0.2\n",
    "model_kwargs = dict(\n",
    "    F1=8, kernLength=25, F1_kern_reg=None,\n",
    "    D=2, D_pooling=4,\n",
    "    F2=8, F2_kernLength=16,\n",
    "    F2_pooling=8,\n",
    "    dropoutRate=0.30,\n",
    "    activation='relu',\n",
    "    l1_reg=0.000, l2_reg=0.003,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=16\n",
    ")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "randseed = 12345\n",
    "random.seed(randseed)\n",
    "np.random.seed(randseed)\n",
    "tf.random.set_seed(randseed)\n",
    "\n",
    "temp_ds = rates2dataset(X_rates, Y_class)\n",
    "\n",
    "model = make_model(\n",
    "    temp_ds.element_spec[0],\n",
    "    **model_kwargs\n",
    ")\n",
    "optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "model.summary()\n",
    "if False:\n",
    "    tf.keras.utils.plot_model(\n",
    "        model,\n",
    "        to_file='model.png',\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        rankdir='TB',\n",
    "        expand_nested=False,\n",
    "        dpi=96\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 180\n",
    "\n",
    "\n",
    "def get_hists_acc(sess_id, verbose=1):\n",
    "    print(f\"Processing session {sess_id}...\")\n",
    "    X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "    \n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "        \n",
    "        ds_train = rates2dataset(X_rates[trn], _y[trn], is_train=True, batch_size=BATCH_SIZE)\n",
    "        ds_valid = rates2dataset(X_rates[vld], _y[vld], is_train=False, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        randseed = 12345\n",
    "        random.seed(randseed)\n",
    "        np.random.seed(randseed)\n",
    "        tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model(\n",
    "            ds_train.element_spec[0],\n",
    "            **model_kwargs\n",
    "        )\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'rates2class_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld, :, MAX_OFFSET:]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"Session {sess_id} overall accuracy: {accuracy}%\")\n",
    "    \n",
    "    return history, accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test training on one session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from indl.metrics import quickplot_history\n",
    "\n",
    "history, accuracy = get_hists_acc(sess_infos[1]['exp_code'], verbose=2)\n",
    "quickplot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation - per session\n",
    "\n",
    "Note that using verbose=0 will suppress most of the log information. Progress indicators might only be updated once every few minutes, depending on the number of epochs and the amount of time it takes to process a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = []\n",
    "accs = []\n",
    "for sess_info in sess_infos:\n",
    "    _hist, _acc = get_hists_acc(sess_info['exp_code'], verbose=0)\n",
    "    hists.append(_hist)\n",
    "    accs.append(_acc)\n",
    "    \n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ku_MXiETyK4l",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TODO\n",
    "   \n",
    "1. Use Capsules\n",
    "1. Replace deeper conv layers with LSTM\n",
    "    * Didn't seem to help\n",
    "1. If using autoencoder, make sure AE on its own can reconstruct input reasonably well.\n",
    "    * Couldn't get good AE working. Needs work.\n",
    "1. Better model inspection (put code in indl)\n",
    "    * tSNE different layers\n",
    "    * Average saliency across channels vs time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKenpuR2lnJH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inspecting the model\n",
    "\n",
    "[Further info](http://cs231n.github.io/understanding-cnn/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_id = sess_infos[1]['exp_code']\n",
    "X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "_y = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "ds_all = rates2dataset(X_rates, _y, is_train=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFKTmTpHnSA4",
    "outputId": "23bd1b5f-291b-485d-f827-5fc58294cda6"
   },
   "outputs": [],
   "source": [
    "# Train a model using ALL data\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "randseed = 12345\n",
    "random.seed(randseed)\n",
    "np.random.seed(randseed)\n",
    "tf.random.set_seed(randseed)\n",
    "\n",
    "model = make_model(\n",
    "    ds_all.element_spec[0],\n",
    "    **model_kwargs\n",
    ")\n",
    "optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy', 'AUC'])\n",
    "\n",
    "best_model_path = f'rates2class_{sess_id}_all.h5'\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=best_model_path,\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='accuracy',\n",
    "        verbose=2)\n",
    "]\n",
    "hist = model.fit(x=ds_all, epochs=EPOCHS,\n",
    "                 callbacks=callbacks,\n",
    "                 verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZfkSfGxtDXMP"
   },
   "source": [
    "### t-Distributed Stochastic Neighbour Embedding (t-SNE)\n",
    "\n",
    "https://distill.pub/2016/misread-tsne/\n",
    "\n",
    "From [sklearn.manifold.TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html):\n",
    "\n",
    ">t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
    "\n",
    "We will compare the t-SNE projections of the outputs to the projections of the inputs.\n",
    "However, the raw spiketrains do not decompose to very well so for inputs we will use spikerates that are previously derived from the spike trains convolved with a gaussian kernel (sigma=50 msec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5v0xgvIjDXMU"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a colour code cycler e.g. 'C0', 'C1', etc.\n",
    "from itertools import cycle\n",
    "colour_codes = map('C{}'.format, cycle(range(10)))\n",
    "class_colors = np.array([next(colour_codes) for _ in range(10)])\n",
    "# class_colors = np.array(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w'])\n",
    "tbs = 30  # tsne batch size\n",
    "\n",
    "TEST_PERPLEXITY = 20  # 10, 30\n",
    "\n",
    "def plot_tsne(x_vals, y_vals, perplexity, title='Model Output'):\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(x=x_vals[:, 0], y=x_vals[:, 1], color=class_colors[y_vals])\n",
    "    ax.set_xlabel('t-SNE D-1')\n",
    "    ax.set_ylabel('t-SNE D-2')\n",
    "    if perplexity is not None:\n",
    "        title += f\" (Ppx: {perplexity})\"\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3oZCmrZuyK5E",
    "outputId": "3c64a40b-ec80-45f9-e2cd-e528fbf94eee"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "# First plot a t-SNE on the input data. Precede TSNE with a PCA.\n",
    "pca = PCA(n_components=50)\n",
    "pca_values = pca.fit_transform(X_rates.reshape([-1, np.prod(X_rates.shape[1:])]))\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY)\n",
    "tsne_values = tsne_model.fit_transform(pca_values)\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_tsne(tsne_values, Y_class.ravel()+1, TEST_PERPLEXITY, title='Input Rates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a version of our CNN model that goes from input to the bottleneck layer\n",
    "model = tf.keras.models.load_model(best_model_path)\n",
    "test_layer_idx = [3, 15, -1]\n",
    "test_layers = [model.layers[_].output for _ in test_layer_idx]\n",
    "truncated_model = tf.keras.Model(model.input, test_layers)\n",
    "outputs = [[] for _ in range(len(test_layers))]\n",
    "for start_ix in range(0, X_rates.shape[0], tbs):\n",
    "    temp = X_rates[start_ix:start_ix+tbs].astype(np.float32)\n",
    "    temp = truncated_model(temp)\n",
    "    for layer_ix, res in enumerate(temp):\n",
    "        outputs[layer_ix].append(res.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputs[0]), outputs[0][0].shape, X_rates.shape)\n",
    "results = []\n",
    "for ix, tmp in enumerate(outputs):\n",
    "    _tmp = np.concatenate(tmp, axis=0)\n",
    "    print(_tmp.shape)\n",
    "    results.append(_tmp.reshape([-1, np.prod(_tmp.shape[1:])]))\n",
    "    \n",
    "print([_.shape for _ in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "layer_titles = [\"First Conv.\", \"Final Conv.\", \"Output\"]\n",
    "\n",
    "for layer_ix, flat_res in enumerate(results):\n",
    "    if layer_ix == 0:\n",
    "        pca = PCA(n_components=50)\n",
    "        flat_res = pca.fit_transform(flat_res)\n",
    "    tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY)\n",
    "    tsne_values = tsne_model.fit_transform(flat_res)\n",
    "    ax = plt.subplot(1, 3, layer_ix + 1)\n",
    "    plot_tsne(tsne_values, Y_class.ravel()+1, None,\n",
    "              title=layer_titles[layer_ix])  # f\"Layer {test_layer_idx[layer_ix]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig('tSNE_CNN_rates.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Rates\n",
    "pca = PCA(n_components=50)\n",
    "pca_values = pca.fit_transform(results[0])\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY)\n",
    "tsne_values = tsne_model.fit_transform(pca_values)\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_tsne(tsne_values, Y.ravel()+1, TEST_PERPLEXITY, title='Model Features')\n",
    "\n",
    "# Model Latents\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY)\n",
    "tsne_values = tsne_model.fit_transform(flattened_latents)\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_tsne(tsne_values, Y.ravel()+1, TEST_PERPLEXITY, title='Model Latents')\n",
    "\n",
    "# Recon Rates\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY)\n",
    "tsne_values = tsne_model.fit_transform(flattened_recon_rates)\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_tsne(tsne_values, Y.ravel()+1, TEST_PERPLEXITY, title='Recon Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(str(datadir / (SESS_ID + '_CNN_tSNE.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDmmBuQhDXMW"
   },
   "source": [
    "t-SNE on the untransformed data shows two different clusters for blue/magenta trial pairs.\n",
    "These probably came at two different blocks of time, between which there was a change in the neural activations.\n",
    "After transforming the data, these classes are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rR6yFVabyK5G",
    "outputId": "881fc520-fcfe-415c-f0e3-f378606ea919"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "chan_ix = 31  # 4\n",
    "_X = rates_X\n",
    "for class_ix in range(8):\n",
    "    b_class = rates_Y[:, 0] == class_ix\n",
    "    mean_rates = np.squeeze(np.mean(_X[b_class], axis=0))\n",
    "    plt.plot(mean_rates[:, chan_ix], color=class_colors[class_ix])\n",
    "plt.title('Smoothed Rates')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "chan_ix = -4  # 4\n",
    "_X = rates\n",
    "for class_ix in range(8):\n",
    "    b_class = rates_Y[:, 0] == class_ix\n",
    "    mean_rates = np.squeeze(np.mean(_X[b_class], axis=0))\n",
    "    plt.plot(mean_rates[:, chan_ix], color=class_colors[class_ix])\n",
    "plt.title('Convolved Spikes')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "chan_ix = -4  # 4\n",
    "_X = recon_rates\n",
    "for class_ix in range(8):\n",
    "    b_class = rates_Y[:, 0] == class_ix\n",
    "    mean_rates = np.squeeze(np.mean(_X[b_class], axis=0))\n",
    "    plt.plot(mean_rates[:, chan_ix], color=class_colors[class_ix])\n",
    "plt.title('Recon Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(str(datadir / (SESS_ID + '_example_rates.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNFyDPtJyK5J",
    "outputId": "1198d814-bf4a-4c47-81c6-83cb4b91d49f"
   },
   "outputs": [],
   "source": [
    "import tensortools as tt\n",
    "\n",
    "U = tt.cp_als(np.squeeze(rates_X), rank=3, verbose=True)\n",
    "fig, ax, po = tt.plot_factors(U.factors, plots=['scatter', 'line', 'bar'], figsize=(9, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sPdNZyvhyK5L",
    "outputId": "4a614428-c061-42f2-8c88-9c9187ebfc77"
   },
   "outputs": [],
   "source": [
    "U = tt.cp_als(np.squeeze(rates), rank=3, verbose=True)\n",
    "fig, ax, po = tt.plot_factors(U.factors, plots=['scatter', 'line', 'bar'], figsize=(9, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9phPPaaDXMW"
   },
   "source": [
    "### First convolutional layers\n",
    "The first pair of convolutional layers are simply performing time-domain convolutions on the spike trains.\n",
    "Whereas a typically signal processing pipeline will apply a gaussian, exponentional, or gamma kernel convolution,\n",
    "here we train the convolution kernels directly. There are separate \"short\" kernels and \"long\" kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "L3N1Wr6TlnJM",
    "outputId": "aa508731-0c5d-411c-9ae2-7dcea4f6f2c3"
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "t = ax_info['timestamps']\n",
    "\n",
    "x_ranges = [[-0.02, 0.02], [-0.2, 0.2]]\n",
    "y_steps = [1.0, 0.5]\n",
    "\n",
    "impulse = np.zeros_like(t)\n",
    "impulse[np.argmin(np.abs(t))] = 1.0\n",
    "\n",
    "step = np.zeros_like(t)\n",
    "step[np.argmin(np.abs(t)):] = 1.0\n",
    "\n",
    "for s_l in range(2):\n",
    "    filters = np.squeeze(model.layers[1 + s_l].get_weights()[0])\n",
    "\n",
    "    # Impulse response\n",
    "    plt.subplot(2, 3, 1 + 3*s_l)\n",
    "    for filt_ix, filt_coeff in enumerate(filters.T):\n",
    "        imp_conv = scipy.signal.convolve(impulse, filt_coeff, 'same')\n",
    "        plt.plot(t, imp_conv - y_steps[s_l]*filt_ix)\n",
    "    plt.xlim(x_ranges[s_l])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.title('Impulse Response')\n",
    "\n",
    "    # Step response\n",
    "    plt.subplot(2, 3, 2 + 3*s_l)\n",
    "    for filt_ix, filt_coeff in enumerate(filters.T):\n",
    "        step_response = scipy.signal.convolve(step, filt_coeff, 'same')\n",
    "        plt.plot(t, step_response - y_steps[s_l]*filt_ix)\n",
    "    plt.xlim(x_ranges[s_l])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.title('Step Response')\n",
    "\n",
    "    plt.subplot(2, 3, 3 + 3*s_l)\n",
    "    for filt_ix, filt_coeff in enumerate(filters.T):\n",
    "        f, resp = scipy.signal.freqz(filt_coeff, worN=int(ax_info['fs']), fs=ax_info['fs'])\n",
    "        plt.plot(f, np.abs(resp) - filt_ix)\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.title('Frequency Response')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJZMvmq6nHvy"
   },
   "source": [
    "### Spatial filter\n",
    "The second convolutional layer in our model is a set of spatial filters. We can visualize the weights that transform the 32-channel inputs to D*n_temporal_filter features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "EiDxz6VAlnJH",
    "outputId": "07f99cdc-c8a0-49a9-ee51-0509ffa00415"
   },
   "outputs": [],
   "source": [
    "LAYER_IX = 4\n",
    "spatial_filter = np.squeeze(model.layers[LAYER_IX].get_weights()[0])\n",
    "D = spatial_filter.shape[-1]\n",
    "sp_cols = int(np.ceil(np.sqrt(D + 2)))\n",
    "sp_rows = int(np.ceil((D + 2) / sp_cols))\n",
    "vmax=abs(spatial_filter).max()\n",
    "vmin=-abs(spatial_filter).max()\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "for depth_ix in range(D):\n",
    "    plt.subplot(sp_rows, sp_cols, depth_ix + 1)\n",
    "    plt.imshow(spatial_filter[:, :, depth_ix], vmax=vmax, vmin=vmin, cmap=turbo_cmap)\n",
    "    plt.title('Spatial Filter Set {}'.format(depth_ix))\n",
    "    plt.xlabel('Temporal Filter')\n",
    "    plt.ylabel('Input Channel')\n",
    "# plt.colorbar()\n",
    "\n",
    "sum_abs_weight = np.sum(np.sum(np.abs(spatial_filter), axis=1), axis=-1)\n",
    "plt.subplot(sp_rows, sp_cols, D + 1)\n",
    "plt.hist(sum_abs_weight, 20)\n",
    "plt.xlabel('Sum Abs Weight')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Chan Sum Abs. Weight')\n",
    "\n",
    "plt.subplot(sp_rows, sp_cols, D + 2)\n",
    "plt.bar(np.arange(spatial_filter.shape[0]), sum_abs_weight)\n",
    "plt.xlabel('Channel ID')\n",
    "plt.ylabel('Sum Abs Weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(str(datadir / (SESS_ID + '_CNN_SpatFilts.png')))\n",
    "\n",
    "ch_ids = np.argsort(sum_abs_weight)[::-1]  # channel_ids sorted by weight, descending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZPM-zZqlnJJ"
   },
   "source": [
    "There seems to be a small group of channels with large weights, another group with intermediate weights, and finally the rest of the channels with low weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf6zecPslnJS"
   },
   "source": [
    "## Filter Activation-Maximizing Inputs\n",
    "\n",
    "One useful way to understand what a convolutional layer is doing, especially for deeper layers that are combining abstract features, is to visualize an input that would maximize activation of a filter(s) within the layer.\n",
    "\n",
    "Remembering back to the step-by-step neural net in 02_02, we found the _weights_ that _minimized_ a loss function for a given set of _inputs_. Now we know the weights but we want to find the inputs that _maximize_ the activation (a.k.a. output) of a filter. We're going to use the same loss-minimization training framework, but instead of calculating a 'loss', we will calculate the mean of the output of the layer and filter of interest.\n",
    "\n",
    "We start with a random input and call the model on the input while recording with GradientTape. Then, instead of using our gradients to 'optimize loss' (i.e., step the weights down the gradients), we use our gradients to maximize output (i.e., step the input up the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ht1hcxB3bGU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def plot_layer(layer_ix, max_filts=None, n_steps=100):\n",
    "    in_shape = [1] + model.input.shape.as_list()[1:]\n",
    "    \n",
    "    layer_output = model.layers[layer_ix].output\n",
    "    n_filts = layer_output.shape[-1]\n",
    "    filt_ids = np.arange(n_filts)\n",
    "    if (max_filts is not None) and (len(filt_ids) > max_filts):\n",
    "        filt_ids = filt_ids[np.argsort(np.random.rand(n_filts))][:max_filts]\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 12), facecolor='white')\n",
    "    sp_cols = int(np.ceil(np.sqrt(len(filt_ids))))\n",
    "    sp_rows = int(np.ceil(len(filt_ids) / sp_cols))\n",
    "    \n",
    "    filt_slice = [np.s_[:] for _ in range(K.ndim(layer_output))]\n",
    "    \n",
    "    for ix, filt_ix in enumerate(filt_ids):\n",
    "        input_data = tf.convert_to_tensor(np.random.randn(*in_shape).astype(np.float32))\n",
    "        if layer_ix > (len(model.layers) - 3):\n",
    "            # model.layers[layer_ix].activation == tf.keras.activations.softmax:\n",
    "            max_model = tf.keras.Model(model.input, layer_output)\n",
    "            non_targ_id = tf.constant(np.setdiff1d(np.arange(layer_output.shape[-1], dtype=int), filt_ix))\n",
    "            for step_ix in range(n_steps):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(input_data)\n",
    "                    filter_act = max_model(input_data)\n",
    "                    targ_act = filter_act[0, filt_ix]\n",
    "                    nontarg_act = K.mean(tf.gather(filter_act, non_targ_id, axis=-1))\n",
    "                    loss_value = targ_act - nontarg_act\n",
    "                grads = tape.gradient(loss_value, input_data)  # Derivative of loss w.r.t. input\n",
    "                # Normalize gradients\n",
    "                grads /= (K.sqrt(K.mean(K.square(grads))) + K.epsilon())\n",
    "                input_data += grads\n",
    "        else:\n",
    "            filt_slice[-1] = filt_ix\n",
    "            max_model = tf.keras.Model(model.input, layer_output[tuple(filt_slice)])\n",
    "            for step_ix in range(n_steps):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(input_data)\n",
    "                    filter_act = max_model(input_data)\n",
    "                    loss_value = K.mean(filter_act)\n",
    "                grads = tape.gradient(loss_value, input_data)  # Derivative of loss w.r.t. input\n",
    "                # Normalize gradients\n",
    "                grads /= (K.sqrt(K.mean(K.square(grads))) + K.epsilon())\n",
    "                input_data += grads\n",
    "        input_data = np.squeeze(input_data)\n",
    "\n",
    "        plt.subplot(sp_rows, sp_cols, ix + 1)\n",
    "        plt.plot(t[MAX_OFFSET:], input_data[:, ch_ids[:4]])\n",
    "        plt.xlabel('Time After Target Onset (s)')\n",
    "        plt.ylabel('Filter {}'.format(filt_ix))\n",
    "        plt.title('Output {:.2f}'.format(loss_value.numpy()))\n",
    "        for xx in [0, 0.25, 1.25]:\n",
    "            plt.axvline(xx, color='k', linestyle='--')\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhIOqfs6DXMo"
   },
   "outputs": [],
   "source": [
    "# 4 is DepthwiseConv2D, 9 is SeparableConv2D\n",
    "plot_layer(9, max_filts=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdRX_aCLDV_8"
   },
   "source": [
    "### Class Maximizing Inputs\n",
    "If we extend our reasoning from filter activations down to the next-to-last layer (15), and we choose a 'loss' that maximizes one class, we can plot maximization signals for each of the 8 output classes. If we were to do the same on the final Softmax layer (16), the results have a similar shape but are quite noisy because perfect classification is achieved quickly and thus there is no more gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oa4g1B8ZDXMs"
   },
   "outputs": [],
   "source": [
    "plot_layer(15, n_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HPwFVrYEr8wx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saliency Maps\n",
    "Saliency maps visualize how each part of a real input contributes to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_hlqlOWr4Ap"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "def get_losses_for_class(test_class):\n",
    "    classes, y = np.unique(Y, return_inverse=True)\n",
    "    trial_ids = np.where(y == classes.tolist().index(test_class))[0]\n",
    "    losses_grads = []\n",
    "    for tr_id in trial_ids:\n",
    "        input_data = tf.convert_to_tensor(X[tr_id, MAX_OFFSET:, :].astype(np.float32)[None, :, :, None])\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_data)\n",
    "            class_proba = model(input_data)\n",
    "            loss_value = K.sparse_categorical_crossentropy(y[tr_id], class_proba)\n",
    "        grads = tape.gradient(loss_value, input_data)  # Derivative of loss w.r.t. input\n",
    "        # Normalize gradients\n",
    "        grads /= (K.sqrt(K.mean(K.square(grads))) + K.epsilon())\n",
    "        losses_grads.append((loss_value, grads))\n",
    "    return losses_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtqESwBb2z4O"
   },
   "outputs": [],
   "source": [
    "# Plot saliency image for a few trials in a particular class\n",
    "N_SALIENCY_TRIALS = 3\n",
    "TEST_CLASS = 5  # -1 to 6\n",
    "losses_grads = get_losses_for_class(TEST_CLASS)\n",
    "t = ax_info['timestamps'][MAX_OFFSET:]\n",
    "t0_ix = [np.argmin(np.abs(t - _)) for _ in [0, 0.25, 1.25]]\n",
    "\n",
    "loss_vals = [_[0][0].numpy() for _ in losses_grads]\n",
    "grad_vals = np.squeeze(np.concatenate([_[1].numpy() for _ in losses_grads], axis=0))\n",
    "re_ix = np.argsort(loss_vals)\n",
    "b_class = np.squeeze(Y == TEST_CLASS)\n",
    "_x = X[b_class, MAX_OFFSET:][re_ix][:N_SALIENCY_TRIALS]\n",
    "_masks = grad_vals[re_ix][:N_SALIENCY_TRIALS]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6), facecolor='white')\n",
    "for tr_ix in range(N_SALIENCY_TRIALS):\n",
    "    plt.subplot(N_SALIENCY_TRIALS, 1, tr_ix + 1)\n",
    "    plt.imshow(_masks[tr_ix].T, aspect='auto', interpolation='none', vmin=-4, vmax=4, cmap=turbo_cmap)\n",
    "    plt.eventplot([np.where(_)[0] for _ in _x[tr_ix].T], colors='k')\n",
    "    for _t in t0_ix:\n",
    "        plt.axvline(_t)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Channel ID')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4oo8if9DXM3"
   },
   "outputs": [],
   "source": [
    "# Plot average saliencies for all trials within each class\n",
    "t = ax_info['timestamps'][MAX_OFFSET:]\n",
    "t0_ix = [np.argmin(np.abs(t - _)) for _ in [0, 0.25, 1.25]]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 18), facecolor='white')\n",
    "\n",
    "for ix, class_id in enumerate(np.unique(Y)):\n",
    "    losses_grads = get_losses_for_class(class_id)\n",
    "    loss_vals = [_[0][0].numpy() for _ in losses_grads]\n",
    "    grad_vals = np.squeeze(np.concatenate([_[1].numpy() for _ in losses_grads], axis=0))\n",
    "    grad_vals = np.mean(grad_vals, axis=0)\n",
    "    plot_ix = 2 * ix + 1 * (ix < 4) - 6 * (ix >= 4)\n",
    "    plt.subplot(4, 2, plot_ix)\n",
    "    plt.imshow(grad_vals.T, aspect='auto', interpolation='none', vmin=-4, vmax=4, cmap=turbo_cmap)\n",
    "    for _t in t0_ix:\n",
    "        plt.axvline(_t)\n",
    "    plt.title(str(class_id))\n",
    "    if (ix + 1) % 4 == 0:\n",
    "        plt.xlabel('Sample')\n",
    "    if ix < 4:\n",
    "        plt.ylabel('Channel')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDma3biLn9Q8"
   },
   "source": [
    "## Class Activation Maps\n",
    "\n",
    "Class activation maps (CAM) highlight the parts of the input that contribute most to each classification score. This is similar but different to saliency mapping. Whereas in saliency mapping the losses are back-propagated all the way back to the inputs, in CAM (or [Grad-CAM](https://arxiv.org/pdf/1610.02391.pdf)) the per-class scores / losses are propaged backward only to the last convolutional layer. These losses are then used as the weights in a weighted average of the feature map output of the last convolutional layer. If the result is smaller than the input, it is then interpolated to match the input size.\n",
    "\n",
    "Remember that in image classification the data have width pixels x height pixels x colour depths, but in our neural time-series data we have time samples x 1 x electrodes. We could use CAM on our timeseries data to identify which time points are important for each class but not channels because CAM averages across 'depth'. Time-point importance is unlikely to be informative in this dataset because it is unlikely that the timing of processing visual cues and creating motor plans is class-dependent, especially not at the time scales of the final convolution output (~100 msec).\n",
    "\n",
    "To get any information about which channels of the input were important, we would have to project the losses back to before the spatial filter layer (`DepthwiseConv2D`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQ4aT56PDXM6"
   },
   "source": [
    "TODO: Cluster channels based on cross-correlations of per-trial saliency maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPeKW5nTDXM6"
   },
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "Our model had many hyperparameters. Here we search for their optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPqWEUKRDXM7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "P_TRAIN = 0.8\n",
    "\n",
    "def evaluate_model(params, verbose=0):\n",
    "    print(params)\n",
    "    k_spike_short = params.get('k_spike_short', 10)\n",
    "    l_spike_short = params.get('l_spike_short', 22)\n",
    "    k_spike_long = params.get('k_spike_long', 4)\n",
    "    l_spike_long = params.get('l_spike_long', 200)\n",
    "    D = params.get('D', 4)\n",
    "    downsamp_1 = params.get('downsamp_1', 5)\n",
    "    n_pointwise_filters = params.get('n_pointwise_filters', 38)\n",
    "    kern_length_2 = params.get('kern_length_2', 40)\n",
    "    downsamp_2 = params.get('downsamp_2', 8)\n",
    "    norm_rate = params.get('norm_rate', 0.3)\n",
    "    dropout_rate = params.get('dropout_rate', 0.3)\n",
    "    latent_dim = params.get('latent_dim', 10)\n",
    "    gamma = params.get('gamma', 1.0)\n",
    "    window_scale = params.get('window_scale', 1.0)\n",
    "    l1_reg = params.get('l1_reg', 0.0002)\n",
    "    l2_reg = params.get('l2_reg', 0.0002)\n",
    "    epochs = params.get('epochs', 80)\n",
    "    \n",
    "    # Get the training/testing data for this split.\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=P_TRAIN, random_state=123)\n",
    "    trn, tst = next(sss.split(X, Y))\n",
    "    ds_train, ds_valid, n_train = get_ds_train_valid(X, Y, trn, tst, batch_size=BATCH_SIZE, max_offset=MAX_OFFSET)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    model = make_model(X.shape[1], X.shape[2], aug_offset=MAX_OFFSET,\n",
    "                       k_spike_short=k_spike_short, l_spike_short=l_spike_short,\n",
    "                       k_spike_long=k_spike_long, l_spike_long=l_spike_long,\n",
    "                       D=D,\n",
    "                       n_pointwise_filters=n_pointwise_filters, kern_length_2=kern_length_2, downsamp_2=downsamp_2,\n",
    "                       norm_rate=norm_rate, dropout_rate=dropout_rate, l2_reg=l2_reg)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x=ds_train, epochs=epochs, validation_data=ds_valid, verbose=verbose)\n",
    "    max_val_acc = max(history.history['val_accuracy'])\n",
    "    print(\"Max validation accuracy with these parameters: {}\".format(max_val_acc))\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    return -max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSrG9mOZDXM9"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, Trials, tpe, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "trials = None\n",
    "hyperoptBest = None\n",
    "del trials\n",
    "del hyperoptBest\n",
    "\n",
    "space = {\n",
    "#     'k_spike_short': scope.int(hp.quniform('k_spike_short', 1, 15, 1)),\n",
    "#     'l_spike_short': scope.int(hp.quniform('l_spike_short', 8, 50, 2)),\n",
    "    'k_spike_long': scope.int(hp.quniform('k_spike_long', 1, 15, 1)),\n",
    "#     'l_spike_long': scope.int(hp.quniform('l_spike_long', 60, 250, 5)),\n",
    "#     'D': scope.int(hp.quniform('D', 1, 12, 1)),\n",
    "#     'downsamp_1': scope.int(hp.quniform('downsamp_1', 4, 10, 1)),\n",
    "#     'n_pointwise_filters': scope.int(hp.quniform('n_pointwise_filters', 2, 65, 1)),\n",
    "#     'kern_length_2': scope.int(hp.quniform('kern_length_2', 4, 64, 1)),\n",
    "#     'downsamp_2': scope.int(hp.quniform('downsamp_2', 2, 9, 1)),\n",
    "#     'norm_rate': hp.uniform('norm_rate', 0., 0.5),\n",
    "#     'dropout_rate': hp.uniform('dropout_rate', 0., 0.5),\n",
    "#     '12_reg': hp.loguniform('l1_reg', np.log(0.000001), np.log(1.0)),\n",
    "#     'l2_reg': hp.loguniform('l2_reg', np.log(0.000001), np.log(1.0)),\n",
    "#     'epochs': scope.int(hp.quniform('epochs', 60, 300, 20)),\n",
    "    'latent_dim': scope.int(hp.quniform('latent_dim', 5, 50, 5)),\n",
    "    'gamma': hp.loguniform('gamma', np.log(1e-4), np.log(100)),\n",
    "    'window_scale': hp.loguniform('window_scale', np.log(1e-4), np.log(100))\n",
    "}    \n",
    "\n",
    "trials = Trials()  # object that holds iteration results\n",
    "#Do optimization\n",
    "eval_hours = 5.\n",
    "minutes_per_eval = 2.5\n",
    "max_evals = int(eval_hours * 60 / minutes_per_eval)\n",
    "hyperoptBest = fmin(evaluate_model, space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "print(\"Best Acc: {}\".format(-trials.best_trial['result']['loss']))\n",
    "print(\"Best Parameters: {}\".format(hyperoptBest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6YAMqGADXNA"
   },
   "outputs": [],
   "source": [
    "def scatterplot_matrix_colored(params_names, params_values, best_losses,\n",
    "                               alpha=0.3, minmax='min'):\n",
    "    \"\"\"Scatterplot colored according to the Z values of the points.\"\"\"\n",
    "    import matplotlib\n",
    "    \n",
    "    nb_params = len(params_values)\n",
    "    \n",
    "    best_losses = np.array(best_losses)\n",
    "    if minmax == 'min':\n",
    "        best_trial = np.argmin(best_losses)\n",
    "    else:\n",
    "        best_trial = np.argmax(best_losses)\n",
    "        \n",
    "    norm = matplotlib.colors.Normalize(vmin=best_losses.min(), vmax=best_losses.max())\n",
    "    \n",
    "    fig, ax = plt.subplots(nb_params, nb_params, figsize=(16, 16))\n",
    "    \n",
    "    for i in range(nb_params):\n",
    "        p1 = params_values[i]\n",
    "        for j in range(nb_params):\n",
    "            p2 = params_values[j]\n",
    "            \n",
    "            axes = ax[i, j]\n",
    "            \n",
    "            axes.axvline(p2[best_trial], color='r', zorder=-1, alpha=0.3)\n",
    "            axes.axhline(p1[best_trial], color='r', zorder=-1, alpha=0.3)\n",
    "                \n",
    "            # Subplot:\n",
    "            s = axes.scatter(p2, p1, s=30, alpha=alpha,\n",
    "                             c=best_losses, cmap=turbo_cmap, norm=norm)\n",
    "\n",
    "            # Labels only on side subplots, for x and y:\n",
    "            if j == 0:\n",
    "                axes.set_ylabel(params_names[i], rotation=0)\n",
    "            else:\n",
    "                axes.set_yticks([])\n",
    "            \n",
    "            if i == nb_params - 1:\n",
    "                axes.set_xlabel(params_names[j], rotation=90)\n",
    "            else:\n",
    "                axes.set_xticks([])\n",
    "\n",
    "    fig.subplots_adjust(right=0.82, top=0.95)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "    cb = fig.colorbar(s, cax=cbar_ax)\n",
    "    \n",
    "    plt.suptitle('Scatterplot matrix of tried values in the search space over different params, colored according to best validation loss')\n",
    "    plt.show()\n",
    "\n",
    "# Prepare loss values. Maybe transform.\n",
    "hp_loss = np.array([_['result']['loss'] for _ in trials.trials])\n",
    "hp_loss = -hp_loss\n",
    "\n",
    "hp_names = list(space.keys())\n",
    "hp_vals = [[_['misc']['vals'][key][0] for _ in trials.trials] for key in hp_names]\n",
    "log_hps = [_ for _ in ['l1_reg', 'l2_reg', 'window_scale', 'gamma'] if _ in hp_names]\n",
    "for hp_name in log_hps:\n",
    "    hp_vals[hp_names.index(hp_name)] = np.log10(hp_vals[hp_names.index(hp_name)])\n",
    "\n",
    "scatterplot_matrix_colored(hp_names, hp_vals, hp_loss, minmax='max',\n",
    "                           alpha=0.8)\n",
    "print(np.max(hp_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruk6vmQwDXNC",
    "outputId": "c9ce72d2-e1ca-4e5d-e3c5-14b68064c185"
   },
   "outputs": [],
   "source": [
    "# Grouped bar plots of manually-input data.\n",
    "monkey_names = ['M', 'JL']\n",
    "data_types = ['LR\\nBaseline', 'LR\\nTargets', 'LR\\nFull', 'EEGNet\\nFull', 'LSTM\\nFull', 'EEGNet\\n& AE']\n",
    "accuracies = [[30.2, 43.4, 61.2, 72.3, 76.1, 69.9],[36.8, 46.5, 81.1, 85.4, 85.5, 82.5]]\n",
    "\n",
    "ind = np.arange(len(data_types))  # the x locations for the groups\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "for m_ix, m_name in enumerate(monkey_names):\n",
    "    ax.bar(ind - width/2 + m_ix * width, accuracies[m_ix], width, label=m_name)\n",
    "\n",
    "ax.set_ylabel('Accuracies (%)')\n",
    "ax.set_ylim([0, 100])\n",
    "ax.set_title('Target Prediction by Session and Model')\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(data_types)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(str(datadir / ('Acc_Bars.png')))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "03_02_CNN_faces_houses.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

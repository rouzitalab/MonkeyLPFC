{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064759e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dca6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# for device in gpu_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe22df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import indl\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from indl.fileio import from_neuropype_h5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from itertools import cycle\n",
    "\n",
    "import os\n",
    "\n",
    "if Path.cwd().stem == 'Analysis':\n",
    "    os.chdir(Path.cwd().parent.parent)\n",
    "\n",
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc    \n",
    "    \n",
    "data_path = Path.cwd() / 'StudyLocationRule'/ 'Data' / 'Preprocessed'\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")\n",
    "    \n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "load_kwargs_ul = {\n",
    "    'valid_outcomes': (0, 9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, -1),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_error = {\n",
    "    'valid_outcomes': (9,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_all = {\n",
    "    'valid_outcomes': (0,9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "model_kwargs = dict(\n",
    "    filt=8,\n",
    "    kernLength=20,\n",
    "    ds_rate=5,\n",
    "    n_rnn=32,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=32\n",
    ")\n",
    "model_kwargs1 = dict(\n",
    "    filt=16,\n",
    "    kernLength=30,\n",
    "#     ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "model_kwargs2 = dict(\n",
    "    filt=32,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "\n",
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "EPOCHS2 = 100\n",
    "LABEL_SMOOTHING = 0.2\n",
    "\n",
    "from indl.model import parts\n",
    "from indl.model.helper import check_inputs\n",
    "from indl.regularizers import KernelLengthRegularizer\n",
    "\n",
    "def make_model(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=32,\n",
    "    kernLength=16,\n",
    "    n_rnn=32,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.1,\n",
    "    activation='tanh',\n",
    "    l1_reg=0.010, l2_reg=0.010,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=32,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "#     if _input.shape[2] < 10:\n",
    "#         kernLength = 4\n",
    "#         filt = 4\n",
    "#         ds_rate = 4\n",
    "#     elif _input.shape[2] < 20:\n",
    "#         kernLength = 8\n",
    "#         ds_rate = 8\n",
    "#     elif _input.shape[2] < 30:\n",
    "#         kernLength = 16\n",
    "    \n",
    "#     input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "#     if len(input_shape) < 4:\n",
    "#         input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "#     _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv1D(filt, kernLength, strides=1, padding='valid', dilation_rate=1, groups=1,\n",
    "                                activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None,\n",
    "                                bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "                                bias_constraint=None)(inputs)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = tf.keras.layers.Dense(latent_dim, activation=activation)(_y)\n",
    "#     _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(_y)\n",
    "#     outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def kfold_pred(sess_id,X_rates,Y_class,name, verbose=1):\n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=np.max(Y_class)+1)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "#         randseed = 12345\n",
    "#         random.seed(randseed)\n",
    "#         np.random.seed(randseed)\n",
    "#         tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model(X_rates, _y.shape[-1], **model_kwargs1)\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'{name}_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"Accuracy: {accuracy}%\\n\\n\")\n",
    "    \n",
    "    return history, accuracy, pred_y, true_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_infos.append ({'name': 'Marty',\n",
    "                 'bank': 'A',\n",
    "                 'name_short': 'm',\n",
    "                 'date': '',\n",
    "                 'exp_code': 'sra3_1_m_074_00+01',\n",
    "                 'nsx': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3151da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session(s_idx, t_start=-np.inf, t_end=np.inf):\n",
    "    if s_idx == 8:\n",
    "        chunk = 2\n",
    "    else:\n",
    "        chunk = 1\n",
    "    sess_info = sess_infos[s_idx]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    sess_id = sess_id.replace(\"+\", \"\") + \"_segmented.h5\"\n",
    "    segmented_path = data_path / sess_id\n",
    "    segmented_data = from_neuropype_h5(segmented_path)\n",
    "    outcome = np.array(segmented_data[chunk][1]['axes'][0]['data']['OutcomeCode'])\n",
    "    flag = np.argwhere(outcome>-1).flatten()\n",
    "    outcome = outcome[flag]\n",
    "    Y = np.array(segmented_data[chunk][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "    Y_class = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "    X_rates = segmented_data[chunk][1]['data'][flag]\n",
    "    X_rates = np.nan_to_num(X_rates)\n",
    "#     X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "    block = np.array(segmented_data[chunk][1]['axes'][0]['data']['Block']).flatten()[flag]\n",
    "    b=np.diff(block, axis=0)\n",
    "    border=np.array(np.where(b>0)).flatten()\n",
    "    to_keep = [0]\n",
    "    for i in range(len(border) - 1):\n",
    "        if (border[i + 1] - border[i]) > 30 and (len(outcome)-border[i+1]) > 30:\n",
    "            to_keep.append(i + 1)\n",
    "    border = border[to_keep]\n",
    "    color = np.array(segmented_data[chunk][1]['axes'][0]['data']['CueColour']).flatten()[flag]\n",
    "    target = np.array(segmented_data[chunk][1]['axes'][0]['data']['TargetRule']).flatten()[flag]\n",
    "    classes = np.array(segmented_data[chunk][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "    times = np.array(segmented_data[chunk][1]['axes'][1]['times']).flatten()\n",
    "    idx_e = np.argwhere(times < t_end).flatten()[-1]\n",
    "    idx_s = np.argwhere(times > t_start).flatten()[0]\n",
    "    rule = np.zeros(np.size(X_rates,0))\n",
    "    for i in range(len(rule)):\n",
    "        if (target[i]=='UU'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=0\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=1\n",
    "            else:\n",
    "                rule[i]=2\n",
    "        elif (target[i]=='UR'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=3\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=4\n",
    "            else:\n",
    "                rule[i]=5\n",
    "        elif (target[i]=='RR'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=6\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=7\n",
    "            else:\n",
    "                rule[i]=8\n",
    "        elif (target[i]=='DR'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=9\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=10\n",
    "            else:\n",
    "                rule[i]=11\n",
    "        elif (target[i]=='DD'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=12\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=13\n",
    "            else:\n",
    "                rule[i]=14\n",
    "        elif (target[i]=='DL'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=15\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=16\n",
    "            else:\n",
    "                rule[i]=17\n",
    "        elif (target[i]=='LL'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=18\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=19\n",
    "            else:\n",
    "                rule[i]=20\n",
    "        elif (target[i]=='UL'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=21\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=22\n",
    "            else:\n",
    "                rule[i]=23\n",
    "    rule = rule.astype(int)\n",
    "#     tmp = rule\n",
    "#     _yu = np.unique(rule)\n",
    "#     for i in range(len(tmp)):\n",
    "#         rule[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    \n",
    "    m_performance = np.zeros(len(outcome))\n",
    "    cor = 0\n",
    "    b=0\n",
    "    tot = 25\n",
    "    for i in range(tot):\n",
    "        if outcome[i]==0:\n",
    "            cor += 1\n",
    "\n",
    "    m_performance[:tot] = 100 * (cor / tot)\n",
    "    # for i in range(tot, len(outcome)):\n",
    "    i = tot\n",
    "    while i<len(outcome):\n",
    "        if i == border[b]:\n",
    "            cor = 0\n",
    "            for j in range(tot):\n",
    "                if outcome[i+j]==0:\n",
    "                    cor += 1\n",
    "            m_performance[i:i+tot] = 100 * (cor / tot)\n",
    "            i += tot\n",
    "            b = (b+1)%len(border)\n",
    "        elif outcome[i] == outcome[i-tot]:\n",
    "            m_performance[i] = m_performance[i-1]\n",
    "            i += 1\n",
    "        elif outcome[i]==0:\n",
    "            cor += 1\n",
    "            m_performance[i] = 100 * (cor / tot)\n",
    "            i += 1\n",
    "        else:\n",
    "            cor -= 1\n",
    "            m_performance[i] = 100 * (cor / tot)\n",
    "            i +=1\n",
    "\n",
    "    for i in range(len(m_performance)):\n",
    "        if m_performance[i]<0:\n",
    "            m_performance[i] = 0\n",
    "    learned = np.argwhere(m_performance>75).flatten()\n",
    "    unlearned = np.argwhere(m_performance<65).flatten()\n",
    "    cor = np.array(np.where(outcome==0)).flatten()\n",
    "    icor = np.array(np.where(outcome==9)).flatten()\n",
    "    ic_l = []\n",
    "    c_l = []\n",
    "    ic_ul = []\n",
    "    for c in cor:\n",
    "        if c in learned:\n",
    "            c_l.append(c)\n",
    "    for ic in icor:\n",
    "        if ic in unlearned:\n",
    "            ic_ul.append(ic)\n",
    "        else:\n",
    "            ic_l.append(ic)\n",
    "    X_rates = X_rates[:, idx_s:idx_e+1, :]\n",
    "    \n",
    "    return X_rates, Y, rule, learned, unlearned, c_l, ic_ul, ic_l, border"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c91e5d",
   "metadata": {},
   "source": [
    "# Forgetting Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e2a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_MAX = np.inf\n",
    "T_MIN = 0.75\n",
    "for s_idx in range(1,4):\n",
    "    X_rates, Y, rule, learned, unlearned, c_l, ic_ul, ic_l, border = load_session(s_idx, t_start=T_MIN, t_end=T_MAX)\n",
    "    X_learned = X_rates[learned]\n",
    "    rule_learned = rule[learned]\n",
    "    b = [0] + list(border) + [X_rates.shape[0] + 1]\n",
    "    idx = list(learned[np.argwhere((learned > b[0]-1) & (learned < b[1])).flatten()])\n",
    "    X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "    for i in range(len(idx)):\n",
    "        X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_rates[idx[i]]).T\n",
    "\n",
    "    pca1 = PCA(n_components=20)\n",
    "    X1 = pca1.fit_transform(X)\n",
    "\n",
    "    X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "    for i in range(len(idx)):\n",
    "        X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "    pca2 = PCA(n_components=5)\n",
    "    X2 = pca2.fit_transform(X)\n",
    "\n",
    "    X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "    for i in range(len(idx)):\n",
    "        X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()\n",
    "    transformed = X\n",
    "    for i in range(2, len(b)):\n",
    "        idx2 = list(learned[np.argwhere((learned > b[i-1]-1) & (learned < b[i])).flatten()])\n",
    "        X = np.zeros((len(idx2)*X_learned.shape[2], X_learned.shape[1]))\n",
    "        for i in range(len(idx2)):\n",
    "            X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_rates[idx2[i]]).T\n",
    "\n",
    "        X1 = pca1.transform(X)\n",
    "\n",
    "        X = np.zeros((len(idx2)*X1.shape[1], X_learned.shape[2]))\n",
    "        for i in range(len(idx2)):\n",
    "            X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "        X2 = pca2.transform(X)\n",
    "\n",
    "        X = np.zeros((len(idx2), X1.shape[1] * X2.shape[1]))\n",
    "        for i in range(len(idx2)):\n",
    "            X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()\n",
    "        transformed = np.concatenate((transformed, X), axis=0)\n",
    "\n",
    "        idx = idx + idx2\n",
    "        X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "        for i in range(len(idx)):\n",
    "            X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_rates[idx[i]]).T\n",
    "\n",
    "        pca1 = PCA(n_components=20)\n",
    "        X1 = pca1.fit_transform(X)\n",
    "\n",
    "        X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "        for i in range(len(idx)):\n",
    "            X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "        pca2 = PCA(n_components=5)\n",
    "        pca2.fit(X)\n",
    "    rule_idx=[]\n",
    "    if s_idx == 1:\n",
    "        r = np.array([[2, 13],[23,9],[2,13],[4,17],[7,18],[7,18],[0,13],[23,10],[4,15],[3,17]]) # sess 1\n",
    "        dist1= np.zeros((r.shape[0]*2,r.shape[0]*2,100,100))\n",
    "    elif s_idx == 2:\n",
    "        r = np.array([[2, 13],[8,18],[8,18],[6,19],[6,20],[4,15],[5,15],[4,17]]) # sess 2\n",
    "        dist2= np.zeros((r.shape[0]*2,r.shape[0]*2,100,100))\n",
    "    elif s_idx == 3:\n",
    "        r = np.array([[21, 11],[3,16],[8,18],[6,19],[2,12],[3,16]]) # sess 3\n",
    "        dist3= np.zeros((r.shape[0]*2,r.shape[0]*2,100,100))\n",
    "    for j in range(0,len(b)-1):\n",
    "        idx=[]\n",
    "        idx2=[]\n",
    "        for i in np.argwhere((learned > b[j]-1) & (learned < b[j+1])).flatten():\n",
    "            if rule_learned[i]==r[j,0]:\n",
    "                idx.append(i)\n",
    "            elif rule_learned[i]==r[j,1]:\n",
    "                idx2.append(i)\n",
    "        rule_idx.append(np.array(idx))\n",
    "        rule_idx.append(np.array(idx2))\n",
    "    for i in range(r.shape[0]*2):\n",
    "        X1 = transformed[rule_idx[i]]\n",
    "        for j in range(r.shape[0]*2):\n",
    "            X2 = transformed[rule_idx[j]]\n",
    "            if s_idx == 1:\n",
    "                dist1[i,j]= ((np.transpose(X1)@X1)/X1.shape[0])-((np.transpose(X2)@X2)/X2.shape[0])\n",
    "            elif s_idx == 2:\n",
    "                dist2[i,j]= ((np.transpose(X1)@X1)/X1.shape[0])-((np.transpose(X2)@X2)/X2.shape[0])\n",
    "            elif s_idx == 3:\n",
    "                dist3[i,j]= ((np.transpose(X1)@X1)/X1.shape[0])-((np.transpose(X2)@X2)/X2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66afa021",
   "metadata": {},
   "outputs": [],
   "source": [
    "fem = np.zeros((9,9))\n",
    "fem[0,8]=10\n",
    "# fem[0,7]=34\n",
    "fem[0,0]=120\n",
    "fem[1,4]=10\n",
    "# fem[1,2]=118\n",
    "fem[4,8]=10\n",
    "fem[4,6]=35\n",
    "fem[4,4]=26\n",
    "fem[4,1]=47\n",
    "fem[4,0]=82\n",
    "fem[5,3]=83\n",
    "fem[5,1]=152\n",
    "fem[6,8]=10\n",
    "fem[6,6]=10\n",
    "fem[6,2]=90\n",
    "fem[7,1]=62\n",
    "fem[7,2]=62\n",
    "# fem[8,4]=10\n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        if (i!=j):\n",
    "            plt.plot(j,i,marker='o', markersize=fem[i,j], alpha=0.7)\n",
    "plt.plot([0,8],[0,8])\n",
    "p = [0,1,2,3,4,5,6,7,8]\n",
    "l = [\"-1\", \"-0.75\", \"-0.5\",\"-0.25\",\"0.0\", \"0.25\",\"0.5\", \"0.75\", \"1\"]\n",
    "plt.xticks(p,l)\n",
    "plt.yticks(p,l)\n",
    "plt.grid()\n",
    "plt.ylabel(\"RSS Two Blocks Away\")\n",
    "plt.xlabel(\"RSS One Block Away\")\n",
    "plt.savefig(\"intereference.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 5\n",
    "col = 6\n",
    "print(dist[row,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6592d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = dist1\n",
    "a2 = dist2\n",
    "a3 = dist3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = a1\n",
    "dist2 = a2\n",
    "dist3 = a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf62013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist1=dist1/(LA.norm(dist1)**(1/m))\n",
    "# dist2=dist2/(LA.norm(dist2)**(1/m))\n",
    "# dist3=dist3/(LA.norm(dist3)**(1/m))\n",
    "dist1=dist1/2\n",
    "dist2=dist2/2\n",
    "dist3=dist3/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9523cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=3\n",
    "fem = np.zeros((9,9))\n",
    "# fem[0,7]=34\n",
    "fem[0,8]=LA.norm(dist2[5,8]+dist2[4,9]+dist2[6,8]+dist2[7,9])**(1/m)\n",
    "fem[0,0]=LA.norm(dist2[3,6]+dist2[2,7]+dist2[5,6]+dist2[4,7])**(1/m)\n",
    "fem[1,4]=LA.norm(dist1[13,16]+dist1[12,17]+dist1[15,16]+dist1[14,17])**(1/m)\n",
    "# fem[1,2]=LA.norm(dist2[7,10]+dist2[6,11]+dist2[8,11]+dist2[9,10])**(1/m)\n",
    "fem[4,8]=LA.norm(dist1[0,4]+dist1[1,5]+dist1[2,4]+dist1[3,5])**(1/m)\n",
    "# fem[4,8]=LA.norm(dist2[0,4]+dist2[1,5]+dist2[2,4]+dist2[3,5])**(1/m)\n",
    "# fem[4,8]=LA.norm(dist2[9,12]+dist2[8,13]+dist2[10,12]+dist2[11,13])**(1/m)\n",
    "fem[7,1]=LA.norm(dist1[5,8]+dist1[4,9]+dist1[6,8]+dist1[7,9])**(1/m)\n",
    "fem[4,4]=LA.norm(dist1[9,12]+dist1[8,13]+dist1[11,12]+dist1[10,13])**(1/m)\n",
    "fem[4,1]=LA.norm(dist1[2,7]+dist1[3,6]+dist1[4,7]+dist1[5,6])**(1/m)\n",
    "# fem[4,0]=LA.norm(dist3[2,6]+dist3[3,7]+dist3[5,6]+dist3[4,7])**(1/m)\n",
    "fem[4,0]=LA.norm(dist1[15,18]+dist1[14,19]+dist1[17,18]+dist1[16,19])**(1/m)\n",
    "fem[5,3]=LA.norm(dist3[4,8]+dist3[5,9]+dist3[6,9]+dist3[7,8])**(1/m)\n",
    "fem[5,1]=LA.norm(dist2[3,6]+dist2[2,7]+dist2[5,6]+dist2[4,7])**(1/m)+30\n",
    "fem[6,8]=LA.norm(dist1[6,10]+dist1[7,11]+dist1[8,10]+dist1[9,11])**(1/m)\n",
    "fem[6,6]=LA.norm(dist1[11,14]+dist1[10,15]+dist1[12,14]+dist1[13,15])**(1/m)\n",
    "fem[6,2]=LA.norm(dist3[4,8]+dist3[5,9]+dist3[6,9]+dist3[7,8])**(1/m)+10\n",
    "fem[4,6]=LA.norm(dist2[10,14]+dist2[11,15]+dist2[13,14]+dist2[12,15])**(1/m)\n",
    "fem[7,2]=LA.norm(dist3[1,4]+dist3[0,5]+dist3[3,4]+dist3[2,5])**(1/m)\n",
    "# fem[8,4]=LA.norm(dist1[0,4]+dist1[1,5]+dist1[2,4]+dist1[3,5])**(1/m)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        if (i!=j):\n",
    "            plt.plot(j,i,marker='o', markersize=fem[i,j], alpha=0.7)\n",
    "plt.plot([0,8],[0,8])\n",
    "p = [0,1,2,3,4,5,6,7,8]\n",
    "l = [\"-1\", \"-0.75\", \"-0.5\",\"-0.25\",\"0.0\", \"0.25\",\"0.5\", \"0.75\", \"1\"]\n",
    "plt.xticks(p,l)\n",
    "plt.yticks(p,l)\n",
    "plt.grid()\n",
    "plt.ylabel(\"RSS Two Blocks Away\")\n",
    "plt.xlabel(\"RSS One Block Away\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ad940",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=2\n",
    "fem = np.zeros((9,9))\n",
    "# fem[0,7]=34\n",
    "fem[0,8]=np.sum(np.dot((dist2[5,8]+dist2[4,9]),(dist2[6,8]+dist2[7,9])))\n",
    "fem[0,0]=np.sum(np.dot((dist2[3,6]+dist2[2,7]),(dist2[5,6]+dist2[4,7])))\n",
    "fem[1,4]=np.sum(np.dot((dist1[13,16]+dist1[12,17]),(dist1[15,16]+dist1[14,17])))\n",
    "# fem[1,2]=np.sum(np.dot((dist2[7,10]+dist2[6,11]),(dist2[8,11]+dist2[9,10])))\n",
    "fem[4,8]=np.sum(np.dot((dist1[0,4]+dist1[1,5]),(dist1[2,4]+dist1[3,5])))\n",
    "# fem[4,8]=np.sum(np.dot((dist2[0,4]+dist2[1,5]),(dist2[2,4]+dist2[3,5])))\n",
    "# fem[4,8]=np.sum(np.dot((dist2[9,12]+dist2[8,13]),(dist2[10,12]+dist2[11,13])))\n",
    "# fem[7,1]=np.sum(np.dot((dist1[5,8]+dist1[4,9]),(dist1[6,8]+dist1[7,9])))\n",
    "fem[7,1]=np.sum(np.dot((dist3[1,4]+dist3[0,5]),(dist3[3,4]+dist3[2,5])))\n",
    "fem[4,4]=np.sum(np.dot((dist1[9,12]+dist1[8,13]),(dist1[11,12]+dist1[10,13])))\n",
    "fem[4,1]=np.sum(np.dot((dist1[2,7]+dist1[3,6]),(dist1[4,7]+dist1[5,6])))\n",
    "# fem[4,0]=np.sum(np.multiply((dist3[2,6]+dist3[3,7]),(dist3[5,6]+dist3[4,7])))\n",
    "fem[4,0]=np.sum(np.dot((dist1[15,18]+dist1[14,19]),(dist1[17,18]+dist1[16,19])))\n",
    "fem[5,3]=np.sum(np.dot((dist3[4,8]+dist3[5,9]),(dist3[6,9]+dist3[7,8])))\n",
    "fem[5,1]=np.sum(np.multiply((dist2[3,6]+dist2[2,7]),(dist2[5,6]+dist2[4,7])))/2.5\n",
    "fem[6,8]=np.sum(np.dot((dist1[6,10]+dist1[7,11]),(dist1[8,10]+dist1[9,11])))\n",
    "fem[6,6]=np.sum(np.dot((dist1[11,14]+dist1[10,15]),(dist1[12,14]+dist1[13,15])))\n",
    "fem[6,2]=np.sum(np.dot((dist3[4,8]+dist3[5,9]),(dist3[6,9]+dist3[7,8])))\n",
    "# fem[4,6]=np.sum(np.multiply((dist2[10,14]+dist2[11,15]),(dist2[13,14]+dist2[12,15])))+500000\n",
    "fem[4,6]=np.sum(np.dot((dist1[5,8]+dist1[4,9]),(dist1[6,8]+dist1[7,9])))\n",
    "fem[7,2]=np.sum(np.dot((dist3[1,4]+dist3[0,5]),(dist3[3,4]+dist3[2,5])))\n",
    "# fem[8,4]=np.sum(np.multiply((dist1[0,4]+dist1[1,5]),(dist1[2,4]+dist1[3,5])))\n",
    "fem = fem / 10000\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        if (i!=j):\n",
    "            plt.plot(j,i,marker='o', markersize=fem[i,j], alpha=0.7)\n",
    "plt.plot([0,8],[0,8])\n",
    "p = [0,1,2,3,4,5,6,7,8]\n",
    "l = [\"-1\", \"-0.75\", \"-0.5\",\"-0.25\",\"0.0\", \"0.25\",\"0.5\", \"0.75\", \"1\"]\n",
    "plt.xticks(p,l)\n",
    "plt.yticks(p,l)\n",
    "plt.grid()\n",
    "plt.ylabel(\"RSS Two Blocks Away\")\n",
    "plt.xlabel(\"RSS One Block Away\")\n",
    "plt.savefig(\"intereference_latent.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca84b0",
   "metadata": {},
   "source": [
    "# Latent Space Similar Rule Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ab11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_idx = 1\n",
    "T_MAX = 1.25\n",
    "T_MIN = 0.25\n",
    "X_rates, Y, rule, learned, unlearned, c_l, ic_ul, ic_l, border = load_session(s_idx, t_start=T_MIN, t_end=T_MAX)\n",
    "X_learned = X_rates[learned]\n",
    "rule_learned = rule[learned]\n",
    "print(np.unique(rule_learned, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02597065",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [0] + list(border) + [X_rates.shape[0]]\n",
    "for i in range(1, len(b)):\n",
    "    print(np.unique(rule[b[i-1]:b[i]], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a7b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [0] + list(border) + [X_rates.shape[0] + 1]\n",
    "idx = list(learned[np.argwhere((learned > b[0]-1) & (learned < b[1])).flatten()])\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_rates[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()\n",
    "transformed = X\n",
    "for i in range(2, len(b)):\n",
    "    idx2 = list(learned[np.argwhere((learned > b[i-1]-1) & (learned < b[i])).flatten()])\n",
    "    X = np.zeros((len(idx2)*X_learned.shape[2], X_learned.shape[1]))\n",
    "    for i in range(len(idx2)):\n",
    "        X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_rates[idx2[i]]).T\n",
    "\n",
    "    X1 = pca1.transform(X)\n",
    "\n",
    "    X = np.zeros((len(idx2)*X1.shape[1], X_learned.shape[2]))\n",
    "    for i in range(len(idx2)):\n",
    "        X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "    X2 = pca2.transform(X)\n",
    "\n",
    "    X = np.zeros((len(idx2), X1.shape[1] * X2.shape[1]))\n",
    "    for i in range(len(idx2)):\n",
    "        X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()\n",
    "    transformed = np.concatenate((transformed, X), axis=0)\n",
    "\n",
    "    idx = idx + idx2\n",
    "    X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "    for i in range(len(idx)):\n",
    "        X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_rates[idx[i]]).T\n",
    "\n",
    "    pca1 = PCA(n_components=20)\n",
    "    X1 = pca1.fit_transform(X)\n",
    "\n",
    "    X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "    for i in range(len(idx)):\n",
    "        X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "    pca2 = PCA(n_components=5)\n",
    "    pca2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb359e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_learned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023876e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_idx=[]\n",
    "if s_idx == 1:\n",
    "    r = np.array([[2, 13],[23,9],[2,13],[4,17],[7,18],[7,18],[0,13],[23,10],[4,15],[3,17]]) # sess 1\n",
    "    dist1= np.zeros((r.shape[0]*2,r.shape[0]*2,100,100))\n",
    "elif s_idx == 2:\n",
    "    r = np.array([[2, 13],[8,18],[8,18],[6,19],[6,20],[4,15],[5,15],[4,17]]) # sess 2\n",
    "    dist2= np.zeros((r.shape[0]*2,r.shape[0]*2,100,100))\n",
    "elif s_idx == 3:\n",
    "    r = np.array([[21, 11],[3,16],[8,18],[6,19],[2,12],[3,16]]) # sess 3\n",
    "    dist3= np.zeros((r.shape[0]*2,r.shape[0]*2,100,100))\n",
    "for j in range(0,len(b)-1):\n",
    "    idx=[]\n",
    "    idx2=[]\n",
    "    for i in np.argwhere((learned > b[j]-1) & (learned < b[j+1])).flatten():\n",
    "        if rule_learned[i]==r[j,0]:\n",
    "            idx.append(i)\n",
    "        elif rule_learned[i]==r[j,1]:\n",
    "            idx2.append(i)\n",
    "    rule_idx.append(np.array(idx))\n",
    "    rule_idx.append(np.array(idx2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.zeros((r.shape[0]*2,r.shape[0]*2))\n",
    "# dist1= np.zeros((r.shape[0]*2,r.shape[0]*2,100,100))\n",
    "# dist2= np.zeros((r.shape[0]*2,r.shape[0]*2,100,100))\n",
    "# dist3= np.zeros((r.shape[0]*2,r.shape[0]*2,100,100))\n",
    "for i in range(r.shape[0]*2):\n",
    "    X1 = transformed[rule_idx[i]]\n",
    "    for j in range(r.shape[0]*2):\n",
    "        X2 = transformed[rule_idx[j]]\n",
    "        dist[i,j] = np.sqrt(LA.norm(((np.transpose(X1)@X1)/X1.shape[0]-(np.transpose(X2)@X2)/X2.shape[0])))\n",
    "        if s_idx == 1:\n",
    "            dist1[i,j]= ((np.transpose(X1)@X1)/X1.shape[0])-((np.transpose(X2)@X2)/X2.shape[0])\n",
    "        elif s_idx == 2:\n",
    "            dist2[i,j]= ((np.transpose(X1)@X1)/X1.shape[0])-((np.transpose(X2)@X2)/X2.shape[0])\n",
    "        elif s_idx == 3:\n",
    "            dist3[i,j]= ((np.transpose(X1)@X1)/X1.shape[0])-((np.transpose(X2)@X2)/X2.shape[0])\n",
    "        \n",
    "# dist = dist / (2*np.max(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 3\n",
    "col = 5\n",
    "print(dist[row,col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447581ae",
   "metadata": {},
   "source": [
    "## RSS -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc8ba78",
   "metadata": {},
   "source": [
    "### Sess 1 (13, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11787abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [0,  2,  4,  7,  8,  9, 10, 14, 16, 17, 19, 20, 23, 24, 26, 29, 32,\n",
    "       39, 40, 41, 43]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [44,  49,  52,  54,  56,  58,  61,  62,  63,  65,  66,  67,  73,\n",
    "        75,  76,  79,  81,  88,  91,  92,  93,  95,  98, 100, 102, 103,\n",
    "       104, 111, 112, 113, 116, 118, 122, 124, 125, 128, 129, 132, 133,\n",
    "       134, 135, 137, 139, 142, 147, 148, 153, 159, 160, 161, 162, 163,\n",
    "       164, 166, 167, 171, 173]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508e316",
   "metadata": {},
   "source": [
    "## Neutral Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b9733",
   "metadata": {},
   "source": [
    "### Sess 1 (0, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [324, 325, 327, 331, 333, 335, 336, 337, 338, 339, 344, 345, 346,\n",
    "       348, 349, 352, 353, 354, 355, 356, 358, 362, 363, 365, 371, 373,\n",
    "       377, 379, 380, 382, 384, 389, 393, 395, 396, 398, 399, 401, 404,\n",
    "       405, 406, 409, 410, 413, 414, 416, 417, 418, 419, 421, 424, 427,\n",
    "       428, 429, 430, 432, 436, 437, 438, 442, 443, 447, 448, 450, 453,\n",
    "       454, 456, 457, 458, 459, 460, 462, 465, 466, 467, 480, 481, 482,\n",
    "       483, 488, 489, 490, 491, 492, 493, 495, 496, 497, 498, 501, 502,\n",
    "       503, 504, 505, 507, 511, 512, 513, 514, 515, 516, 519, 520, 521,\n",
    "       524, 527, 530, 533, 534, 535, 537, 538]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625046ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [540, 542, 543, 545, 546, 547, 548, 550, 551, 552, 555, 557, 560,\n",
    "       561, 564, 565, 566, 567, 570, 575, 576, 577, 578, 579, 580, 581,\n",
    "       582, 584, 586, 588, 589]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512415fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd42aa",
   "metadata": {},
   "source": [
    "### Sess 2 (6, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [1,  3,  5,  6, 11, 12, 13, 15, 18, 21, 22, 25, 27, 28, 30, 31, 33,\n",
    "       34, 35, 36, 37, 38, 42]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [174, 175, 180, 185, 186, 187, 190, 191, 193, 194, 195, 197, 201,\n",
    "       202, 204, 205, 206, 208, 209, 211, 212, 213, 215, 219, 221, 223,\n",
    "       225, 226, 227, 231, 232, 233, 234, 238, 239, 240, 241, 243, 244,\n",
    "       245, 247, 248, 250, 252, 255, 256, 257]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7682922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111a1ce",
   "metadata": {},
   "source": [
    "### Sess 3 (2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [92,  95,  98, 100, 106, 108, 109, 111, 112, 115, 116, 117, 121,\n",
    "       122, 127, 128, 133, 271, 277, 278, 279, 285]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [136, 139, 140, 143, 144, 146, 147, 149, 151, 152, 153, 157, 158,\n",
    "       160, 163, 165, 166, 167, 168, 169, 170, 171, 172]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eebf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d223b14",
   "metadata": {},
   "source": [
    "### Sess 5 (16, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 14, 18, 19, 20, 21, 22,\n",
    "       25, 27]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [28,  29,  30,  31,  32,  33,  34,  35, 126, 127, 128]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce5b9d",
   "metadata": {},
   "source": [
    "## RSS 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [73,  75,  77,  81,  82,  84,  85,  87,  90,  92,  94,  96,  98,\n",
    "       101, 102, 106, 107, 109, 110, 111, 112, 113, 114, 115, 118, 123,\n",
    "       124, 126, 128, 129, 132, 133, 134, 135, 136, 137, 139, 143]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()\n",
    "\n",
    "pca3 = PCA(n_components=2)\n",
    "X3 = pca3.fit_transform(X)\n",
    "\n",
    "for i in range(len(idx)):\n",
    "    plt.plot(X3[i,0],X3[i,1],marker='o',color='cyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [594, 597, 598, 601, 602, 603, 605, 606, 607, 608, 611, 613, 614, 616,\n",
    "       617, 618, 621, 623, 624, 625, 627, 628, 629, 630, 631, 632, 634,\n",
    "       635, 637, 639, 640, 643, 648, 652, 653, 654, 655, 657, 661, 663,\n",
    "       665, 667]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b178a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(Xrss1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1880cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d641f8e",
   "metadata": {},
   "source": [
    "### sess 2, rule 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [1,   3,   6,   7,   8,  11,  12,  15,  16,  19,  20,  21,  25,\n",
    "        26,  29,  33,  36,  38,  42,  45,  46,  50,  51,  53,  54,  57,\n",
    "        62,  63,  65,  70,  71,  72]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()\n",
    "\n",
    "pca3 = PCA(n_components=2)\n",
    "X3 = pca3.fit_transform(X)\n",
    "\n",
    "for i in range(len(idx)):\n",
    "    plt.plot(X3[i,0],X3[i,1],marker='o',color='cyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [148, 150, 152, 153, 157, 158, 159,\n",
    "       160, 161, 162, 163, 164, 167, 168, 171, 172, 178, 179, 181, 182,\n",
    "       184, 185, 187, 188, 189, 191, 193, 194, 196]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5dbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47264df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(Xrss1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19835e",
   "metadata": {},
   "source": [
    "### sess 3, rule 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97310e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [90,  91,  93,  94,  96,  97,  99, 101, 102, 103, 104, 105, 107,\n",
    "       110, 113, 114, 118, 119, 120, 123, 124, 125, 126, 129, 130, 131,\n",
    "       132, 134, 135]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()\n",
    "\n",
    "pca3 = PCA(n_components=2)\n",
    "X3 = pca3.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [270, 272, 273, 274, 275, 276, 280, 281, 282, 283,\n",
    "       284, 286, 287, 288, 289, 290]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4707ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f1b980",
   "metadata": {},
   "source": [
    "### sess 2, rule 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [0,   2,   4,   5,   9,  10,  13,  14,  17,  18,  22,  23,  24,\n",
    "        27,  28,  30,  31,  32,  34,  35,  37,  39,  40,  41,  43,  44,\n",
    "        47,  48,  49,  52,  55,  56,  58,  59,  60,  61,  64,  66,  67,\n",
    "        68,  69]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [146, 147, 149, 151, 154, 155, 156, 165, 166, 169, 170,\n",
    "       173, 174, 175, 176, 177, 180, 183, 186, 190, 192, 195, 197, 198]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a5712",
   "metadata": {},
   "source": [
    "### sess 2, rule 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06695b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [201, 203, 204, 205, 208, 209, 210, 217, 222, 226, 227, 231, 232,\n",
    "       236, 237, 240, 242, 243, 244, 245, 249, 250, 252, 254, 256, 258,\n",
    "       259, 260, 261, 262, 265, 270, 271, 274, 275, 276, 285, 286, 287,\n",
    "       290, 291, 292, 293, 296, 297, 298, 299, 301, 302, 305, 309, 310,\n",
    "       311, 313, 315, 316, 320]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf18b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [714, 715, 716, 719, 720, 721, 723, 724,\n",
    "       727, 728, 730, 731, 732, 734, 735, 737, 738, 739, 740, 742, 747,\n",
    "       748, 750, 751, 755, 758, 761, 762, 764, 766, 767, 772, 777, 779]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf007898",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e491f",
   "metadata": {},
   "source": [
    "### sess 2, rule 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c26ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [73,  75,  77,  81,  82,  84,  85,  87,  90,  92,  94,  96,  98,\n",
    "       101, 102, 106, 107, 109, 110, 111, 112, 113, 114, 115, 118, 123,\n",
    "       124, 126, 128, 129, 132, 133, 134, 135, 136, 137, 139, 143]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [594, 597, 598, 601, 602, 603, 605, 606, 607, 608, 611, 613, 614, 616,\n",
    "       617, 618, 621, 623, 624, 625, 627, 628, 629, 630, 631, 632, 634,\n",
    "       635, 637, 639, 640, 643, 648, 652, 653, 654, 655, 657, 661, 663,\n",
    "       665, 667]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c49d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc20e47",
   "metadata": {},
   "source": [
    "## RSS -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b01653",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [540, 542, 543, 545, 546, 547, 548, 550, 551, 552, 555, 557, 560,\n",
    "       561, 564, 565, 566, 567, 570, 575, 576, 577, 578, 579, 580, 581,\n",
    "       582, 584, 586, 588, 589]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()\n",
    "\n",
    "pca3 = PCA(n_components=2)\n",
    "X3 = pca3.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [468, 469, 470, 471]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938011a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaddda2f",
   "metadata": {},
   "source": [
    "### Sess 2, (5, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dde830",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [318, 320, 324, 326, 330, 332, 334, 339, 340, 342, 350, 351, 352,\n",
    "       356]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [360, 361, 362, 363, 367, 371, 372, 373, 376, 377, 378, 379, 380,\n",
    "       382, 383, 385, 386, 387, 388, 389, 394, 395, 396, 397, 398]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99556a95",
   "metadata": {},
   "source": [
    "### Sess 3, (6, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [174, 176, 177, 179, 181, 182, 185, 186, 187, 188, 190, 192, 194,\n",
    "       195, 197, 198, 199, 200, 203, 205, 207, 208, 209, 211, 212, 213,\n",
    "       216, 217, 218, 221, 222, 223, 224, 225, 226, 231, 232, 234, 235,\n",
    "       236, 237, 239, 240, 244, 245, 246, 247, 248]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [137, 138, 141, 142, 145, 148, 150, 154, 155, 156, 159, 161, 162,\n",
    "       164, 173]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ddf5e",
   "metadata": {},
   "source": [
    "### Sess 5, (6, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [36,  37,  38,  39,  45,  46,  50,  52,  54,  56,  58,  61,  63,\n",
    "        65,  67,  69,  71,  75,  77,  78,  80,  82,  83,  84,  85,  88,\n",
    "        89,  90,  91,  93,  95,  98,  99, 101, 102, 103, 104, 106, 107]\n",
    "X = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "pca1 = PCA(n_components=20)\n",
    "X1 = pca1.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    X[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "pca2 = PCA(n_components=5)\n",
    "X2 = pca2.fit_transform(X)\n",
    "\n",
    "X = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    X[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [109, 113, 114, 115, 116, 119, 122, 123, 124]\n",
    "Xrss1 = np.zeros((len(idx)*X_learned.shape[2], X_learned.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X_learned.shape[2]: (i+1)*X_learned.shape[2], :] = np.squeeze(X_learned[idx[i]]).T\n",
    "    \n",
    "X1 = pca1.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx)*X1.shape[1], X_learned.shape[2]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i*X1.shape[1]:(i+1)*X1.shape[1], :] = X1[i*X_learned.shape[2]:(i+1)*X_learned.shape[2], :].T\n",
    "\n",
    "X2 = pca2.transform(Xrss1)\n",
    "\n",
    "Xrss1 = np.zeros((len(idx), X1.shape[1] * X2.shape[1]))\n",
    "for i in range(len(idx)):\n",
    "    Xrss1[i, :] = X2[i*X1.shape[1]:(i+1)*X1.shape[1], :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.mean(X, axis=0)\n",
    "b = np.mean(Xrss1, axis=0)\n",
    "np.sqrt(np.sum((a-b)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09696c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_idx = 5\n",
    "T_MAX = 1.45\n",
    "T_MIN = 0\n",
    "X_rates, Y, rule, learned, unlearned, c_l, ic_ul, ic_l = load_session(s_idx, t_start=T_MIN, t_end=T_MAX)\n",
    "X_learned = X_rates[learned]\n",
    "rule_learned = rule[learned]\n",
    "print(np.unique(rule_learned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd82fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(rule_learned==6).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cfe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "p = [-1, -0.5, 0.0, 0.25, 0.75, 1]\n",
    "x = [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-.5,-.5,-.5,-.5,-.5,-.5,-.5,-.5,-.5,-.5,0,0,0,0,0,0,0,0,0,0,0.25,0.25,0.25,0.25,\n",
    "     0.25,0.25,0.25,0.25,0.25,0.25,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,0.75,1,1,1,1,1,1,1,1,1,1]\n",
    "y = [0.47060431,0.46919378,0.46608955,0.43311404,0.46608955,0.4935631,0.48819465,0.46025645,0.45148687,0.46919378,\n",
    "    0.41660472,0.42554191,0.47060431,0.42554191,0.43206866,0.43341848,0.40057758,0.40932008,0.41660443,0.42488055,\n",
    "    0.46919378,0.40932008,0.38087621,0.40057758,0.38526801,0.38122559,0.36266173,0.38533932,0.37608731,0.36438002,\n",
    "    0.29072802,0.37608731,0.31394658,0.3387681,0.2948543,0.29786349,0.29796858,0.21262282,0.33028902,0.33241842,\n",
    "    0.2953976,0.27435858,0.2664055,0.26552351,0.26267236,0.27692681,0.25010187,0.30305333,0.21967157,0.27326948,\n",
    "    0.18671827,0.14783386,0.15601248,0.20842128,0.21722556,0.222167,0.22249631,0.15601248,0.20346332,0.20916751]\n",
    "m, b = np.polyfit(x,np.dot(y,2), 1)\n",
    "y2 = np.dot(m,x) + b\n",
    "plt.errorbar(p, np.dot([0.466778608, 0.445516263, 0.391492963, 0.308554664, 0.268738061, 0.192951807],2), yerr=np.dot([0.016164735, 0.01783762, 0.02921676, 0.040594603, 0.021942751, 0.027806258],2), fmt='.', markersize='20', capsize=5, label=\"Normalized Distance\")\n",
    "plt.plot(x, y2, lw=3, color='tab:orange', label='Fitted Linear Model')\n",
    "plt.ylim([0, 1.02])\n",
    "l = [\"-1\", \"-0.5\", \"0.0\", \"0.25\", \"0.75\", \"1\"]\n",
    "plt.xticks(p,l)\n",
    "plt.ylabel(\"Normalized Distance of Rules in Latent Space\")\n",
    "plt.xlabel(\"RSS\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "# handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "# #specify order of items in legend\n",
    "# order = [1,0]\n",
    "\n",
    "# #add legend to plot\n",
    "# plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order])\n",
    "plt.savefig(\"rule_latent_distance.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "\n",
    "mod = sm.OLS(y, x)\n",
    "res = mod.fit()\n",
    "print (res.conf_int(0.05))   # 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl1 = 16\n",
    "rl2 = 4\n",
    "idx = [1148, 1150, 1152, 1154, 1155, 1156, 1157, 1158, 1160, 1168, 1169,\n",
    "       1170, 1171, 1173, 1174, 1175, 1176, 1178, 1179, 1180, 1181, 1185,\n",
    "       1187, 1188, 1191, 1193, 1195, 1197, 1198, 1199, 1200, 1202, 1203,\n",
    "       1204, 1206, 1207, 1210, 1212, 1213, 1215, 1217, 1218, 1221, 1223,\n",
    "       1226, 1227, 1230, 1232, 1236, 1237, 1238, 1239, 1242, 1246, 1248,\n",
    "       1250, 1252, 1253, 1254]\n",
    "x = X_rates[idx,:,:].reshape(len(idx),-1, order='F')\n",
    "y = rule[idx]\n",
    "pca = PCA(n_components=5)\n",
    "ls = pca.fit_transform(x)\n",
    "for i in range(len(ls)):\n",
    "    if y[i] == rl1:\n",
    "        c = 'red'\n",
    "    else:\n",
    "        c='blue'\n",
    "    plt.plot(ls[i,0],ls[i,1],marker='o',color=c)\n",
    "idx = [1256, 1259, 1260, 1264, 1265, 1269, 1270, 1271, 1272, 1273, 1274,\n",
    "       1276, 1277, 1278, 1279, 1283, 1284, 1287, 1300, 1301, 1302, 1303,\n",
    "       1304, 1306, 1307, 1308, 1311, 1312, 1313, 1314, 1315, 1316, 1317,\n",
    "       1318, 1319, 1320, 1322, 1323, 1324, 1327, 1329, 1331, 1332, 1333,\n",
    "       1337, 1343, 1346, 1349, 1350, 1351, 1352, 1355, 1356, 1357, 1361,\n",
    "       1366, 1369, 1373, 1374, 1375]\n",
    "x = X_rates[idx,:,:].reshape(len(idx),-1, order='F')\n",
    "y = rule[idx]\n",
    "ls2 = pca.fit_transform(x)\n",
    "for i in range(len(ls2)):\n",
    "    if y[i] == rl2:\n",
    "        c = 'pink'\n",
    "    else:\n",
    "        c='cyan'\n",
    "    plt.plot(ls2[i,0],ls2[i,1],marker='o',color=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122cc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_m = np.mean(ls, axis=0)\n",
    "ls2_m = np.mean(ls2, axis=0)\n",
    "print(ls_m**2, ls2_m**2)\n",
    "print(1e14*np.sum(np.sqrt(np.abs(ls_m**2 - ls2_m**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rssone = [3.2454208949147847,4.906998836516008,5.1916406351952125, 3.8126293022235607, 5.935598684255813]\n",
    "rssnone = [19.98012394316079,4.596427852143187,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_idx = 5\n",
    "if s_idx == 8:\n",
    "    chunk = 2\n",
    "else:\n",
    "    chunk = 1\n",
    "sess_info = sess_infos[s_idx]\n",
    "sess_id = sess_info['exp_code']\n",
    "sess_id = sess_id.replace(\"+\", \"\") + \"_v1_segmented.h5\"\n",
    "segmented_path = data_path / sess_id\n",
    "segmented_data = from_neuropype_h5(segmented_path)\n",
    "outcome = np.array(segmented_data[chunk][1]['axes'][0]['data']['OutcomeCode'])\n",
    "flag = np.argwhere(outcome>-1).flatten()\n",
    "outcome = outcome[flag]\n",
    "Y = np.array(segmented_data[chunk][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "X_rates = segmented_data[chunk][1]['data'][flag]\n",
    "X_rates = np.nan_to_num(X_rates)\n",
    "#     X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "block = np.array(segmented_data[chunk][1]['axes'][0]['data']['Block']).flatten()[flag]\n",
    "b=np.diff(block, axis=0)\n",
    "border=np.array(np.where(b>0)).flatten()\n",
    "to_keep = [0]\n",
    "for i in range(len(border) - 1):\n",
    "    if (border[i + 1] - border[i]) > 30 and (len(outcome)-border[i+1]) > 30:\n",
    "        to_keep.append(i + 1)\n",
    "border = border[to_keep]\n",
    "# to_keep = []\n",
    "# for i in range(len(border)-1):\n",
    "#     if i==0 and border[i] > 30:\n",
    "#         to_keep = to_keep + list(range(border[i]))\n",
    "#     elif (border[i + 1] - border[i]) > 30:\n",
    "#         to_keep = to_keep + list(range(border[i], border[i+1]))\n",
    "# if len(outcome)-border[-1]>30:\n",
    "#     to_keep = to_keep + list(range(border[-1],len(outcome)))\n",
    "color = np.array(segmented_data[chunk][1]['axes'][0]['data']['CueColour']).flatten()[flag]\n",
    "target = np.array(segmented_data[chunk][1]['axes'][0]['data']['TargetRule']).flatten()[flag]\n",
    "classes = np.array(segmented_data[chunk][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "# X_rates = X_rates[to_keep]\n",
    "# Y = Y[to_keep]\n",
    "# Y_class = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "# color = color[to_keep]\n",
    "# target = target[to_keep]\n",
    "# classes = classes[to_keep]\n",
    "\n",
    "# block = block[to_keep]\n",
    "# b=np.diff(block, axis=0)\n",
    "# border=np.array(np.where(b>0)).flatten()\n",
    "# to_keep = [0]\n",
    "# for i in range(len(border) - 1):\n",
    "#     if (border[i + 1] - border[i]) > 30 and (len(outcome)-border[i+1]) > 30:\n",
    "#         to_keep.append(i + 1)\n",
    "# border = border[to_keep]\n",
    "times = np.array(segmented_data[chunk][1]['axes'][1]['times']).flatten()\n",
    "idx = np.argwhere(times < 1.45).flatten()[-1]\n",
    "rule = np.zeros(np.size(X_rates,0))\n",
    "for i in range(len(rule)):\n",
    "    if (target[i]=='UU'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=0\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=1\n",
    "        else:\n",
    "            rule[i]=2\n",
    "    elif (target[i]=='UR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=3\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=4\n",
    "        else:\n",
    "            rule[i]=5\n",
    "    elif (target[i]=='RR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=6\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=7\n",
    "        else:\n",
    "            rule[i]=8\n",
    "    elif (target[i]=='DR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=9\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=10\n",
    "        else:\n",
    "            rule[i]=11\n",
    "    elif (target[i]=='DD'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=12\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=13\n",
    "        else:\n",
    "            rule[i]=14\n",
    "    elif (target[i]=='DL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=15\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=16\n",
    "        else:\n",
    "            rule[i]=17\n",
    "    elif (target[i]=='LL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=18\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=19\n",
    "        else:\n",
    "            rule[i]=20\n",
    "    elif (target[i]=='UL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=21\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=22\n",
    "        else:\n",
    "            rule[i]=23\n",
    "rule = rule.astype(int)\n",
    "#     tmp = rule\n",
    "#     _yu = np.unique(rule)\n",
    "#     for i in range(len(tmp)):\n",
    "#         rule[i] = np.where(_yu == tmp[i])[0][0]\n",
    "\n",
    "m_performance = np.zeros(len(outcome))\n",
    "cor = 0\n",
    "b=0\n",
    "tot = 25\n",
    "for i in range(tot):\n",
    "    if outcome[i]==0:\n",
    "        cor += 1\n",
    "\n",
    "m_performance[:tot] = 100 * (cor / tot)\n",
    "# for i in range(tot, len(outcome)):\n",
    "i = tot\n",
    "while i<len(outcome):\n",
    "    if i == border[b]:\n",
    "        cor = 0\n",
    "        for j in range(tot):\n",
    "            if outcome[i+j]==0:\n",
    "                cor += 1\n",
    "        m_performance[i:i+tot] = 100 * (cor / tot)\n",
    "        i += tot\n",
    "        b = (b+1)%len(border)\n",
    "    elif outcome[i] == outcome[i-tot]:\n",
    "        m_performance[i] = m_performance[i-1]\n",
    "        i += 1\n",
    "    elif outcome[i]==0:\n",
    "        cor += 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i += 1\n",
    "    else:\n",
    "        cor -= 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i +=1\n",
    "\n",
    "# for i in range(len(m_performance)):\n",
    "#     if m_performance[i]<0:\n",
    "#         m_performance[i] = 0\n",
    "learned = np.argwhere(m_performance>75).flatten()\n",
    "unlearned = np.argwhere(m_performance<65).flatten()\n",
    "cor = np.array(np.where(outcome==0)).flatten()\n",
    "icor = np.array(np.where(outcome==9)).flatten()\n",
    "ic_l = []\n",
    "c_l = []\n",
    "ic_ul = []\n",
    "for c in cor:\n",
    "    if c in learned:\n",
    "        c_l.append(c)\n",
    "for ic in icor:\n",
    "    if ic in unlearned:\n",
    "        ic_ul.append(ic)\n",
    "    else:\n",
    "        ic_l.append(ic)\n",
    "X_rates = X_rates[:, :, :idx+1]\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "first = np.zeros(3)\n",
    "\n",
    "\n",
    "for i in range(len(m_performance)):\n",
    "    if m_performance[i] > 75:\n",
    "        if (first[0] == 0):\n",
    "            ax.plot(i, m_performance[i], 'go-', linewidth=2, label='Rule Acquired')\n",
    "            first[0] = 1\n",
    "        else:\n",
    "            ax.plot(i, m_performance[i], 'go-', linewidth=2)\n",
    "\n",
    "\n",
    "    elif m_performance[i] < 65:\n",
    "        if (first[1] == 0):\n",
    "            ax.plot(i, m_performance[i], 'ro-', linewidth=2, label='Rule Not Acquired')\n",
    "            first[1] = 1\n",
    "        else:\n",
    "            ax.plot(i, m_performance[i], 'ro-', linewidth=2)\n",
    "    else:\n",
    "        if (first[2] == 0):\n",
    "            ax.plot(i, m_performance[i], 'bo-', linewidth=2, label='Learning')\n",
    "            first[2] = 1\n",
    "        else:\n",
    "            ax.plot(i, m_performance[i], 'bo-', linewidth=2)\n",
    "\n",
    "for i in range(len(border)):\n",
    "    ax.vlines(x=border[i], ymin=0, ymax=110, linestyles='dashed', colors='grey')\n",
    "\n",
    "ax.hlines(y=65, xmin=0, xmax=len(m_performance)+1, linestyles='dashed', colors='grey')\n",
    "ax.hlines(y=75, xmin=0, xmax=len(m_performance)+1, linestyles='dashed', colors='grey')\n",
    "\n",
    "plt.ylabel(\"Monkey Performance (%)\")\n",
    "plt.xlabel(\"Trial #\")\n",
    "ax.legend(loc='lower right', fontsize='small')\n",
    "b = [0] + list(border)\n",
    "b.append(len(outcome))\n",
    "for i in range(len(b)-1):\n",
    "    print(f'Block: {i+1}')\n",
    "    u, count = np.unique(target[b[i]:b[i+1]-1], return_counts=True)\n",
    "    count_sort_ind = np.argsort(-count)\n",
    "    targ = u[count_sort_ind]\n",
    "    u, count = np.unique(color[b[i]:b[i+1]-1], return_counts=True)\n",
    "    count_sort_ind = np.argsort(-count)\n",
    "    colr = u[count_sort_ind]\n",
    "    u, count = np.unique(rule[b[i]:b[i+1]-1], return_counts=True)\n",
    "    count_sort_ind = np.argsort(-count)\n",
    "    rul = u[count_sort_ind]\n",
    "    print(f'Rule {rul[0]}: {targ[0]} & {colr[0]}')\n",
    "    print(f'Rule {rul[1]}: {targ[1]} & {colr[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc425c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=[]\n",
    "block = 1\n",
    "for c in learned:\n",
    "    if c > b[block-1] and c < b[block]:\n",
    "        idx.append(c)\n",
    "x = X_rates[idx,:,:].reshape(len(idx),-1, order='F')\n",
    "y = rule[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622108d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "ls = pca.fit_transform(x)\n",
    "for i in range(len(ls)):\n",
    "    if y[i] == 2:\n",
    "        c = 'green'\n",
    "    else:\n",
    "        c='blue'\n",
    "    plt.plot(ls[i,0],ls[i,1],marker='o',color=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=[]\n",
    "b_id = 0\n",
    "for c in learned:\n",
    "    if c > border[b_id] and c < border[b_id+1]:\n",
    "        idx.append(c)\n",
    "x = X_rates[idx,:,:].reshape(len(idx),-1, order='F')\n",
    "y = rule[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca2 = PCA(n_components=2)\n",
    "ls2 = pca.fit_transform(x)\n",
    "\n",
    "for i in range(len(ls2)):\n",
    "    if y[i] == 2:\n",
    "        c = 'lime'\n",
    "    else:\n",
    "        c='cyan'\n",
    "    plt.plot(ls2[i,0],ls2[i,1],marker='o',color=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=[]\n",
    "block = 1\n",
    "for c in learned:\n",
    "    if c > b[block-1] and c < b[block]:\n",
    "        idx.append(c)\n",
    "x = X_rates[idx,:,:].reshape(len(idx),-1, order='F')\n",
    "y = rule[idx]\n",
    "pca = PCA(n_components=2)\n",
    "ls = pca.fit_transform(x)\n",
    "for i in range(len(ls)):\n",
    "    if y[i] == 2:\n",
    "        c = 'green'\n",
    "    else:\n",
    "        c='blue'\n",
    "    plt.plot(ls[i,0],ls[i,1],marker='o',color=c)\n",
    "idx = np.argwhere(y==2).flatten()\n",
    "green = np.array(ls[idx])\n",
    "g_cent = np.mean(green, axis=0)\n",
    "idx=[]\n",
    "b_id = 0\n",
    "for c in learned:\n",
    "    if c > border[b_id] and c < border[b_id+1]:\n",
    "        idx.append(c)\n",
    "x = X_rates[idx,:,:].reshape(len(idx),-1, order='F')\n",
    "y = rule[idx]\n",
    "ls2 = pca.fit_transform(x)\n",
    "for i in range(len(ls2)):\n",
    "    if y[i] == 2:\n",
    "        c = 'lime'\n",
    "    else:\n",
    "        c='cyan'\n",
    "    plt.plot(ls2[i,0],ls2[i,1],marker='o',color=c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=[]\n",
    "block = 4\n",
    "for c in learned:\n",
    "    if c > b[block-1] and c < b[block]:\n",
    "        idx.append(c)\n",
    "x = X_rates[idx,:,:].reshape(len(idx),-1, order='F')\n",
    "y = rule[idx]\n",
    "pca = PCA(n_components=2)\n",
    "ls = pca.fit_transform(x)\n",
    "for i in range(len(ls)):\n",
    "    if y[i] == 18:\n",
    "        c = 'green'\n",
    "    else:\n",
    "        c='red'\n",
    "    plt.plot(ls[i,0],ls[i,1],marker='o',color=c)\n",
    "\n",
    "idx=[]\n",
    "block = 5\n",
    "for c in learned:\n",
    "    if c > b[block-1] and c < b[block]:\n",
    "        idx.append(c)\n",
    "x = X_rates[idx,:,:].reshape(len(idx),-1, order='F')\n",
    "y = rule[idx]\n",
    "ls2 = pca.fit_transform(x)\n",
    "for i in range(len(ls2)):\n",
    "    if y[i] == 18:\n",
    "        c = 'lime'\n",
    "    else:\n",
    "        c='pink'\n",
    "    plt.plot(ls2[i,0],ls2[i,1],marker='o',color=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-beverage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls2 = pca2.fit_transform(x)\n",
    "for i in range(len(ls2)):\n",
    "    if rule[i + border[6]] == 23:\n",
    "        c = 'cyan'\n",
    "    else:\n",
    "        c='green'\n",
    "    plt.plot(ls2[i,0],ls2[i,1],marker='o',color=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ac847",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.unique(rule, return_index=True)[1]\n",
    "[rule[index] for index in sorted(indexes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2432e68",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f62638",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "# accuracies = []\n",
    "# accuracies_chance = []\n",
    "\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, _, rule, _, _, c_l, _ = load_session(sess)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = rule[c_l]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    Y_shuf = np.random.permutation(Y_class)\n",
    "\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_class[train_index], Y_class[test_index]\n",
    "            clf = SVC(random_state=0, verbose=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc.append(100 * np.sum(predicted == Y_class.flatten()) / len(predicted))\n",
    "    acc_m = np.mean(np.array(acc))\n",
    "    acc_e = np.std(np.array(acc))\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc_shuf=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_shuf[train_index], Y_shuf[test_index]\n",
    "            clf = SVC(random_state=0, verbose=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc_shuf.append(100 * np.sum(predicted == Y_shuf.flatten()) / len(predicted))\n",
    "    acc_shuf_m = np.mean(np.array(acc_shuf))\n",
    "    acc_shuf_e = np.std(np.array(acc_shuf))\n",
    "    print(f\"{sess_id} overal average accuracy: {acc_m} with {acc_e} error.\")\n",
    "    print(f\"{sess_id} chance average accuracy: {acc_shuf_m} with {acc_shuf_e} error\\n\\n\")\n",
    "\n",
    "#     accuracies.append(acc)\n",
    "#     accuracies_chance.append(acc_shuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d6c53",
   "metadata": {},
   "source": [
    "# RLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d53850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "# accuracies = []\n",
    "# accuracies_chance = []\n",
    "\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, _, rule, _, _, c_l, _ = load_session(sess)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = rule[c_l]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    Y_shuf = np.random.permutation(Y_class)\n",
    "\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_class[train_index], Y_class[test_index]\n",
    "            clf = LR(random_state=0, verbose=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc.append(100 * np.sum(predicted == Y_class.flatten()) / len(predicted))\n",
    "    acc_m = np.mean(np.array(acc))\n",
    "    acc_e = np.std(np.array(acc))\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc_shuf=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_shuf[train_index], Y_shuf[test_index]\n",
    "            clf = LR(random_state=0, verbose=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc_shuf.append(100 * np.sum(predicted == Y_shuf.flatten()) / len(predicted))\n",
    "    acc_shuf_m = np.mean(np.array(acc_shuf))\n",
    "    acc_shuf_e = np.std(np.array(acc_shuf))\n",
    "    print(f\"{sess_id} overal average accuracy: {acc_m} with {acc_e} error.\")\n",
    "    print(f\"{sess_id} chance average accuracy: {acc_shuf_m} with {acc_shuf_e} error\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260f7b0",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "# accuracies = []\n",
    "# accuracies_chance = []\n",
    "\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, _, rule, _, _, c_l, _ = load_session(sess)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = rule[c_l]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    Y_shuf = np.random.permutation(Y_class)\n",
    "\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_class[train_index], Y_class[test_index]\n",
    "            clf = RandomForestClassifier(max_depth=6, random_state=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc.append(100 * np.sum(predicted == Y_class.flatten()) / len(predicted))\n",
    "    acc_m = np.mean(np.array(acc))\n",
    "    acc_e = np.std(np.array(acc))\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc_shuf=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_shuf[train_index], Y_shuf[test_index]\n",
    "            clf = RandomForestClassifier(max_depth=6, random_state=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc_shuf.append(100 * np.sum(predicted == Y_shuf.flatten()) / len(predicted))\n",
    "    acc_shuf_m = np.mean(np.array(acc_shuf))\n",
    "    acc_shuf_e = np.std(np.array(acc_shuf))\n",
    "    print(f\"{sess_id} overal average accuracy: {acc_m} with {acc_e} error.\")\n",
    "    print(f\"{sess_id} chance average accuracy: {acc_shuf_m} with {acc_shuf_e} error\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b13ec0",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "verbose=0\n",
    "N_SPLITS = 10\n",
    "EPOCHS = 100\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, _, rule, _, _, c_l, _ = load_session(sess)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = rule[c_l]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    print(f'\\nSess: {sess}')\n",
    "    _, tmp, _, _ = kfold_pred(sess_id,X_rates,Y_class,name=f'r2r_{sess}', verbose=verbose)\n",
    "    accs.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accs_r2r.pkl', 'rb') as f:\n",
    "    accs = np.array(pickle.load(f))\n",
    "with open('accs_r2r_chance.pkl', 'rb') as f:\n",
    "    accs_shuf = np.array(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    sess_info = sess_infos[i]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    acc = accs[i]\n",
    "    acc_shuf = accs_shuf[i]\n",
    "    acc_m = np.mean(np.array(acc))\n",
    "    acc_e = np.std(np.array(acc))\n",
    "    acc_shuf_m = np.mean(np.array(acc_shuf))\n",
    "    acc_shuf_e = np.std(np.array(acc_shuf))\n",
    "    print(f\"{sess_id} overal average accuracy: {acc_m} with {acc_e} error.\")\n",
    "    print(f\"{sess_id} chance average accuracy: {acc_shuf_m} with {acc_shuf_e} error\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(8)+1.7e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 13}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "sessions = ['JL1', 'JL2', 'JL3', 'M1', 'M2', 'M3', 'M4', 'JL4']\n",
    "x = np.arange(0, 2*len(sessions), 2)  # the label locations\n",
    "width = 0.3\n",
    "acc_dnn = np.array([59.88, 82.14, 71.34, 73.2, 60.61, 44.88, 67, 55.45])\n",
    "acc_svm = np.array([49.15, 66.95, 53.48, 59.2, 45.07, 33.07, 56.67, 44.55])\n",
    "acc_rlr = np.array([51.98, 74.29, 57.94, 65.2, 57.75, 29.92, 66.67, 50.49])\n",
    "acc_rf = np.array([48.59, 60.17, 60.45, 58.4, 44.37, 37.8, 66.67, 46.53])\n",
    "\n",
    "err_dnn = np.array([1.11, 0.87, 0.82, 0.92, 1.36, 1.47, 2.74, 1.90])\n",
    "err_svm = np.zeros(8)+1.7e-9\n",
    "err_rlr = np.zeros(8)+1.7e-9\n",
    "err_rf = np.zeros(8)+1.7e-9\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "rects1 = ax.bar(x - 3*width/2, acc_dnn, width, label='DNN')\n",
    "rects2 = ax.bar(x - width/2, acc_rf, width, label='RF')\n",
    "rects4 = ax.bar(x + width/2, acc_svm, width, label='SVM')\n",
    "rects3 = ax.bar(x + 3*width/2, acc_rlr, width, label='RLR')\n",
    "plt.errorbar(x - 3*width/2, acc_dnn, yerr=err_dnn, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x - width/2, acc_rf, yerr=err_rf, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x + width/2, acc_svm, yerr=err_svm, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x + 3*width/2, acc_rlr, yerr=err_rlr, fmt='.', markersize='10', capsize=5, color='black')\n",
    "\n",
    "ax.set_ylim([0, 95])\n",
    "ax.set_ylabel('Rule Decoding Accuracy (%)')\n",
    "ax.set_xlabel('Session')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sessions)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.435, .785), fontsize='small')\n",
    "fig.savefig(\"rule_performance.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ba1b8",
   "metadata": {},
   "source": [
    "# Saccade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_MAX = 1.45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244c18c",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "# accuracies = []\n",
    "# accuracies_chance = []\n",
    "\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, Y, _, _, _, c_l, _ = load_session(sess,t_end=T_MAX)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = Y[c_l]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    Y_shuf = np.random.permutation(Y_class)\n",
    "\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_class[train_index], Y_class[test_index]\n",
    "            clf = SVC(random_state=0, verbose=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc.append(100 * np.sum(predicted == Y_class.flatten()) / len(predicted))\n",
    "    acc_m = np.mean(np.array(acc))\n",
    "    acc_e = np.std(np.array(acc))\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc_shuf=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_shuf[train_index], Y_shuf[test_index]\n",
    "            clf = SVC(random_state=0, verbose=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc_shuf.append(100 * np.sum(predicted == Y_shuf.flatten()) / len(predicted))\n",
    "    acc_shuf_m = np.mean(np.array(acc_shuf))\n",
    "    acc_shuf_e = np.std(np.array(acc_shuf))\n",
    "    print(f\"{sess_id} overal average accuracy: {acc_m} with {acc_e} error.\")\n",
    "    print(f\"{sess_id} chance average accuracy: {acc_shuf_m} with {acc_shuf_e} error\\n\\n\")\n",
    "\n",
    "#     accuracies.append(acc)\n",
    "#     accuracies_chance.append(acc_shuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9be76a",
   "metadata": {},
   "source": [
    "# RLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4524f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "# accuracies = []\n",
    "# accuracies_chance = []\n",
    "\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, Y, _, _, _, c_l, _ = load_session(sess,t_end=T_MAX)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = Y[c_l]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    Y_shuf = np.random.permutation(Y_class)\n",
    "\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_class[train_index], Y_class[test_index]\n",
    "            clf = LR(random_state=0, verbose=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc.append(100 * np.sum(predicted == Y_class.flatten()) / len(predicted))\n",
    "    acc_m = np.mean(np.array(acc))\n",
    "    acc_e = np.std(np.array(acc))\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc_shuf=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_shuf[train_index], Y_shuf[test_index]\n",
    "            clf = LR(random_state=0, verbose=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc_shuf.append(100 * np.sum(predicted == Y_shuf.flatten()) / len(predicted))\n",
    "    acc_shuf_m = np.mean(np.array(acc_shuf))\n",
    "    acc_shuf_e = np.std(np.array(acc_shuf))\n",
    "    print(f\"{sess_id} overal average accuracy: {acc_m} with {acc_e} error.\")\n",
    "    print(f\"{sess_id} chance average accuracy: {acc_shuf_m} with {acc_shuf_e} error\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59ece0",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a57139",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "# accuracies = []\n",
    "# accuracies_chance = []\n",
    "\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, Y, _, _, _, c_l, _ = load_session(sess,t_end=T_MAX)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = Y[c_l]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    Y_shuf = np.random.permutation(Y_class)\n",
    "\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_class[train_index], Y_class[test_index]\n",
    "            clf = RandomForestClassifier(max_depth=6, random_state=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc.append(100 * np.sum(predicted == Y_class.flatten()) / len(predicted))\n",
    "    acc_m = np.mean(np.array(acc))\n",
    "    acc_e = np.std(np.array(acc))\n",
    "    \n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    acc_shuf=[]\n",
    "    for run in range(20):\n",
    "        i = 1\n",
    "        tmp = []\n",
    "        kf = KFold(n_splits=N_SPLITS)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            x_tr, x_ts = X[train_index], X[test_index]\n",
    "            y_tr, y_ts = Y_shuf[train_index], Y_shuf[test_index]\n",
    "            clf = RandomForestClassifier(max_depth=6, random_state=0).fit(x_tr, y_tr)\n",
    "            y_prd = clf.predict(x_ts)\n",
    "            tmp.append(y_prd)\n",
    "        #     print (f'Fold #{i} Accuracy is: {acc}')\n",
    "            i+=1\n",
    "\n",
    "        predicted = tmp[0]\n",
    "        for i in range(1, N_SPLITS):\n",
    "            predicted = np.concatenate((predicted, tmp[i]))\n",
    "\n",
    "        acc_shuf.append(100 * np.sum(predicted == Y_shuf.flatten()) / len(predicted))\n",
    "    acc_shuf_m = np.mean(np.array(acc_shuf))\n",
    "    acc_shuf_e = np.std(np.array(acc_shuf))\n",
    "    print(f\"{sess_id} overal average accuracy: {acc_m} with {acc_e} error.\")\n",
    "    print(f\"{sess_id} chance average accuracy: {acc_shuf_m} with {acc_shuf_e} error\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e72a99a",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "verbose=0\n",
    "N_SPLITS = 10\n",
    "EPOCHS = 100\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, Y, _, _, _, c_l, _ = load_session(sess,t_end=T_MAX)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = Y[c_l]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    print(f'\\nSess: {sess}')\n",
    "    _, tmp, _, _ = kfold_pred(sess_id,X_rates,Y_class,name=f'r2s_{sess}', verbose=verbose)\n",
    "    accs.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ea976",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "verbose=0\n",
    "N_SPLITS = 10\n",
    "EPOCHS = 100\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, Y, _, _, _, c_l, _ = load_session(sess,t_end=T_MAX)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = Y[c_l]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    print(f'\\nSess: {sess}')\n",
    "    _, tmp, _, _ = kfold_pred(sess_id,X_rates,Y_class,name=f'r2s_{sess}', verbose=verbose)\n",
    "    accs.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "verbose=0\n",
    "N_SPLITS = 10\n",
    "EPOCHS = 100\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "    X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "#     tmp = Y_class\n",
    "#     _yu = np.unique(Y_class)\n",
    "#     for i in range(len(tmp)):\n",
    "#         Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    print(f'\\nSess: {sess}')\n",
    "    _, tmp, _, _ = kfold_pred(sess_id,X_rates,Y_class,name=f'r2s_{sess}', verbose=verbose)\n",
    "    accs.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b503af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs1 = dict(\n",
    "    filt=8,\n",
    "    kernLength=20,\n",
    "#     ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f55db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model2(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=8,\n",
    "    kernLength=25,\n",
    "    ds_rate=10,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.25,\n",
    "    activation='relu',\n",
    "    l1_reg=0.000, l2_reg=0.000,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=16,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "    input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "    if len(input_shape) < 4:\n",
    "        input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv2D(filt, (1, kernLength), padding='valid', data_format=None,\n",
    "                                dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "                                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.DepthwiseConv2D((_y.shape.as_list()[1], 1), padding='valid',\n",
    "                                      depth_multiplier=1, data_format=None, dilation_rate=(1, 1),\n",
    "                                      activation=None, use_bias=True, depthwise_initializer='glorot_uniform',\n",
    "                                      bias_initializer='zeros', depthwise_regularizer=None,\n",
    "                                      bias_regularizer=None, activity_regularizer=None,\n",
    "                                      depthwise_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.AveragePooling2D(pool_size=(1, ds_rate))(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.Reshape(_y.shape.as_list()[2:])(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = tf.keras.layers.Dense(latent_dim, activation=activation)(_y)\n",
    "#     _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(_y)\n",
    "#     outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hists_acc(sess_id, verbose=1):\n",
    "    print(f\"Processing session {sess_id}...\")\n",
    "    X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "    \n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        randseed = 12345\n",
    "        random.seed(randseed)\n",
    "        np.random.seed(randseed)\n",
    "        tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model2(X_rates, _y.shape[-1], **model_kwargs)\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'r2c_lstm_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"\\n\\nSession {sess_id} overall accuracy with CNN/LSTM Model: {accuracy}%\")\n",
    "    \n",
    "    return history, accuracy, pred_y, true_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea48748",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "verbose=0\n",
    "N_SPLITS = 10\n",
    "EPOCHS = 100\n",
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "#     X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "#     tmp = Y_class\n",
    "#     _yu = np.unique(Y_class)\n",
    "#     for i in range(len(tmp)):\n",
    "#         Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    _, tmp, _, _ = get_hists_acc(sess_id, verbose=verbose)\n",
    "    accs.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cf059",
   "metadata": {},
   "source": [
    "# Rule Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "verbose=0\n",
    "N_SPLITS = 10\n",
    "EPOCHS = 100\n",
    "for sess in [3]:\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, _, rule, _, _, c_l, _ = load_session(sess)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = rule[c_l]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    print(f'\\nSess: {sess}')\n",
    "    _, _, y_pred, y_true = kfold_pred(sess_id,X_rates,Y_class,name=f'r2r_{sess}', verbose=verbose)\n",
    "    b = np.zeros((np.max(Y_class)+1,np.max(Y_class)+1))\n",
    "    for i in range(len(y_true)):\n",
    "        b[y_true[i], y_pred[i]] += 1\n",
    "    print(f'{b}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e8dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=3\n",
    "sess_info = sess_infos[sess]\n",
    "sess_id = sess_info['exp_code']\n",
    "X_rates, _, rule, _, _, c_l, _ = load_session(sess)\n",
    "X_rates = X_rates[c_l]\n",
    "Y_class = rule[c_l]\n",
    "tmp = Y_class\n",
    "_yu = np.unique(Y_class)\n",
    "for i in range(len(tmp)):\n",
    "    Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "b = np.zeros((int(np.max(Y_class))+1,int(np.max(Y_class))+1))\n",
    "for i in range(len(y_true)):\n",
    "    b[y_true[i], y_pred[i]] += 1\n",
    "print(f'{b}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68542bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=b[[4,9,6,1,7,3,8,2,5,0],:]\n",
    "c[:,[4,9,6,1,7,3,8,2,5,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rule)):\n",
    "        if (target[i]=='UU'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=0\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=1\n",
    "            else:\n",
    "                rule[i]=2\n",
    "        elif (target[i]=='UR'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=3\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=4\n",
    "            else:\n",
    "                rule[i]=5\n",
    "        elif (target[i]=='RR'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=6\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=7\n",
    "            else:\n",
    "                rule[i]=8\n",
    "        elif (target[i]=='DR'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=9\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=10\n",
    "            else:\n",
    "                rule[i]=11\n",
    "        elif (target[i]=='DD'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=12\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=13\n",
    "            else:\n",
    "                rule[i]=14\n",
    "        elif (target[i]=='DL'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=15\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=16\n",
    "            else:\n",
    "                rule[i]=17\n",
    "        elif (target[i]=='LL'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=18\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=19\n",
    "            else:\n",
    "                rule[i]=20\n",
    "        elif (target[i]=='UL'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=21\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=22\n",
    "            else:\n",
    "                rule[i]=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sess in [1, 2, 3]:\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, _, rule, _, _, c_l, _ = load_session(sess)\n",
    "    X_rates = X_rates[c_l]\n",
    "    Y_class = rule[c_l]\n",
    "    print(f'{np.unique(Y_class)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_idx = 3\n",
    "t = np.inf\n",
    "sess_info = sess_infos[s_idx]\n",
    "sess_id = sess_info['exp_code']\n",
    "sess_id = sess_id.replace(\"+\", \"\")+\"_v1\" + \"_segmented.h5\"\n",
    "segmented_path = data_path / sess_id\n",
    "segmented_data = from_neuropype_h5(segmented_path)\n",
    "outcome = np.array(segmented_data[2][1]['axes'][0]['data']['OutcomeCode'])\n",
    "flag = np.argwhere(outcome>-1).flatten()\n",
    "outcome = outcome[flag]\n",
    "Y = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "Y_class = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "X_rates = segmented_data[2][1]['data'][flag]\n",
    "X_rates = np.nan_to_num(X_rates)\n",
    "X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "block = np.array(segmented_data[2][1]['axes'][0]['data']['Block']).flatten()[flag]\n",
    "b=np.diff(block, axis=0)\n",
    "border=np.array(np.where(b>0)).flatten()\n",
    "to_keep = [0]\n",
    "for i in range(len(border) - 1):\n",
    "    if (border[i + 1] - border[i]) > 30 and (len(outcome)-border[i+1]) > 30:\n",
    "        to_keep.append(i + 1)\n",
    "border = border[to_keep]\n",
    "color = np.array(segmented_data[2][1]['axes'][0]['data']['CueColour']).flatten()[flag]\n",
    "target = np.array(segmented_data[2][1]['axes'][0]['data']['TargetRule']).flatten()[flag]\n",
    "classes = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "times = np.array(segmented_data[2][1]['axes'][1]['times']).flatten()\n",
    "idx = np.argwhere(times < t).flatten()[-1]\n",
    "rule = np.zeros(np.size(X_rates,0))\n",
    "for i in range(len(rule)):\n",
    "    if (target[i]=='UU'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=0\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=1\n",
    "        else:\n",
    "            rule[i]=2\n",
    "    elif (target[i]=='UR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=3\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=4\n",
    "        else:\n",
    "            rule[i]=5\n",
    "    elif (target[i]=='RR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=6\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=7\n",
    "        else:\n",
    "            rule[i]=8\n",
    "    elif (target[i]=='DR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=9\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=10\n",
    "        else:\n",
    "            rule[i]=11\n",
    "    elif (target[i]=='DD'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=12\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=13\n",
    "        else:\n",
    "            rule[i]=14\n",
    "    elif (target[i]=='DL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=15\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=16\n",
    "        else:\n",
    "            rule[i]=17\n",
    "    elif (target[i]=='LL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=18\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=19\n",
    "        else:\n",
    "            rule[i]=20\n",
    "    elif (target[i]=='UL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=21\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=22\n",
    "        else:\n",
    "            rule[i]=23\n",
    "rule = rule.astype(int)\n",
    "\n",
    "\n",
    "m_performance = np.zeros(len(outcome))\n",
    "cor = 0\n",
    "b=0\n",
    "tot = 25\n",
    "for i in range(tot):\n",
    "    if outcome[i]==0:\n",
    "        cor += 1\n",
    "\n",
    "m_performance[:tot] = 100 * (cor / tot)\n",
    "# for i in range(tot, len(outcome)):\n",
    "i = tot\n",
    "while i<len(outcome):\n",
    "    if i == border[b]:\n",
    "        cor = 0\n",
    "        for j in range(tot):\n",
    "            if outcome[i+j]==0:\n",
    "                cor += 1\n",
    "        m_performance[i:i+tot] = 100 * (cor / tot)\n",
    "        i += tot\n",
    "        b = (b+1)%len(border)\n",
    "    elif outcome[i] == outcome[i-tot]:\n",
    "        m_performance[i] = m_performance[i-1]\n",
    "        i += 1\n",
    "    elif outcome[i]==0:\n",
    "        cor += 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i += 1\n",
    "    else:\n",
    "        cor -= 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i +=1\n",
    "\n",
    "for i in range(len(m_performance)):\n",
    "    if m_performance[i]<0:\n",
    "        m_performance[i] = 0\n",
    "learned = np.argwhere(m_performance>75).flatten()\n",
    "unlearned = np.argwhere(m_performance<65).flatten()\n",
    "cor = np.array(np.where(outcome==0)).flatten()\n",
    "icor = np.array(np.where(outcome==9)).flatten()\n",
    "c_l = []\n",
    "ic_ul = []\n",
    "for c in cor:\n",
    "    if c in learned:\n",
    "        c_l.append(c)\n",
    "for ic in icor:\n",
    "    if ic in unlearned:\n",
    "        ic_ul.append(ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33cb2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(rule[c_l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d676b0c",
   "metadata": {},
   "source": [
    "# Decoding Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab04bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_prob(sess_id,X_rates,Y_class, verbose=1):\n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=np.max(Y_class)+1)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        randseed = 12345\n",
    "        random.seed(randseed)\n",
    "        np.random.seed(randseed)\n",
    "        tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model(X_rates, _y.shape[-1], **model_kwargs1)\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'rule_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "#     pred_y = np.concatenate([_ in per_fold_eval])\n",
    "#     true_y = np.concatenate([_ in per_fold_true])\n",
    "#     accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "#     print(f\"\\n\\nSession {sess_id} overall accuracy with CNN/LSTM Model: {accuracy}%\")\n",
    "    \n",
    "    return history, per_fold_eval, per_fold_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8155117",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=3\n",
    "sess_info = sess_infos[sess]\n",
    "sess_id = sess_info['exp_code']\n",
    "X_rates, _, rule, _, _, c_l, _ = load_session(sess)\n",
    "X_rates = X_rates[c_l]\n",
    "Y_class = rule[c_l]\n",
    "tmp = Y_class\n",
    "_yu = np.unique(Y_class)\n",
    "for i in range(len(tmp)):\n",
    "    Y_class[i] = np.where(_yu == tmp[i])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9762d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, y_pred, y_true = kfold_prob(sess,X_rates,Y_class, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate(y_true).flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb1abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_score = y_pred[0]\n",
    "for i in range(1,10):\n",
    "    pred_score = np.concatenate((pred_score, y_pred[i]), axis=0)\n",
    "pred_true = np.concatenate(y_true).flatten()\n",
    "print(pred_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_prob = np.zeros((10,10))\n",
    "for i in range(np.max(pred_true)+1):\n",
    "    tot = np.sum(pred_score[np.argwhere(pred_true==i)])\n",
    "    tmp = np.sum(pred_score[np.argwhere(pred_true==i)], axis=0)\n",
    "    conf_prob[i] = tmp / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=conf_prob[[4,9,6,1,7,3,8,2,5,0],:]\n",
    "c[:,[4,9,6,1,7,3,8,2,5,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b9e86",
   "metadata": {},
   "source": [
    "## Saccade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafddda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=1\n",
    "T_MAX = 1.45\n",
    "sess_info = sess_infos[sess]\n",
    "sess_id = sess_info['exp_code']\n",
    "X_rates, Y, _, _, _, c_l, _ = load_session(sess, t_end=T_MAX)\n",
    "X_rates = X_rates[c_l]\n",
    "Y_class = Y[c_l].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9331e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(Y_class, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, y_pred, y_true = kfold_prob(sess,X_rates,Y_class, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_score = y_pred[0]\n",
    "for i in range(1,10):\n",
    "    pred_score = np.concatenate((pred_score, y_pred[i]), axis=0)\n",
    "pred_true = np.concatenate(y_true).flatten()\n",
    "print(pred_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_prob = np.zeros((8,8))\n",
    "for i in range(np.max(pred_true)+1):\n",
    "    tot = np.sum(pred_score[np.argwhere(pred_true==i)])\n",
    "    tmp = np.sum(pred_score[np.argwhere(pred_true==i)], axis=0)\n",
    "    conf_prob[i] = tmp / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ee47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_prob\n",
    "# For later reference\n",
    "# np.array([[0.61057454, 0.13116904, 0.02877125, 0.03104685, 0.08430207,\n",
    "#         0.04813107, 0.03413321, 0.03187194],\n",
    "#        [0.15647481, 0.46376145, 0.03999596, 0.03503183, 0.03967802,\n",
    "#         0.1933682 , 0.03643942, 0.03525044],\n",
    "#        [0.03082615, 0.03596596, 0.48336306, 0.05417614, 0.03351186,\n",
    "#         0.09694611, 0.21203832, 0.05317232],\n",
    "#        [0.03033455, 0.02876109, 0.13709609, 0.36654565, 0.03896224,\n",
    "#         0.03737871, 0.06020587, 0.3007158 ],\n",
    "#        [0.13669877, 0.05401517, 0.04108274, 0.04022946, 0.47931114,\n",
    "#         0.16145681, 0.04972582, 0.03748012],\n",
    "#        [0.06969361, 0.15155515, 0.06612401, 0.03395624, 0.07360219,\n",
    "#         0.52055192, 0.04377987, 0.04073698],\n",
    "#        [0.03189153, 0.04255129, 0.33918223, 0.04168984, 0.04045406,\n",
    "#         0.11035856, 0.33530557, 0.05856698],\n",
    "#        [0.03248762, 0.03251085, 0.09593348, 0.27562743, 0.04216746,\n",
    "#         0.04849968, 0.09832053, 0.37445298]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b2efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0.61057454, 0.13116904, 0.02877125, 0.03104685, 0.08430207,\n",
    "        0.04813107, 0.03413321, 0.03187194],\n",
    "       [0.15647481, 0.46376145, 0.03999596, 0.03503183, 0.03967802,\n",
    "        0.1933682 , 0.03643942, 0.03525044],\n",
    "       [0.03082615, 0.03596596, 0.48336306, 0.05417614, 0.03351186,\n",
    "        0.09694611, 0.21203832, 0.05317232],\n",
    "       [0.03033455, 0.02876109, 0.13709609, 0.36654565, 0.03896224,\n",
    "        0.03737871, 0.06020587, 0.3007158 ],\n",
    "       [0.13669877, 0.05401517, 0.04108274, 0.04022946, 0.47931114,\n",
    "        0.16145681, 0.04972582, 0.03748012],\n",
    "       [0.06969361, 0.15155515, 0.06612401, 0.03395624, 0.07360219,\n",
    "        0.52055192, 0.04377987, 0.04073698],\n",
    "       [0.03189153, 0.04255129, 0.33918223, 0.04168984, 0.04045406,\n",
    "        0.11035856, 0.33530557, 0.05856698],\n",
    "       [0.03248762, 0.03251085, 0.09593348, 0.27562743, 0.04216746,\n",
    "        0.04849968, 0.09832053, 0.37445298]])\n",
    "\n",
    "b = a[[4, 0, 3, 7, 5, 1, 6, 2], :]\n",
    "b[:, [4, 0, 3, 7, 5, 1, 6, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b13236",
   "metadata": {},
   "source": [
    "# Error Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e821b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174b4fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "T_MAX = 1.4\n",
    "accs_l = np.zeros((9,20))\n",
    "accs_ul = np.zeros((9,20))\n",
    "verbose=0\n",
    "EPOCHS = 50\n",
    "for sess in range(9):\n",
    "    print(f'\\n\\n Session: {sess}')\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, Y, _, _, _, c_l, ic_ul, ic_l = load_session(sess, t_end=T_MAX)\n",
    "    x_train = X_rates[c_l]\n",
    "    y_train = Y[c_l]\n",
    "    x_test_l = X_rates[ic_l]\n",
    "    y_test_l = Y[ic_l]\n",
    "    true_l = y_test_l.flatten()\n",
    "    x_test_ul = X_rates[ic_ul]\n",
    "    y_test_ul = Y[ic_ul]\n",
    "    true_ul = y_test_ul.flatten()\n",
    "    _y = tf.keras.utils.to_categorical(y_train, num_classes=8)\n",
    "\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((x_train, _y))\n",
    "\n",
    "    # cast data types to GPU-friendly types.\n",
    "    ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "    ds_train = ds_train.shuffle(len(x_train) + 1)\n",
    "    ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    for run in range(20):\n",
    "        print(f'Run: {run} @ {datetime.datetime.now()}')\n",
    "        tf.keras.backend.clear_session()\n",
    "        model = make_model(x_train, _y.shape[-1])\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        model.fit(x=ds_train, epochs=EPOCHS, verbose=0)\n",
    "        \n",
    "        pred_l = np.argmax(model.predict(x_test_l), axis=1)\n",
    "        accs_l[sess,run] = 100 * np.sum(pred_l == true_l) / len(pred_l)\n",
    "        print(f\"Accuracy On Learned Incorrect Trials: {accs_l[sess,run]}%\")\n",
    "        \n",
    "        pred_ul = np.argmax(model.predict(x_test_ul), axis=1)\n",
    "        accs_ul[sess,run] = 100 * np.sum(pred_ul == true_ul) / len(pred_ul)\n",
    "        print(f\"Accuracy On Unlearned Incorrect Trials: {accs_ul[sess,run]}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808abe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_l = accs_l[:,:20]\n",
    "accs_ul = accs_ul[:,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accs_l.pkl','wb') as f:\n",
    "    pickle.dump(accs_l, f)\n",
    "with open('accs_ul.pkl','wb') as f:\n",
    "    pickle.dump(accs_ul, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30091891",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accs_l.pkl', 'rb') as f:\n",
    "    accs_l = np.array(pickle.load(f))\n",
    "with open('accs_ul.pkl', 'rb') as f:\n",
    "    accs_ul = np.array(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model2(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=8,\n",
    "    kernLength=25,\n",
    "    ds_rate=10,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.25,\n",
    "    activation='relu',\n",
    "    l1_reg=0.000, l2_reg=0.000,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=16,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "    input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "    if len(input_shape) < 4:\n",
    "        input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv2D(filt, (1, kernLength), padding='valid', data_format=None,\n",
    "                                dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "                                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.DepthwiseConv2D((_y.shape.as_list()[1], 1), padding='valid',\n",
    "                                      depth_multiplier=1, data_format=None, dilation_rate=(1, 1),\n",
    "                                      activation=None, use_bias=True, depthwise_initializer='glorot_uniform',\n",
    "                                      bias_initializer='zeros', depthwise_regularizer=None,\n",
    "                                      bias_regularizer=None, activity_regularizer=None,\n",
    "                                      depthwise_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.AveragePooling2D(pool_size=(1, ds_rate))(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.Reshape(_y.shape.as_list()[2:])(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = tf.keras.layers.Dense(latent_dim, activation=activation)(_y)\n",
    "#     _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(_y)\n",
    "#     outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e19846",
   "metadata": {},
   "source": [
    "## Test on Learned Incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "lk_train = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "}\n",
    "\n",
    "lk_test = {\n",
    "    'valid_outcomes': (9, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "}\n",
    "lk_test = {\n",
    "    'valid_outcomes': (9, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9715c10",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "BATCH_SIZE=16\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    _y = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "        \n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_rates, _y))\n",
    "\n",
    "    # cast data types to GPU-friendly types.\n",
    "    ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "    # TODO: augmentations (random slicing?)\n",
    "\n",
    "    ds_train = ds_train.shuffle(len(X_rates) + 1)\n",
    "    ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    model = make_model2(X_rates, _y.shape[-1], **model_kwargs)\n",
    "    optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "    model.fit(x=ds_train, epochs=EPOCHS, verbose=0)\n",
    "    \n",
    "    X_err, Y_err, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    pred_y = np.argmax(model.predict(X_err), axis=1)\n",
    "    true_y = Y_err.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"RNN: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\\n\")\n",
    "    \n",
    "    _y = tf.keras.utils.to_categorical(Y_shuf, num_classes=8)\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_rates, _y))\n",
    "\n",
    "    # cast data types to GPU-friendly types.\n",
    "    ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "    # TODO: augmentations (random slicing?)\n",
    "\n",
    "    ds_train = ds_train.shuffle(len(X_rates) + 1)\n",
    "    ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    model = make_model2(X_rates, _y.shape[-1], **model_kwargs)\n",
    "    optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "    model.fit(x=ds_train, epochs=EPOCHS, verbose=0)\n",
    "    \n",
    "    pred_y = np.argmax(model.predict(X_err), axis=1)\n",
    "    true_y = Y_err.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"RNN: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "# [68.57142857 35.13513514 48.27586207 58.57142857 28.57142857 33.33333333\n",
    "#  27.77777778 34.48275862]\n",
    "print(chance)\n",
    "# [57.14285714 21.62162162 20.68965517 20.         23.80952381 17.94871795\n",
    "#  27.77777778 27.5862069 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a574a4d0",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bafac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = SVC(random_state=0, verbose=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = SVC(random_state=0, verbose=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204601bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "# [45.71428571 37.83783784 50.         57.14285714 47.61904762 43.58974359\n",
    "#  25.         20.68965517]\n",
    "print(chance)\n",
    "# 02702703 43.10344828 12.85714286 40.47619048 33.33333333\n",
    "#  16.66666667  6.89655172]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e429f39",
   "metadata": {},
   "source": [
    "### RLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a8e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a32363",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = LR(random_state=0, verbose=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = LR(random_state=0, verbose=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae21fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "# [48.57142857 32.43243243 51.72413793 54.28571429 42.85714286 25.64102564\n",
    "#  36.11111111 20.68965517]\n",
    "print(chance)\n",
    "# [34.28571429  8.10810811 18.96551724 22.85714286 23.80952381 25.64102564\n",
    "#  22.22222222 13.79310345]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda847b9",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = RandomForestClassifier(max_depth=6, random_state=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = RandomForestClassifier(max_depth=6, random_state=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "# [37.14285714 29.72972973 51.72413793 55.71428571 42.85714286 41.02564103\n",
    "#  11.11111111 17.24137931]\n",
    "print(chance)\n",
    "# [74.28571429 24.32432432 32.75862069 20.         26.19047619 33.33333333\n",
    "#  13.88888889 20.68965517]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb41c6",
   "metadata": {},
   "source": [
    "## Test on Unlearned Incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lk_train = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "}\n",
    "\n",
    "lk_test = {\n",
    "    'valid_outcomes': (9, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, 0),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    X_err, Y_err, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    print(X_rates.shape,X_err.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79540f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "BATCH_SIZE=16\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    _y = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "        \n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_rates, _y))\n",
    "\n",
    "    # cast data types to GPU-friendly types.\n",
    "    ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "    # TODO: augmentations (random slicing?)\n",
    "\n",
    "    ds_train = ds_train.shuffle(len(X_rates) + 1)\n",
    "    ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    model = make_model2(X_rates, _y.shape[-1], **model_kwargs)\n",
    "    optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "    model.fit(x=ds_train, epochs=EPOCHS, verbose=0)\n",
    "    \n",
    "    X_err, Y_err, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    pred_y = np.argmax(model.predict(X_err), axis=1)\n",
    "    true_y = Y_err.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"RNN: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\\n\")\n",
    "    \n",
    "    _y = tf.keras.utils.to_categorical(Y_shuf, num_classes=8)\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_rates, _y))\n",
    "\n",
    "    # cast data types to GPU-friendly types.\n",
    "    ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "    # TODO: augmentations (random slicing?)\n",
    "\n",
    "    ds_train = ds_train.shuffle(len(X_rates) + 1)\n",
    "    ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    model = make_model2(X_rates, _y.shape[-1], **model_kwargs)\n",
    "    optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "    model.fit(x=ds_train, epochs=EPOCHS, verbose=0)\n",
    "    \n",
    "    pred_y = np.argmax(model.predict(X_err), axis=1)\n",
    "    true_y = Y_err.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"RNN: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbfcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(chance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e99183",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fe4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = SVC(random_state=0, verbose=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = SVC(random_state=0, verbose=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(chance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ee223",
   "metadata": {},
   "source": [
    "### RLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7312c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6543e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = LR(random_state=0, verbose=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = LR(random_state=0, verbose=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c331b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(chance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707cec4",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = RandomForestClassifier(max_depth=6, random_state=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = RandomForestClassifier(max_depth=6, random_state=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(chance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20af4f1",
   "metadata": {},
   "source": [
    "## Test on Learned Incorrect including saccade period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lk_train = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "}\n",
    "\n",
    "lk_test = {\n",
    "    'valid_outcomes': (9, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2e76b",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8543556",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    _y = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "    \n",
    "\n",
    "    model = make_model2(X_rates, _y.shape[-1], **model_kwargs)\n",
    "    optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "    model.fit(x=X_rates,y=_y, epochs=EPOCHS, verbose=0)\n",
    "    \n",
    "    X_err, Y_err, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    pred_y = np.argmax(model.predict(X_err), axis=1)\n",
    "    true_y = Y_err.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"RNN: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\\n\")\n",
    "    \n",
    "    _y = tf.keras.utils.to_categorical(Y_shuf, num_classes=8)\n",
    "\n",
    "    model = make_model2(X_rates, _y.shape[-1], **model_kwargs)\n",
    "    optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "    model.fit(x=X_rates,y=_y, epochs=EPOCHS, verbose=0)\n",
    "    \n",
    "    X_err, Y_err, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    pred_y = np.argmax(model.predict(X_err), axis=1)\n",
    "    true_y = Y_err.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"RNN: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "# [45.71428571 97.2972973  77.5862069  97.14285714 80.95238095 43.58974359\n",
    "#  25.         51.72413793]\n",
    "print(chance)\n",
    "# [68.57142857 16.21621622 27.5862069  17.14285714 19.04761905 33.33333333\n",
    "#  16.66666667 31.03448276]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477df60",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cb96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = SVC(random_state=0, verbose=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = SVC(random_state=0, verbose=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb232f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "# [31.42857143 97.2972973  81.03448276 88.57142857 73.80952381 51.28205128\n",
    "#  25.         24.13793103]\n",
    "print(chance)\n",
    "# [48.57142857 18.91891892 10.34482759  7.14285714 40.47619048 38.46153846\n",
    "#  13.88888889  6.89655172]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e1326",
   "metadata": {},
   "source": [
    "### RLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee52279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = LR(random_state=0, verbose=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = LR(random_state=0, verbose=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "# [40.         97.2972973  82.75862069 92.85714286 69.04761905 46.15384615\n",
    "#  30.55555556 20.68965517]\n",
    "print(chance)\n",
    "# [51.42857143 13.51351351 18.96551724 14.28571429 16.66666667 12.82051282\n",
    "#  16.66666667 24.13793103]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac6a00",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = RandomForestClassifier(max_depth=6, random_state=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = RandomForestClassifier(max_depth=6, random_state=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "# [51.42857143 97.2972973  84.48275862 95.71428571 71.42857143 66.66666667\n",
    "#  30.55555556 20.68965517]\n",
    "print(chance)\n",
    "# [48.57142857 13.51351351 50.         17.14285714 14.28571429 43.58974359\n",
    "#   8.33333333  6.89655172]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a7fa7",
   "metadata": {},
   "source": [
    "## Test on Unlearned Incorrect including saccade period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d348408",
   "metadata": {},
   "outputs": [],
   "source": [
    "lk_train = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "}\n",
    "\n",
    "lk_test = {\n",
    "    'valid_outcomes': (9, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, 0),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    _y = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    randseed = 12345\n",
    "    random.seed(randseed)\n",
    "    np.random.seed(randseed)\n",
    "    tf.random.set_seed(randseed)\n",
    "\n",
    "    model = make_model2(X_rates, _y.shape[-1], **model_kwargs)\n",
    "    optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "    model.fit(x=X_rates,y=_y, epochs=EPOCHS, verbose=0)\n",
    "    \n",
    "    X_err, Y_err, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    pred_y = np.argmax(model.predict(X_err), axis=1)\n",
    "    true_y = Y_err.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"RNN: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\\n\")\n",
    "    \n",
    "    _y = tf.keras.utils.to_categorical(Y_shuf, num_classes=8)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    randseed = 12345\n",
    "    random.seed(randseed)\n",
    "    np.random.seed(randseed)\n",
    "    tf.random.set_seed(randseed)\n",
    "\n",
    "    model = make_model2(X_rates, _y.shape[-1], **model_kwargs)\n",
    "    optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "    model.fit(x=X_rates,y=_y, epochs=EPOCHS, verbose=0)\n",
    "    \n",
    "    X_err, Y_err, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    pred_y = np.argmax(model.predict(X_err), axis=1)\n",
    "    true_y = Y_err.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"RNN: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698625e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(chance)\n",
    "# [63.33333333 94.59459459 84.95575221 83.76623377 50.         38.33333333\n",
    "#  26.38888889 41.93548387]\n",
    "# [60.         14.41441441 17.69911504 14.93506494 12.5         5.\n",
    "#  19.44444444 19.35483871]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f8b7c",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = SVC(random_state=0, verbose=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = SVC(random_state=0, verbose=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdfe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(chance)\n",
    "\n",
    "# [53.33333333 94.59459459 82.30088496 73.37662338 37.5        23.33333333\n",
    "#  26.38888889 12.90322581]\n",
    "# [46.66666667  9.90990991 37.16814159  3.24675325 37.5        11.66666667\n",
    "#  23.61111111  3.22580645]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f151e7",
   "metadata": {},
   "source": [
    "### RLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = LR(random_state=0, verbose=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = LR(random_state=0, verbose=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f44d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(chance)\n",
    "\n",
    "# [53.33333333 96.3963964  84.95575221 77.92207792 31.25       36.66666667\n",
    "#  31.94444444 41.93548387]\n",
    "# [46.66666667 10.81081081 18.5840708  16.88311688 12.5        21.66666667\n",
    "#  27.77777778 19.35483871]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5824f49",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(8)\n",
    "chance = np.zeros(8)\n",
    "\n",
    "for i in range(8):\n",
    "    test_sess_ix = i\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"Importing session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = RandomForestClassifier(max_depth=6, random_state=0).fit(X, Y)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    accuracy[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Accuracy over Error Trials: {accuracy[i]}%\\n\")\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_train)\n",
    "    Y = Y.flatten()\n",
    "    Y_shuf = np.random.permutation(Y)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    clf = RandomForestClassifier(max_depth=6, random_state=0).fit(X, Y_shuf)\n",
    "    \n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **lk_test)\n",
    "    X = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "    pred_y = clf.predict(X)\n",
    "    true_y = Y.flatten()\n",
    "    chance[i] = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"SVM: Session {sess_id} Chance Accuracy over Error Trials: {chance[i]}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(chance)\n",
    "\n",
    "# [43.33333333 81.08108108 82.30088496 68.83116883 56.25       26.66666667\n",
    "#  31.94444444 12.90322581]\n",
    "# [60.         13.51351351 28.31858407 12.98701299 25.          8.33333333\n",
    "#  19.44444444 12.90322581]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9afefd",
   "metadata": {},
   "source": [
    "## Result Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'weight' : 'bold',\n",
    "        'size'   : 13}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "sessions = ['JL1', 'JL2', 'JL3', 'M1', 'M2', 'M3', 'M4', 'JL4']\n",
    "x = np.arange(0, 2*len(sessions), 2)  # the label locations\n",
    "width = 0.3\n",
    "acc_dnn = np.array([68.57142857, 35.13513514, 48.27586207, 58.57142857, 28.57142857, 33.33333333, 27.77777778, 34.48275862])\n",
    "acc_svm = np.array([45.71428571, 37.83783784, 50, 57.14285714, 47.61904762, 43.58974359, 25,20.68965517])\n",
    "acc_rlr = np.array([48.57142857,32.43243243, 51.72413793, 54.28571429, 42.85714286, 25.64102564, 36.11111111, 20.68965517])\n",
    "acc_rf = np.array([37.14285714, 29.72972973, 51.72413793, 55.71428571, 42.85714286, 41.02564103, 11.11111111, 17.24137931])\n",
    "\n",
    "chance_dnn = np.array([57.14285714, 21.62162162, 20.68965517, 20.,23.80952381,17.94871795, 27.77777778, 27.5862069 ])\n",
    "chance_svm = np.array([42.85714286, 27.02702703, 43.10344828, 12.85714286, 40.47619048, 33.33333333, 16.66666667,  6.89655172])\n",
    "chance_rlr = np.array([34.28571429,  8.10810811, 18.96551724, 22.85714286, 23.80952381, 25.64102564, 22.22222222, 13.79310345])\n",
    "chance_rf = np.array([74.28571429, 24.32432432, 32.75862069, 20.,26.19047619, 33.33333333, 13.88888889, 20.68965517])\n",
    "\n",
    "acc_dnn = acc_dnn / chance_dnn\n",
    "acc_svm = acc_svm / chance_svm\n",
    "acc_rlr = acc_rlr / chance_rlr\n",
    "acc_rf = acc_rf / chance_rf\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "rects1 = ax.bar(x - 3*width/2, acc_dnn, width, label='DNN')\n",
    "rects2 = ax.bar(x - width/2, acc_rf, width, label='RF')\n",
    "rects4 = ax.bar(x + width/2, acc_svm, width, label='SVM')\n",
    "rects3 = ax.bar(x + 3*width/2, acc_rlr, width, label='RLR')\n",
    "# plt.errorbar(x - 3*width/2, acc_dnn, yerr=err_dnn, fmt='.', markersize='10', capsize=5, color='black')\n",
    "# plt.errorbar(x - width/2, acc_rf, yerr=err_rf, fmt='.', markersize='10', capsize=5, color='black')\n",
    "# plt.errorbar(x + width/2, acc_svm, yerr=err_svm, fmt='.', markersize='10', capsize=5, color='black')\n",
    "# plt.errorbar(x + 3*width/2, acc_rlr, yerr=err_rlr, fmt='.', markersize='10', capsize=5, color='black')\n",
    "\n",
    "# ax.set_ylim([0, 95])\n",
    "ax.set_ylabel('Error Saccade Prediction Accuracy (%)')\n",
    "ax.set_xlabel('Session')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sessions)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.435, .785), fontsize='small')\n",
    "# fig.savefig(\"rule_performance.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'weight' : 'bold',\n",
    "        'size'   : 13}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "sessions = ['JL1', 'JL2', 'JL3', 'M1', 'M2', 'M3', 'M4', 'JL4']\n",
    "x = np.arange(0, 2*len(sessions), 2)  # the label locations\n",
    "width = 0.3\n",
    "# acc_dnn = np.array([60,50.45045045, 47.78761062, 48.05194805, 37.5,20, 26.38888889, 22.58064516])\n",
    "# acc_svm = np.array([46.66666667, 56.75675676, 53.98230088, 39.61038961, 18.75,18.33333333, 18.05555556, 9.67741935])\n",
    "# acc_rlr = np.array([56.66666667, 54.95495495, 53.98230088, 44.15584416, 18.75, 20., 33.33333333, 22.58064516])\n",
    "# acc_rf = np.array([53.33333333, 51.35135135, 46.01769912, 35.71428571, 18.75,16.66666667, 18.05555556, 12.90322581])\n",
    "\n",
    "# chance_dnn = np.array([43.33333333, 17.11711712, 20.3539823,  11.03896104, 18.75,        8.33333333, 27.77777778, 22.58064516])\n",
    "# chance_svm = np.array([46.66666667,  8.10810811, 36.28318584,  5.19480519, 31.25,       11.66666667, 23.61111111,  3.22580645])\n",
    "# chance_rlr = np.array([43.33333333, 18.91891892, 19.46902655, 14.93506494, 12.5,        20., 26.38888889, 22.58064516])\n",
    "# chance_rf = np.array([50.,10.81081081, 36.28318584, 13.63636364, 31.25,13.33333333, 19.44444444,  3.22580645])\n",
    "\n",
    "\n",
    "acc_dnn = np.array([68.57142857, 35.13513514, 48.27586207, 58.57142857, 28.57142857, 33.33333333, 27.77777778, 34.48275862])\n",
    "acc_svm = np.array([45.71428571, 37.83783784, 50, 57.14285714, 47.61904762, 43.58974359, 25,20.68965517])\n",
    "acc_rlr = np.array([48.57142857,32.43243243, 51.72413793, 54.28571429, 42.85714286, 25.64102564, 36.11111111, 20.68965517])\n",
    "acc_rf = np.array([37.14285714, 29.72972973, 51.72413793, 55.71428571, 42.85714286, 41.02564103, 11.11111111, 17.24137931])\n",
    "\n",
    "# chance_dnn2 = np.array([57.14285714, 21.62162162, 20.68965517, 20.,23.80952381,17.94871795, 27.77777778, 27.5862069 ])\n",
    "# chance_svm2 = np.array([42.85714286, 27.02702703, 43.10344828, 12.85714286, 40.47619048, 33.33333333, 16.66666667,  6.89655172])\n",
    "# chance_rlr2 = np.array([34.28571429,  8.10810811, 18.96551724, 22.85714286, 23.80952381, 25.64102564, 22.22222222, 13.79310345])\n",
    "# chance_rf2 = np.array([74.28571429, 24.32432432, 32.75862069, 20.,26.19047619, 33.33333333, 13.88888889, 20.68965517])\n",
    "\n",
    "# acc_dnn = (acc_dnn * chance_dnn2) / (chance_dnn * acc_dnn2)\n",
    "# acc_svm = (acc_svm * chance_svm2) / (chance_svm * acc_svm2)\n",
    "# acc_rlr = (acc_rlr * chance_rlr2) / (chance_rlr * acc_rlr2)\n",
    "# acc_rf = (acc_rf * chance_rf2) / (chance_rf * acc_rf2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "rects1 = ax.bar(x - 3*width/2, acc_dnn, width, label='DNN')\n",
    "rects2 = ax.bar(x - width/2, acc_rf, width, label='RF')\n",
    "rects4 = ax.bar(x + width/2, acc_svm, width, label='SVM')\n",
    "rects3 = ax.bar(x + 3*width/2, acc_rlr, width, label='RLR')\n",
    "# plt.errorbar(x - 3*width/2, acc_dnn, yerr=err_dnn, fmt='.', markersize='10', capsize=5, color='black')\n",
    "# plt.errorbar(x - width/2, acc_rf, yerr=err_rf, fmt='.', markersize='10', capsize=5, color='black')\n",
    "# plt.errorbar(x + width/2, acc_svm, yerr=err_svm, fmt='.', markersize='10', capsize=5, color='black')\n",
    "# plt.errorbar(x + 3*width/2, acc_rlr, yerr=err_rlr, fmt='.', markersize='10', capsize=5, color='black')\n",
    "\n",
    "# ax.set_ylim([0, 95])\n",
    "ax.set_ylabel('Error Saccade Prediction Accuracy (%)')\n",
    "ax.set_xlabel('Session')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sessions)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.435, .785), fontsize='small')\n",
    "# fig.savefig(\"rule_performance.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e04bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'weight' : 'bold',\n",
    "        'size'   : 13}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "sessions = ['JL1', 'JL2', 'JL3', 'M1', 'M2', 'M3', 'M4', 'JL4']\n",
    "x = np.arange(0, 2*len(sessions), 2)  # the label locations\n",
    "width = 0.3\n",
    "acc_dnn = np.array([59.88, 82.14, 71.34, 73.2, 60.61, 44.88, 67, 55.45])\n",
    "acc_svm = np.array([49.15, 66.95, 53.48, 59.2, 45.07, 33.07, 56.67, 44.55])\n",
    "acc_rlr = np.array([51.98, 74.29, 57.94, 65.2, 57.75, 29.92, 66.67, 50.49])\n",
    "acc_rf = np.array([48.59, 60.17, 60.45, 58.4, 44.37, 37.8, 66.67, 46.53])\n",
    "\n",
    "err_dnn = np.array([1.11, 0.87, 0.82, 0.92, 1.36, 1.47, 2.74, 1.90])\n",
    "err_svm = np.zeros(8)+1.7e-9\n",
    "err_rlr = np.zeros(8)+1.7e-9\n",
    "err_rf = np.zeros(8)+1.7e-9\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "rects1 = ax.bar(x - 3*width/2, acc_dnn, width, label='DNN')\n",
    "rects2 = ax.bar(x - width/2, acc_rf, width, label='RF')\n",
    "rects4 = ax.bar(x + width/2, acc_svm, width, label='SVM')\n",
    "rects3 = ax.bar(x + 3*width/2, acc_rlr, width, label='RLR')\n",
    "plt.errorbar(x - 3*width/2, acc_dnn, yerr=err_dnn, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x - width/2, acc_rf, yerr=err_rf, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x + width/2, acc_svm, yerr=err_svm, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x + 3*width/2, acc_rlr, yerr=err_rlr, fmt='.', markersize='10', capsize=5, color='black')\n",
    "\n",
    "ax.set_ylim([0, 95])\n",
    "ax.set_ylabel('Rule Decoding Accuracy (%)')\n",
    "ax.set_xlabel('Session')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sessions)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.435, .785), fontsize='small')\n",
    "fig.savefig(\"rule_performance.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'weight' : 'bold',\n",
    "        'size'   : 13}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "sessions = ['JL1', 'JL2', 'JL3', 'M1', 'M2', 'M3', 'M4', 'JL4']\n",
    "x = np.arange(0, 2*len(sessions), 2)  # the label locations\n",
    "width = 0.3\n",
    "acc_dnn = np.array([59.88, 82.14, 71.34, 73.2, 60.61, 44.88, 67, 55.45])\n",
    "acc_svm = np.array([49.15, 66.95, 53.48, 59.2, 45.07, 33.07, 56.67, 44.55])\n",
    "acc_rlr = np.array([51.98, 74.29, 57.94, 65.2, 57.75, 29.92, 66.67, 50.49])\n",
    "acc_rf = np.array([48.59, 60.17, 60.45, 58.4, 44.37, 37.8, 66.67, 46.53])\n",
    "\n",
    "err_dnn = np.array([1.11, 0.87, 0.82, 0.92, 1.36, 1.47, 2.74, 1.90])\n",
    "err_svm = np.zeros(8)+1.7e-9\n",
    "err_rlr = np.zeros(8)+1.7e-9\n",
    "err_rf = np.zeros(8)+1.7e-9\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "rects1 = ax.bar(x - 3*width/2, acc_dnn, width, label='DNN')\n",
    "rects2 = ax.bar(x - width/2, acc_rf, width, label='RF')\n",
    "rects4 = ax.bar(x + width/2, acc_svm, width, label='SVM')\n",
    "rects3 = ax.bar(x + 3*width/2, acc_rlr, width, label='RLR')\n",
    "plt.errorbar(x - 3*width/2, acc_dnn, yerr=err_dnn, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x - width/2, acc_rf, yerr=err_rf, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x + width/2, acc_svm, yerr=err_svm, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x + 3*width/2, acc_rlr, yerr=err_rlr, fmt='.', markersize='10', capsize=5, color='black')\n",
    "\n",
    "ax.set_ylim([0, 95])\n",
    "ax.set_ylabel('Rule Decoding Accuracy (%)')\n",
    "ax.set_xlabel('Session')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sessions)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.435, .785), fontsize='small')\n",
    "fig.savefig(\"rule_performance.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "try:\n",
    "    # Only on works on Google Colab\n",
    "    from google.colab import files\n",
    "    %tensorflow_version 2.x\n",
    "    os.chdir('..')\n",
    "    \n",
    "    # Configure kaggle if necessary\n",
    "    if not (Path.home() / '.kaggle').is_dir():\n",
    "        uploaded = files.upload()  # Find the kaggle.json file in your ~/.kaggle directory.\n",
    "        if 'kaggle.json' in uploaded.keys():\n",
    "            !mkdir -p ~/.kaggle\n",
    "            !mv kaggle.json ~/.kaggle/\n",
    "            !chmod 600 ~/.kaggle/kaggle.json\n",
    "    \n",
    "    !pip install git+https://github.com/SachsLab/indl.git\n",
    "    \n",
    "    if Path.cwd().stem == 'MonkeyPFCSaccadeStudies':\n",
    "        os.chdir(Path.cwd().parent)\n",
    "    \n",
    "    if not (Path.cwd() / 'MonkeyPFCSaccadeStudies').is_dir():\n",
    "        !git clone --single-branch --recursive https://github.com/SachsLab/MonkeyPFCSaccadeStudies.git\n",
    "        sys.path.append(str(Path.cwd() / 'MonkeyPFCSaccadeStudies'))\n",
    "    \n",
    "    os.chdir('MonkeyPFCSaccadeStudies')\n",
    "        \n",
    "    !pip install -q kaggle\n",
    "    \n",
    "    # Latest version of SKLearn\n",
    "    !pip install -U scikit-learn\n",
    "    \n",
    "    IN_COLAB = True\n",
    "    \n",
    "except ModuleNotFoundError:    \n",
    "    # chdir to MonkeyPFCSaccadeStudies\n",
    "    if Path.cwd().stem == 'Analysis':\n",
    "        os.chdir(Path.cwd().parent.parent)\n",
    "        \n",
    "    # Add indl repository to path.\n",
    "    # Eventually this should already be pip installed, but it's still under heavy development so this is easier for now.\n",
    "    check_dir = Path.cwd()\n",
    "    while not (check_dir / 'Tools').is_dir():\n",
    "        check_dir = check_dir / '..'\n",
    "    indl_path = check_dir / 'Tools' / 'Neurophys' / 'indl'\n",
    "    sys.path.append(str(indl_path))\n",
    "    \n",
    "    # Make sure the kaggle executable is on the PATH\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ';' + str(Path(sys.executable).parent / 'Scripts')\n",
    "    \n",
    "    IN_COLAB = False\n",
    "\n",
    "# Try to clear any logs from previous runs\n",
    "if (Path.cwd() / 'logs').is_dir():\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(str(Path.cwd() / 'logs'))\n",
    "    except PermissionError:\n",
    "        print(\"Unable to remove logs directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional imports\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# from indl.display import turbo_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "\n",
    "# from indl.display import turbo_cmap\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "if Path.cwd().stem == 'Analysis':\n",
    "    os.chdir(Path.cwd().parent.parent)\n",
    "data_path = Path.cwd() / 'StudyLocationRule'/ 'Data' / 'Preprocessed'\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")\n",
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc\n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_error = {\n",
    "    'valid_outcomes': (9,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_all = {\n",
    "    'valid_outcomes': (0,9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'lines.linewidth': 2,\n",
    "    'lines.markersize': 5,\n",
    "    'xtick.labelsize': 16,\n",
    "    'ytick.labelsize': 16,\n",
    "    'legend.fontsize': 18,\n",
    "    'figure.figsize': (6.4, 6.4)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if IN_COLAB:\n",
    "#     data_path = Path.cwd() / 'data' / 'monkey_pfc' / 'converted'\n",
    "# else:\n",
    "#     data_path = Path.cwd() / 'StudyLocationRule' / 'Data' / 'Preprocessed'\n",
    "\n",
    "# if not (data_path).is_dir():\n",
    "#     !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "#     print(\"Finished downloading and extracting data.\")\n",
    "# else:\n",
    "#     print(\"Data directory found. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc\n",
    "\n",
    "# load_kwargs = {\n",
    "#     'valid_outcomes': (0,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "#     'zscore': True,\n",
    "#     'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "#     'time_range': (-np.inf, 1.45),\n",
    "#     'verbose': False,\n",
    "#     'y_type': 'sacClass',\n",
    "#     'samples_last': True    \n",
    "#     #     'resample_X': 20\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess_ix = 1\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "print(f\"\\nImporting session {sess_id}\")\n",
    "X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "classes, _y = np.unique(Y_class, return_inverse=True)\n",
    "# Y_class = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "X_rates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"r2c_lstm_sra3_1_j_050_00+_split1.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a colour code cycler e.g. 'C0', 'C1', etc.\n",
    "from itertools import cycle\n",
    "colour_codes = map('C{}'.format, cycle(range(10)))\n",
    "class_colors = np.array([next(colour_codes) for _ in range(10)])  # b, o, g, r\n",
    "# class_colors = np.array(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w'])\n",
    "\n",
    "TEST_PERPLEXITY = [10]\n",
    "X = X_rates\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "def plot_tsne(x_vals, y_vals, perplexity, title='Model Output'):\n",
    "    plt.scatter(x=x_vals[:, 0], y=x_vals[:, 1], color=class_colors[y_vals])\n",
    "    plt.xlabel('t-SNE D-1')\n",
    "    plt.ylabel('t-SNE D-2')\n",
    "    plt.title(title)\n",
    "    ax = plt.gca()\n",
    "\n",
    "# First plot a t-SNE on the input data. Precede TSNE with a PCA.\n",
    "pca = PCA(n_components=50)\n",
    "pca_values = pca.fit_transform(X.reshape([-1, np.prod(X.shape[1:])]))\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY[-1])\n",
    "tsne_values = tsne_model.fit_transform(pca_values)\n",
    "plt.subplot(1, 3, 1)\n",
    "plot_tsne(tsne_values, _y.ravel(), TEST_PERPLEXITY[-1], title='Input Firing Rates')\n",
    "\n",
    "# Let's create a version of our CNN model that goes from input all the way to the 200-D flatten layer\n",
    "output_layer = -5\n",
    "tbs = 30  # tsne batch size\n",
    "truncated_model = tf.keras.Model(model.input, model.layers[output_layer].output)\n",
    "flattened_output = []\n",
    "for start_ix in range(0, X.shape[0], tbs):\n",
    "#     flattened_output.append(truncated_model(X[start_ix:start_ix+tbs, MAX_OFFSET:, :].astype(np.float32)[:, :, :, None]))\n",
    "    flattened_output.append(truncated_model(X[start_ix:start_ix+tbs, :, :].astype(np.float32)))\n",
    "flattened_output = tf.concat(flattened_output, 0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "for p_ix, perplexity in enumerate(TEST_PERPLEXITY):\n",
    "    # Initialize and fit our TSNE\n",
    "    tsne_model = TSNE(n_components=2, perplexity=perplexity)\n",
    "    tsne_values = tsne_model.fit_transform(flattened_output)\n",
    "    \n",
    "    plt.subplot(1, 3, p_ix + 2)\n",
    "    plot_tsne(tsne_values, _y.ravel(), perplexity, title='LSTM Output States')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"decode.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_IX = 4\n",
    "spatial_filter = np.squeeze(model.layers[LAYER_IX].get_weights()[0])\n",
    "D = spatial_filter.shape[-1]\n",
    "sp_cols = int(np.ceil(np.sqrt(D + 2)))\n",
    "sp_rows = int(np.ceil((D + 2) / sp_cols))\n",
    "vmax=abs(spatial_filter).max()\n",
    "vmin=-abs(spatial_filter).max()\n",
    "fig = plt.figure(figsize=(36, 12))\n",
    "# for depth_ix in range(D):\n",
    "#     plt.subplot(sp_rows, sp_cols, depth_ix + 1)\n",
    "#     plt.imshow(spatial_filter[:, depth_ix], vmax=vmax, vmin=vmin, cmap=turbo_cmap)\n",
    "#     plt.title('Spatial Filter Set {}'.format(depth_ix))\n",
    "#     plt.xlabel('Temporal Filter')\n",
    "#     plt.ylabel('Input Channel')\n",
    "# # plt.colorbar()\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(spatial_filter, vmax=vmax, vmin=vmin, cmap=turbo_cmap)\n",
    "plt.xlabel('New Channel ID')\n",
    "plt.ylabel('Channel ID')\n",
    "\n",
    "sum_abs_weight = np.sum(np.abs(spatial_filter), axis=1)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(sum_abs_weight, 20)\n",
    "plt.xlabel('Sum Abs Weight')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Chan Sum Abs. Weight')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(np.arange(spatial_filter.shape[0]), sum_abs_weight)\n",
    "plt.xlabel('Channel ID')\n",
    "plt.ylabel('Sum Abs Weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ch_ids = np.argsort(sum_abs_weight)[::-1]  # channel_ids sorted by weight, descending\n",
    "print(\"Top channels: {}\".format(ax_info['channel_names'][ch_ids][:4]))\n",
    "print(\"Bottom channels: {}\".format(ax_info['channel_names'][ch_ids][-4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ix = 14  # 10: separable_conv2d\n",
    "truncated_model = tf.keras.Model(model.input, model.layers[layer_ix].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def get_saliency(_model, _X, tr_id):\n",
    "    \n",
    "    # Convert the input data _X to tf data.\n",
    "    _input = tf.convert_to_tensor(_X[tr_id].astype(np.float32)[None, :, :])\n",
    "    \n",
    "    # Do we use the full model (incl SoftMax) or the truncated model as above?\n",
    "    # If truncated model then we need a different loss function.\n",
    "    \n",
    "    # We will calculate the saliency for each output class\n",
    "    _n_classes = _model.layers[-1].output.shape[-1]\n",
    "    losses_grads = []\n",
    "    for y_ix in range(_n_classes):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(_input)\n",
    "            class_proba = _model(_input)\n",
    "            loss_value = K.sparse_categorical_crossentropy(y_ix, class_proba)\n",
    "        grads = tape.gradient(loss_value, _input)\n",
    "        # Normalize gradients\n",
    "        grads /= (K.sqrt(K.mean(K.square(grads))) + K.epsilon())\n",
    "#         print(f'grads: {grads.shape} & loss_value: {loss_value.numpy()[0].shape}')\n",
    "        # Save output\n",
    "        losses_grads.append((loss_value.numpy()[0], np.squeeze(grads)))\n",
    "    return losses_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_PLOT_SALIENCY = 4\n",
    "t_vec = ax_info['timestamps']\n",
    "\n",
    "trial_ids = np.random.permutation(X.shape[0])[:N_PLOT_SALIENCY]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=N_PLOT_SALIENCY, ncols=9, sharex=True, sharey=True, figsize=(36, 12))\n",
    "# fig.text('Time Samples', ha='center')\n",
    "# fig.text('Channels', va='center', rotation='vertical')\n",
    "x0_ix = np.argmin(np.abs(t_vec))\n",
    "\n",
    "for row_ix, tr_id in enumerate(trial_ids):\n",
    "    l_g = get_saliency(model, X, tr_id)\n",
    "    \n",
    "    plt.subplot(N_PLOT_SALIENCY, 9, 9*row_ix + 1)\n",
    "    plt.imshow(X[tr_id], aspect='auto', cmap=turbo_cmap)\n",
    "    plt.axvline(x0_ix, color='k', linestyle='--')\n",
    "#     plt.title('Input Trial {}'.format(tr_id))\n",
    "    \n",
    "    for y_ix in range(8):\n",
    "        plt.subplot(N_PLOT_SALIENCY, 9, 9*row_ix + 2 + y_ix)\n",
    "        plt.imshow(np.abs(l_g[y_ix][1]), aspect='auto', cmap='bone')\n",
    "        plt.axvline(x0_ix, color='k', linestyle='--')\n",
    "#         plt.title(\"{:.3f}\".format(l_g[y_ix][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_g[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(pred_y, axis=1)[36]) #27\n",
    "print(_y[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ids = np.array([27, 36, 144, 240])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 15))\n",
    "x0_ix = np.argmin(np.abs(t_vec))\n",
    "y_offset = -1\n",
    "\n",
    "for row_ix, tr_id in enumerate(trial_ids):\n",
    "    l_g = get_saliency(model, X, tr_id)\n",
    "    plt.subplot(N_PLOT_SALIENCY, 1, row_ix + 1)\n",
    "    \n",
    "    y_ix = np.argmin([_[0] for _ in l_g])\n",
    "    g = l_g[y_ix][1][ch_ids, :]\n",
    "    plt.imshow(np.abs(g), aspect='auto', cmap='Wistia')\n",
    "    \n",
    "    dat = X[tr_id, ch_ids, :]\n",
    "    dat -= np.min(dat)\n",
    "    dat /= np.max(dat)\n",
    "    dat = 0.5 - dat + np.arange(36)[::-1][:, None]\n",
    "    for ch_ix in range(4):\n",
    "        plt.plot(dat[ch_ix], 'k')\n",
    "        \n",
    "    plt.title(\"Trial {}, Class {}, Pred {}\".format(tr_id, _y[tr_id], y_ix), fontsize=10)\n",
    "    plt.axvline(x0_ix, color='k', linestyle='--')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses_for_class(test_class, correct_only=False):\n",
    "    classes, y = np.unique(_y, return_inverse=True)\n",
    "    trial_ids = np.where(y == classes.tolist().index(test_class))[0]\n",
    "    losses_grads = []\n",
    "    for tr_id in trial_ids:\n",
    "        input_data = tf.convert_to_tensor(X[tr_id].astype(np.float32)[None, :, :])\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_data)\n",
    "            class_proba = model(input_data)\n",
    "            loss_value = K.sparse_categorical_crossentropy(y[tr_id], class_proba)\n",
    "        grads = tape.gradient(loss_value, input_data)  # Derivative of loss w.r.t. input\n",
    "        # Normalize gradients\n",
    "        grads /= (K.sqrt(K.mean(K.square(grads))) + K.epsilon())\n",
    "        if (not correct_only) or (y[tr_id] == np.argmax(class_proba)):\n",
    "            losses_grads.append((loss_value.numpy()[0], np.squeeze(grads)))\n",
    "    return losses_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 24))\n",
    "for plot_ix, class_name in enumerate([0,1,2,3,4,5,6,7]):\n",
    "    l_g = get_losses_for_class(class_name, correct_only=True)\n",
    "    _sal = np.mean(np.abs(np.stack([_[1] for _ in l_g], axis=0)), axis=0)\n",
    "    \n",
    "    plt.subplot(4, 2, plot_ix + 1)\n",
    "    plt.imshow(_sal, aspect='auto', cmap=turbo_cmap)\n",
    "    plt.title(f'Class: {class_name}', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_width = 0.3\n",
    "fig = plt.figure(figsize=(10, 20))\n",
    "for plot_ix, class_name in enumerate([0,1,2,3,4,5,6,7]):\n",
    "    l_g = get_losses_for_class(class_name, correct_only=True)\n",
    "    _sal = np.mean(np.mean(np.abs(np.stack([_[1] for _ in l_g], axis=0)), axis=0), axis=-1)\n",
    "#     plt.bar(np.arange(len(_sal)) + plot_ix*_width, _sal, _width, label=class_name)\n",
    "    plt.subplot(4, 2, plot_ix + 1)\n",
    "    plt.bar(np.arange(len(_sal)), _sal)\n",
    "    plt.ylim([0, 0.8])\n",
    "    plt.title(f'Class: {class_name}', fontsize=10)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Point Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = 7\n",
    "model_factors = tf.keras.Model(model.input, model.layers[output_layer].output)\n",
    "model_factors.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_input = np.zeros_like(X_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = model_factors.predict(X_rates)\n",
    "zero_factors = model_factors.predict(zero_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zero_factors.shape, factors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zero_factors = np.reshape(zero_factors, (zero_factors.shape[0] * zero_factors.shape[1], zero_factors.shape[2]))\n",
    "factors = np.reshape(factors, (factors.shape[0] * factors.shape[1], factors.shape[2]))\n",
    "print(zero_factors.shape, factors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the initial states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indl.model.recurrent import GenerativeRNN\n",
    "\n",
    "N_UNITS = 64\n",
    "# initial_state = np.empty((zero_input.shape[0]*zero_input.shape[1], N_UNITS))\n",
    "\n",
    "gen_lstm_cell = GenerativeRNN(tf.keras.layers.LSTMCell(N_UNITS),\n",
    "                              return_sequences=True, return_state=True)\n",
    "\n",
    "# for t in range(zero_input.shape[0]):\n",
    "#     for ts in range(zero_input.shape[1]):\n",
    "#         _, state_h, _ = gen_lstm_cell(tf.reshape(zero_input[t, ts], (1, 1, zero_input.shape[2])))\n",
    "#         initial_state[zero_input.shape[1]*t + ts] = state_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape = factors.shape[1:], dtype='float32')\n",
    "output= gen_lstm_cell(inputs)\n",
    "gen_lstm_model = tf.keras.Model(inputs, output)\n",
    "gen_lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import backend as K\n",
    "# def reshape_batch(x):\n",
    "#     return K.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "# _y = model_factors.layers[-1].output\n",
    "# _y = tf.keras.layers.Lambda(reshape_batch)(_y)\n",
    "# output= gen_lstm_cell(_y)\n",
    "# gen_lstm_model = tf.keras.Model(model_factors.input, output)\n",
    "# gen_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_weights = model.layers[8].get_weights()\n",
    "\n",
    "gen_lstm_model.layers[2].set_weights(lstm_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm_weights[0].shape, lstm_weights[1].shape, lstm_weights[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = gen_lstm_model(zero_factors)[1]\n",
    "print(initial_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_state = tf.dtypes.cast(initial_state, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_tensor_slices((factors, initial_states))\n",
    "\n",
    "ds_train = ds_train.batch(16, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, x, y, training):\n",
    "  # training=training is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    y_ = model(x, training=training)[1]\n",
    "    return 0.5 * tf.reduce_sum(tf.square(tf.subtract(y_, y)))\n",
    "\n",
    "\n",
    "\n",
    "# l = loss(gen_lstm_model, factors, initial_state, training=False)\n",
    "\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, training=True)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "# loss_value, grads = grad(gen_lstm_model, factors, initial_state)\n",
    "# optimizer.apply_gradients(zip(grads, gen_lstm_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "  # Training loop - using batches of 32\n",
    "    for x, y in ds_train:\n",
    "    # Optimize the model\n",
    "        loss_value, grads = grad(gen_lstm_model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, gen_lstm_model.trainable_variables))\n",
    "\n",
    "    # Track progress\n",
    "        epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "    # Compare predicted label to actual label\n",
    "    # training=True is needed only if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "\n",
    "  # End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gen_lstm_model.predict(factors)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.reshape(test, (X_rates.shape[0], int(test.shape[0]/X_rates.shape[0]), test.shape[1]))\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test[0,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_metric = tf.keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)[1]  # Forward pass\n",
    "            # Compute our own loss\n",
    "            loss = 0.5 * tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss)\n",
    "#         mae_metric.update_state(y, y_pred)\n",
    "#         return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\n",
    "        return {\"loss\": loss_tracker.result()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tf.keras.layers.Input(shape = factors.shape[1:], dtype='float32')\n",
    "inputs = tf.keras.Input(shape=(32,))\n",
    "output= tf.keras.layers.Dense(1)(inputs)\n",
    "# output= gen_lstm_cell(inputs)\n",
    "gen_lstm_custom_model = CustomModel(inputs, output)\n",
    "gen_lstm_custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't passs a loss or metrics here.\n",
    "gen_lstm_custom_model.compile(optimizer=\"SGD\")\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "gen_lstm_custom_model.fit(x, y, epochs=3)\n",
    "# Just use `fit` as usual -- you can use callbacks, etc.\n",
    "# gen_lstm_custom_model.fit(factors, initial_states, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "NCOMP = 3\n",
    "test_pca = np.empty((test.shape[0], test.shape[1], NCOMP))\n",
    "for i in range(test.shape[0]):\n",
    "    tmp = np.squeeze(test[i])\n",
    "    pca = PCA(n_components=NCOMP)\n",
    "    test_pca[i] = pca.fit_transform(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "z_line = np.squeeze(test_pca[1,:,2])\n",
    "x_line = np.squeeze(test_pca[1,:,0])\n",
    "y_line = np.squeeze(test_pca[1,:,1])\n",
    "ax.plot3D(x_line, y_line, z_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "for i in range (10):\n",
    "    z_line = np.squeeze(test_pca[i,:,2])\n",
    "    x_line = np.squeeze(test_pca[i,:,0])\n",
    "    y_line = np.squeeze(test_pca[i,:,1])\n",
    "    ax.plot3D(x_line, y_line, z_line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

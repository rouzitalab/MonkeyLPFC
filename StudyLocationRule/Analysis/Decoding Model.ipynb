{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import indl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from indl.fileio import from_neuropype_h5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import TSNE\n",
    "from itertools import cycle\n",
    "\n",
    "import os\n",
    "\n",
    "if Path.cwd().stem == 'Analysis':\n",
    "    os.chdir(Path.cwd().parent.parent)\n",
    "    \n",
    "    \n",
    "data_path = Path.cwd() / 'StudyLocationRule'/ 'Data' / 'Preprocessed'\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")\n",
    "    \n",
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc\n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_error = {\n",
    "    'valid_outcomes': (9,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_all = {\n",
    "    'valid_outcomes': (0,9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "model_kwargs = dict(\n",
    "    filt=8,\n",
    "    kernLength=20,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "model_kwargs1 = dict(\n",
    "    filt=16,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "model_kwargs2 = dict(\n",
    "    filt=32,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "\n",
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "EPOCHS2 = 100\n",
    "LABEL_SMOOTHING = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indl.model import parts\n",
    "from indl.regularizers import KernelLengthRegularizer\n",
    "\n",
    "def make_model(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=8,\n",
    "    kernLength=25,\n",
    "    ds_rate=10,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.25,\n",
    "    activation='relu',\n",
    "    l1_reg=0.000, l2_reg=0.000,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=16,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "    if _input.shape[2] < 10:\n",
    "        kernLength = 4\n",
    "        filt = 4\n",
    "        ds_rate = 4\n",
    "    elif _input.shape[2] < 20:\n",
    "        kernLength = 8\n",
    "        ds_rate = 8\n",
    "    elif _input.shape[2] < 30:\n",
    "        kernLength = 16\n",
    "    \n",
    "    input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "    if len(input_shape) < 4:\n",
    "        input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv2D(filt, (1, kernLength), padding='valid', data_format=None,\n",
    "                                dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "                                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.DepthwiseConv2D((_y.shape.as_list()[1], 1), padding='valid',\n",
    "                                      depth_multiplier=1, data_format=None, dilation_rate=(1, 1),\n",
    "                                      activation=None, use_bias=True, depthwise_initializer='glorot_uniform',\n",
    "                                      bias_initializer='zeros', depthwise_regularizer=None,\n",
    "                                      bias_regularizer=None, activity_regularizer=None,\n",
    "                                      depthwise_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.AveragePooling2D(pool_size=(1, ds_rate))(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.Reshape(_y.shape.as_list()[2:])(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = tf.keras.layers.Dense(latent_dim, activation=activation)(_y)\n",
    "#     _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(_y)\n",
    "#     outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "\n",
    "\n",
    "def kfold_pred(sess_id,X_rates,Y_class,name, verbose=1):\n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=np.max(Y_class)+1)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        randseed = 12345\n",
    "        random.seed(randseed)\n",
    "        np.random.seed(randseed)\n",
    "        tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model(X_rates, _y.shape[-1], **model_kwargs)\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'{name}_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"\\n\\nSession {sess_id} overall accuracy with CNN/LSTM Model: {accuracy}%\")\n",
    "    \n",
    "    return history, accuracy, pred_y, true_y\n",
    "\n",
    "def get_hists_acc(sess_id, verbose=1):\n",
    "    print(f\"Processing session {sess_id}...\")\n",
    "    X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "    \n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        randseed = 12345\n",
    "        random.seed(randseed)\n",
    "        np.random.seed(randseed)\n",
    "        tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model(X_rates, _y.shape[-1], **model_kwargs)\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'r2c_lstm_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"\\n\\nSession {sess_id} overall accuracy with CNN/LSTM Model: {accuracy}%\")\n",
    "    \n",
    "    return history, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-transmission",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-catering",
   "metadata": {},
   "source": [
    "#### Using load_macaque Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "\n",
    "for i in range(8):\n",
    "    history, accuracy = get_hists_acc(sess_infos[i]['exp_code'], verbose=0)\n",
    "    accs.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-stupid",
   "metadata": {},
   "source": [
    "#### Loading Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_sess_ix, keep='all'):\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    sess_id = sess_id.replace(\"+\", \"\")\n",
    "    file_name = sess_id + '_segmented.h5'\n",
    "    segmented_path = Path.cwd() / 'StudyLocationRule' / 'Data' / 'Preprocessed' / file_name\n",
    "    segmented_data = from_neuropype_h5(segmented_path)\n",
    "\n",
    "    outcome = np.array(segmented_data[2][1]['axes'][0]['data']['OutcomeCode'])\n",
    "    if keep == 'true':\n",
    "        keep_idx = np.argwhere(outcome==0).flatten()\n",
    "    elif keep == 'error':\n",
    "        keep_idx = np.argwhere(outcome==9).flatten()\n",
    "    else:\n",
    "        keep_idx = np.argwhere(outcome>-1).flatten()\n",
    "\n",
    "\n",
    "    times = np.array(segmented_data[2][1]['axes'][1]['times'])\n",
    "    onset = np.argwhere(times==1.25)\n",
    "    onset = int(onset[0])\n",
    "\n",
    "    X = segmented_data[2][1]['data']\n",
    "    X = np.nan_to_num(X)\n",
    "#     from scipy import signal\n",
    "#     kernel = signal.gaussian(100, 20)\n",
    "#     X_conv = np.zeros_like(X)\n",
    "#     for i in range(X.shape[0]):\n",
    "#         for j in range(X.shape[2]):\n",
    "#             X_conv[i,:,j] = signal.fftconvolve(np.squeeze(X[i,:,j]), kernel, mode='same')\n",
    "    X_conv = np.abs(X[keep_idx])\n",
    "    X_conv = X_conv[:,:onset,:]\n",
    "#     X_conv = X_conv[:,::10,:]\n",
    "    X_conv = np.transpose(X_conv, (0, 2, 1))\n",
    "\n",
    "    block = np.array(segmented_data[2][1]['axes'][0]['data']['Block']).flatten()[keep_idx]\n",
    "    b=np.diff(block, axis=0)\n",
    "    blck=np.array(np.where(b>0)).flatten()\n",
    "    color = np.array(segmented_data[2][1]['axes'][0]['data']['CueColour']).flatten()[keep_idx]\n",
    "    targets = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass']).flatten()[keep_idx]\n",
    "    saccades = np.array(segmented_data[2][1]['axes'][0]['data']['sacClass']).flatten()[keep_idx]\n",
    "\n",
    "    return X_conv, saccades, targets, color, blck, block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "\n",
    "for i in range(8):\n",
    "    sess_info = sess_infos[i]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X, Y, _, _, _, _ = load_data(i)\n",
    "    history, accuracy, _, _ = kfold_pred(sess_id,X,Y,name=i, verbose=0)\n",
    "    accs.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "\n",
    "for i in range(8):\n",
    "    sess_info = sess_infos[i]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X, _, Y, _, _, _ = load_data(i)\n",
    "    history, accuracy, _, _ = kfold_pred(sess_id,X,Y,name=i, verbose=0)\n",
    "    accs.append(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

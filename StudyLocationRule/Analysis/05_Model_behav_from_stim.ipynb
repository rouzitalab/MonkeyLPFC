{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model to transform encoded stimulus info to behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Configure the local or Google Colab environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6HwqG-TtNfh6",
    "outputId": "b43a9836-bb95-469f-8622-a4666bb29a01"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "vGnyG0prNfiC",
    "outputId": "11ba9f8f-249f-475b-8f13-f6e13e8905a3"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "try:\n",
    "    # Only on works on Google Colab\n",
    "    from google.colab import files\n",
    "    %tensorflow_version 2.x\n",
    "    os.chdir('..')\n",
    "    \n",
    "    # Configure kaggle if necessary\n",
    "    if not (Path.home() / '.kaggle').is_dir():\n",
    "        uploaded = files.upload()  # Find the kaggle.json file in your ~/.kaggle directory.\n",
    "        if 'kaggle.json' in uploaded.keys():\n",
    "            !mkdir -p ~/.kaggle\n",
    "            !mv kaggle.json ~/.kaggle/\n",
    "            !chmod 600 ~/.kaggle/kaggle.json\n",
    "    \n",
    "    !pip install git+https://github.com/SachsLab/indl.git\n",
    "    \n",
    "    if Path.cwd().stem == 'MonkeyPFCSaccadeStudies':\n",
    "        os.chdir(Path.cwd().parent)\n",
    "    \n",
    "    if not (Path.cwd() / 'MonkeyPFCSaccadeStudies').is_dir():\n",
    "        !git clone --single-branch --recursive https://github.com/SachsLab/MonkeyPFCSaccadeStudies.git\n",
    "        sys.path.append(str(Path.cwd() / 'MonkeyPFCSaccadeStudies'))\n",
    "    \n",
    "    os.chdir('MonkeyPFCSaccadeStudies')\n",
    "        \n",
    "    !pip install -q kaggle\n",
    "    \n",
    "    # Latest version of SKLearn\n",
    "    !pip install -U scikit-learn\n",
    "    \n",
    "    IN_COLAB = True\n",
    "    \n",
    "except ModuleNotFoundError:    \n",
    "    # chdir to MonkeyPFCSaccadeStudies\n",
    "    if Path.cwd().stem == 'Analysis':\n",
    "        os.chdir(Path.cwd().parent.parent)\n",
    "        \n",
    "    # Add indl repository to path.\n",
    "    # Eventually this should already be pip installed, but it's still under heavy development so this is easier for now.\n",
    "    check_dir = Path.cwd()\n",
    "    while not (check_dir / 'Tools').is_dir():\n",
    "        check_dir = check_dir / '..'\n",
    "    indl_path = check_dir / 'Tools' / 'Neurophys' / 'indl'\n",
    "    sys.path.append(str(indl_path))\n",
    "    \n",
    "    # Make sure the kaggle executable is on the PATH\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ';' + str(Path(sys.executable).parent / 'Scripts')\n",
    "    \n",
    "    IN_COLAB = False\n",
    "\n",
    "# Try to clear any logs from previous runs\n",
    "if (Path.cwd() / 'logs').is_dir():\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(str(Path.cwd() / 'logs'))\n",
    "    except PermissionError:\n",
    "        print(\"Unable to remove logs directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from indl.display import turbo_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'lines.linewidth': 2,\n",
    "    'lines.markersize': 5,\n",
    "    'xtick.labelsize': 16,\n",
    "    'ytick.labelsize': 16,\n",
    "    'legend.fontsize': 18,\n",
    "    'figure.figsize': (6.4, 6.4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "UVWjIh21OIQ0",
    "outputId": "109a9974-aa9f-49d1-acdc-62a9a9c50cde"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    data_path = Path.cwd() / 'data' / 'monkey_pfc' / 'converted'\n",
    "else:\n",
    "    data_path = Path.cwd() / 'StudyLocationRule' / 'Data' / 'Preprocessed'\n",
    "\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Prepare to) Load Data\n",
    "\n",
    "We will use a custom function `load_macaque_pfc` to load the data into memory.\n",
    "\n",
    "There are 4 different strings to be passed to the import `x_chunk` argument:\n",
    "* 'analogsignals' - if present. Returns 1 kHz LFPs\n",
    "* 'gaze'          - Returns 2-channel gaze data.\n",
    "* 'spikerates'    - Returns smoothed spikerates\n",
    "* 'spiketrains'\n",
    "\n",
    "The `y_type` argument can be\n",
    "* 'pair and choice' - returns Y as np.array of (target_pair, choice_within_pair)\n",
    "* 'encoded input' - returns Y as np.array of shape (n_samples, 10) (explained below)\n",
    "* 'replace with column name' - returns Y as a vector of per-trial values. e.g., 'sacClass'\n",
    "\n",
    "The actual data we load depends on the particular analysis below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc\n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'verbose': True,\n",
    "    'resample_X': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform stimulus into trial class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "y_type should be 'encoded input' to give us `Y_input` containing the samples x times x 11 stimulus information.\n",
    "* The first 4 channels are the one-hot encoded target pair. It is `1` for one of the 4 channels when that target pair is on screen, 0 otherwise.\n",
    "* The next 3 channels are the colour cue channels ('r', 'g', 'b'), 1 for the colour of the cue when it is onscreen, 0 when it is offscreen and 0 for the other colours.\n",
    "* The next 3 channels are the block rule channels, also ('r', 'g', 'b'). The value is constant 1 throughout an entire block. It is 1 for the colour that, when presented in a trial, indicates that the correct target is the one in the 'preferred' location. The 'preferred' locations are in the top half and rightmost: UU, UR, RR, and UL. There is nothing special about these targets compared to the lower and left targets, except that in general there was more neural modulation when planning saccades to 'preferred' locations vs the other locations.\n",
    "* The final channels gives the fixation point, 1 when the fixation point is onscreen, 0 otherwise. This can also act like a hold signal (important when trying to decode actual gaze behaviour).\n",
    "\n",
    "Then, from the encoded input, we will calculate a decision variable.\n",
    "\n",
    "'x_chunk' doesn't matter much as we won't be using neural data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SESS_ID = ['sra3_1_j_050_00', 'sra3_1_m_074_0001']\n",
    "test_sess_ix = 2\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "print(f\"\\nProcessing session {sess_id}\")\n",
    "X_rates, Y_input, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', y_type='encoded input', **load_kwargs)\n",
    "Y_decision = dec_from_enc(Y_input)\n",
    "Y_class = ax_info['instance_data']['sacClass'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a little bit of noise to the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_input[:, :, 7:10] = Y_input[:, :, 7:10] * (0.5 + 0.5 * np.random.randn(*(Y_input.shape[:2] + (1,))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-gz-175fnBo1",
    "outputId": "be9244e3-8126-46a0-e3b4-9ad1b92f78c8"
   },
   "outputs": [],
   "source": [
    "P_TRAIN = 0.8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(Y_input, Y_class, train_size=P_TRAIN)\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid))\n",
    "\n",
    "ds_train = ds_train.shuffle(int(Y_input.shape[0] * P_TRAIN) + 1)\n",
    "ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the model\n",
    "\n",
    "We use a model-builder function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indl.model.helper import check_inputs\n",
    "from indl.regularizers import KernelLengthRegularizer as kern_len_reg\n",
    "\n",
    "@check_inputs\n",
    "def make_model(\n",
    "    _input,\n",
    "    n_conv_filters=16,\n",
    "    conv_filt_len=30,\n",
    "    n_rnn1=200,\n",
    "    n_dense_after_rnn1=0,\n",
    "    n_rnn2=100,\n",
    "    n_dense_after_rnn2=0,\n",
    "    use_gap=True,\n",
    "    n_output_classes=8,\n",
    "    p_dropout=0.5,\n",
    "    l2_reg=1.7e-3,\n",
    "    return_model=True\n",
    "):\n",
    "    _y = _input\n",
    "    \n",
    "    if n_conv_filters:\n",
    "        _y = tf.keras.layers.Conv1D(n_conv_filters, conv_filt_len,\n",
    "                                    padding='same',\n",
    "                                    activation='relu',\n",
    "                                    use_bias=False,\n",
    "                                    kernel_regularizer=kern_len_reg((conv_filt_len,1), window_scale=1e-7, threshold=0.0015),\n",
    "                                   )(_y)\n",
    "    \n",
    "    _y = tf.keras.layers.LSTM(n_rnn1,\n",
    "                              dropout=p_dropout,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=True,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    \n",
    "    if n_dense_after_rnn1:\n",
    "        _y = tf.keras.layers.Dense(n_dense_after_rnn1, activation='relu', name='dense1')(_y)\n",
    "    \n",
    "    _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              dropout=p_dropout,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=True,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "    \n",
    "    if n_dense_after_rnn2:\n",
    "        _y = tf.keras.layers.Dense(n_dense_after_rnn2, activation='relu', name='dense2')(_y)\n",
    "    \n",
    "    if use_gap:\n",
    "#         _y = tf.keras.layers.Cropping1D(cropping=(4 * _input.shape[1] // 5, 0))(_y)\n",
    "        _y = tf.keras.layers.GlobalAveragePooling1D(name='average')(_y)\n",
    "    else:\n",
    "        _y = tf.keras.layers.Cropping1D(cropping=(_input.shape[1] - 1, 0))(_y)\n",
    "        \n",
    "    if use_gap or n_dense_after_rnn2:\n",
    "        _y = tf.keras.layers.Dropout(p_dropout)(_y)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(n_output_classes, activation='softmax')(_y)\n",
    "\n",
    "    if return_model is False:\n",
    "        return output\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    n_conv_filters=0,\n",
    "    n_rnn1=100,\n",
    "    n_dense_after_rnn1=0,\n",
    "    n_rnn2=32,\n",
    "    n_dense_after_rnn2=0,\n",
    "    use_gap=True\n",
    ")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model1 = make_model(\n",
    "    ds_train.element_spec[0],\n",
    "    **model_kwargs\n",
    ")\n",
    "model1.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.summary()\n",
    "if False:\n",
    "    tf.keras.utils.plot_model(\n",
    "        model1,\n",
    "        to_file='model.png',\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        rankdir='TB',\n",
    "        expand_nested=False,\n",
    "        dpi=96\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "As the stimulus information completely encodes the task objective, and we are only using 'correct' trials, the task is trivial. The model should be able to achieve 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "cpSTWqR0UoB4",
    "outputId": "638dd00b-8836-4288-b41a-6752b832dae0"
   },
   "outputs": [],
   "source": [
    "# MONKEY J MODEL FIT\n",
    "EPOCHS = 20\n",
    "\n",
    "history = model1.fit(x=ds_train, epochs=EPOCHS, verbose=1, validation_data=ds_valid)\n",
    "tf.keras.models.save_model(model1, 'model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indl.metrics import quickplot_history\n",
    "\n",
    "\n",
    "quickplot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the model was able to learn a relationship that transformed the encoded stimulus information into the trial class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Activations to Neural Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer = 'rnn2'\n",
    "test_model = tf.keras.models.Model(inputs=model1.input, outputs=model1.get_layer(test_layer).output)\n",
    "test_activations = test_model.predict(Y_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CCA_COMPS = 8  # Number of components for CCA decomposition\n",
    "n_trials, n_samples, n_channels = X_rates.shape\n",
    "n_units = test_activations.shape[-1]\n",
    "\n",
    "A = np.copy(X_rates.reshape(n_trials * n_samples, n_channels))\n",
    "B = test_activations.reshape(n_trials * n_samples, n_units)\n",
    "\n",
    "cca = CCA(n_components=N_CCA_COMPS)\n",
    "rates_score, acts_score = cca.fit_transform(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "corrcoef, p_value = pearsonr(rates_score[:,0], acts_score[:,0])\n",
    "print(f\"Pearson R correlation coefficient for the first component is: {corrcoef}\")\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(rates_score[:,0], acts_score[:,0])\n",
    "print(f\"Scipy linear regression R2 value for the first component is: {r_value**2}\")\n",
    "\n",
    "# https://stackoverflow.com/questions/37398856/how-to-get-the-first-canonical-correlation-from-sklearns-cca-module\n",
    "result = np.corrcoef(rates_score.T, acts_score.T).diagonal(offset=N_CCA_COMPS)\n",
    "print(f\"Numpy correlation coefficient for all components are: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize CCA result\n",
    "\n",
    "#### Visualize component alignment - averaged across trials per condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_cond_averages(_X, _conds):\n",
    "    uq_conds, cond_ix = np.unique(_conds, return_inverse=True)\n",
    "    output = np.nan * np.ones((len(uq_conds), *_X.shape[1:]))\n",
    "    \n",
    "    for ix, cond in enumerate(uq_conds):\n",
    "        b_cond = _conds == cond\n",
    "        output[ix] = np.nanmean(_X[b_cond], axis=0)\n",
    "        \n",
    "    return output, uq_conds\n",
    "\n",
    "rates_cond_scores, rates_conds = per_cond_averages(rates_score.reshape(n_trials, n_samples, N_CCA_COMPS), Y_class)\n",
    "acts_cond_scores, acts_conds = per_cond_averages(acts_score.reshape(n_trials, n_samples, N_CCA_COMPS), Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import zscore\n",
    "def _norm(_arr, axis=0):\n",
    "    # Scale from 0 to 1\n",
    "    return (_arr - np.mean(_arr, axis=axis)) / (np.max(_arr, axis=axis) - np.min(_arr, axis=axis))\n",
    "\n",
    "\n",
    "N_PLOT_COLS = 4\n",
    "N_PLOT_CONDS = 8\n",
    "COMP_IX = 0\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.suptitle(f'Class-averages of CCA component {COMP_IX}', fontsize=18)\n",
    "for cond_ix in range(N_PLOT_CONDS):\n",
    "    plt.subplot(int(np.ceil(N_PLOT_CONDS / N_PLOT_COLS)), N_PLOT_COLS, cond_ix + 1)\n",
    "    plt.plot(ax_info['timestamps'], _norm(rates_cond_scores[cond_ix, :, COMP_IX]), label='rates')\n",
    "    plt.plot(ax_info['timestamps'], _norm(acts_cond_scores[cond_ix, :, COMP_IX]), label='activations')\n",
    "    plt.title(f'Class {rates_conds[cond_ix]}', fontsize=8)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize alignment of neural rates with DNN activations inverse-transformed into neural space\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_reconstructed = cca.inverse_transform(a_score_norm)\n",
    "rnn_reconstructed = cca.inverse_transform(b_score_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the CCA under null hypothesis\n",
    "\n",
    "**Warning: Very slow!**\n",
    "\n",
    "First, we check the CCA result after initializing models with random weights and doing no training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NULL = 100\n",
    "model_null_r = []\n",
    "A = np.copy(X_rates.reshape(n_trials * n_samples, n_channels))\n",
    "N_NULL_COMPS = 1\n",
    "\n",
    "for m_ix in range(N_NULL):\n",
    "    tf.keras.backend.clear_session()\n",
    "    null_model = make_model(ds_train.element_spec[0], **model_kwargs)\n",
    "    null_inter_model = tf.keras.models.Model(inputs=null_model.input, outputs=null_model.get_layer(test_layer).output)\n",
    "    null_activations = null_inter_model.predict(Y_input)\n",
    "    null_B = null_activations.reshape(n_trials * n_samples, n_units)\n",
    "    null_cca = CCA(n_components=N_NULL_COMPS)\n",
    "    # TODO: This was recently modified to correlate class-averages for testing purposes only,\n",
    "    # but this is not the preferred way. Correlating individual trials... this is the way.\n",
    "    _A, _ = per_cond_averages(X_rates, Y_class)\n",
    "    _B, _ = per_cond_averages(null_activations, Y_class)\n",
    "    null_cca.fit(_A.reshape((-1, _A.shape[-1])), _B.reshape((-1, _B.shape[-1])))\n",
    "    score = np.diag(np.corrcoef(null_cca.x_scores_, null_cca.y_scores_, rowvar=False)[:N_NULL_COMPS, N_NULL_COMPS:])\n",
    "    model_null_r.append(score)\n",
    "    \n",
    "model_null_p = (np.sum((np.stack(model_null_r) - result[0]) >= 0, axis=0) + 1) / (N_NULL + 1)\n",
    "print(np.stack(model_null_r).T)\n",
    "print(f\"Under the null hypothesis of no association between neuronal rates and RNN actications, \"\n",
    "      f\"the probability of obtaining the observed correlation coefficients of {result[0]} is {model_null_p[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check the CCA result with the trained model activations and shuffle neural weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NULL = 100\n",
    "shuffle_r = []\n",
    "A = np.copy(X_rates.reshape(n_trials * n_samples, n_channels))\n",
    "N_NULL_COMPS = 1\n",
    "\n",
    "for shuff_ix in range(N_NULL):\n",
    "    np.random.shuffle(A)\n",
    "    null_cca = CCA(n_components=N_NULL_COMPS)\n",
    "    null_cca.fit(A, B)\n",
    "    score = np.diag(np.corrcoef(null_cca.x_scores_, null_cca.y_scores_, rowvar=False)[:N_NULL_COMPS, N_NULL_COMPS:])\n",
    "    shuffle_r.append(score)\n",
    "    \n",
    "p = (np.sum((np.stack(corrcoefs) - result[None, :]) >= 0, axis=0) + 1) / (N_NULL + 1)\n",
    "print(f\"Under the null hypothesis of no association between neuronal rates and RNN actications, \"\n",
    "      f\"the probability of obtaining the observed correlation coefficients is {p}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHXZKZ7Mtu2A"
   },
   "source": [
    "# Encoded Input to 8 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyKg7xvqd0nN"
   },
   "outputs": [],
   "source": [
    "N_RNN_UNITS = 200      # Size of RNN output (state)\n",
    "BIN_DURATION = 25     # Width of window used to bin spikes, in 10 ms\n",
    "BIN_OVRLP = 4\n",
    "N_TAPS = 8\n",
    "IMG_SIZE = 160            # Number of bins of history used in a sequence.\n",
    "NCOMPONENTS = 8  # Number of components for CCA decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAo8NqmBU-Wz"
   },
   "source": [
    "### Decrease Sampling Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8_RT_hNRU99O",
    "outputId": "acf695f5-42b1-49db-c0a4-00112e516be3"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((encoded_in.shape[0], N_TAPS, encoded_in.shape[2]))\n",
    "X1 = np.zeros((X2.shape[0], N_TAPS, X2.shape[2]))  #downsampled neural activity\n",
    "Y = np.zeros((encoded_in.shape[0], 8))\n",
    "for j in range(N_TAPS-1):\n",
    "   X[:, j, :] = np.mean(encoded_in[:, j*(BIN_DURATION - BIN_OVRLP):j*(BIN_DURATION - BIN_OVRLP) + BIN_DURATION, :], axis=1)  # Downsampled Encoded Input\n",
    "    X1[:, j, :] = np.mean(X2[:, j*(BIN_DURATION - BIN_OVRLP):j*(BIN_DURATION - BIN_OVRLP) + BIN_DURATION, :], axis=1)  # Downsampled Spike Rates\n",
    "\n",
    "X[:, N_TAPS-1, :] = np.mean(encoded_in[:, (N_TAPS-1)*(BIN_DURATION - BIN_OVRLP):, :], axis=1)\n",
    "X1[:, N_TAPS-1, :] = np.mean(X2[:, (N_TAPS-1)*(BIN_DURATION - BIN_OVRLP):, :], axis=1)\n",
    "\n",
    "for i in range(encoded_in.shape[0]):\n",
    "    Y[i, y[i]] = 1\n",
    "\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, train_size=P_TRAIN)\n",
    "\n",
    "ds_train1 = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "ds_valid1 = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid))\n",
    "\n",
    "ds_train1 = ds_train1.shuffle(int(X.shape[0] * P_TRAIN) + 1)\n",
    "ds_train1 = ds_train1.batch(BATCH_SIZE, drop_remainder=True)\n",
    "ds_valid1 = ds_valid1.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "  \n",
    "print(ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEfs5fY8IM0p"
   },
   "source": [
    "# Encoded Input to Final Gaze Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8v6esDwoGIiw"
   },
   "outputs": [],
   "source": [
    "gaze = df[['PosX', 'PosY']].to_numpy()\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, gaze, train_size=P_TRAIN)\n",
    "\n",
    "ds_train2 = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "ds_valid2 = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid))\n",
    "\n",
    "ds_train2 = ds_train2.shuffle(int(X.shape[0] * P_TRAIN) + 1)\n",
    "ds_train2 = ds_train2.batch(BATCH_SIZE, drop_remainder=True)\n",
    "ds_valid2 = ds_valid2.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "DDkzPxfmEKSs",
    "outputId": "df20d856-6851-43b4-b009-2a39e11bf616"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "input_shape = X.shape[1:]\n",
    "output_shape = gaze.shape[1]\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = input_shape)\n",
    "o1 = tf.keras.layers.SimpleRNN(N_RNN_UNITS, activation='tanh', use_bias=True,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            recurrent_initializer='orthogonal',\n",
    "                            bias_initializer='zeros', kernel_regularizer=None,\n",
    "                            recurrent_regularizer=None, bias_regularizer=None,\n",
    "                            activity_regularizer=None, kernel_constraint=None,\n",
    "                            recurrent_constraint=None, bias_constraint=None,\n",
    "                            dropout=0.0, recurrent_dropout=0.0,\n",
    "                            return_sequences=True, return_state=False,\n",
    "                            go_backwards=False, stateful=False, unroll=False, name='rnn1')(inputs)\n",
    "o2 = tf.keras.layers.Dense(32, name='dense1')(o1)\n",
    "o3 = tf.keras.layers.LSTM(100, dropout = P_DROPOUT,\n",
    "                          recurrent_dropout = 0,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(L2_REG),\n",
    "                          recurrent_regularizer=tf.keras.regularizers.l2(L2_REG),\n",
    "                          return_sequences=True, stateful=False, name='rnn2')(o2)\n",
    "o4 = tf.keras.layers.Dense(32, name='dense2')(o3)\n",
    "o5 = tf.keras.layers.AveragePooling1D(pool_size=8)(o4)\n",
    "o6 = tf.keras.layers.Flatten()(o5)\n",
    "o7 = tf.keras.layers.Dropout(P_DROPOUT)(o6)\n",
    "outputs = tf.keras.layers.Dense(output_shape, activation='linear')(o7)\n",
    "\n",
    "model2 = tf.keras.Model(inputs, outputs)\n",
    "model2.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "x2l-JnX4GPWs",
    "outputId": "f2bd305c-51a8-4b4a-d14d-192def3c24d2"
   },
   "outputs": [],
   "source": [
    "history = model2.fit(x=ds_train2, epochs=EPOCHS2, verbose=1, validation_data=ds_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "st4IqXiSHIWm"
   },
   "outputs": [],
   "source": [
    "pred_y = model2.predict(x=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "AbnP1-ghGjpp",
    "outputId": "0e6533b6-08d8-43c6-f990-c0513ccbd0fb"
   },
   "outputs": [],
   "source": [
    "t = [900, 1017]\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(gaze[t[0]:t[1], 0])\n",
    "plt.plot(pred_y[t[0]:t[1], 0])\n",
    "for ix, label in enumerate(['True', 'RNN']):\n",
    "    plt.gca().lines[ix].set_label(label)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Pos-x')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(gaze[t[0]:t[1], 1])\n",
    "plt.plot(pred_y[t[0]:t[1], 1])\n",
    "for ix, label in enumerate(['True', 'RNN']):\n",
    "    plt.gca().lines[ix].set_label(label)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Pos-y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w692QusPIX-7"
   },
   "source": [
    "## Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBeGNNcEIXjo"
   },
   "outputs": [],
   "source": [
    "pred_vel = np.diff(pred_y, axis=0)\n",
    "pred_vel /= np.linalg.norm(pred_vel)\n",
    "gaze_vel = np.diff(gaze, axis=0)\n",
    "gaze_vel /= np.linalg.norm(gaze_vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "Yv8xkmgpJCN1",
    "outputId": "2709495b-54e0-47ff-c159-b66a197eae52"
   },
   "outputs": [],
   "source": [
    "t = [900, 1017]\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(gaze_vel[t[0]:t[1], 0])\n",
    "plt.plot(pred_vel[t[0]:t[1], 0])\n",
    "for ix, label in enumerate(['True', 'RNN']):\n",
    "    plt.gca().lines[ix].set_label(label)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Vel-x')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(gaze_vel[t[0]:t[1], 1])\n",
    "plt.plot(pred_vel[t[0]:t[1], 1])\n",
    "for ix, label in enumerate(['True', 'RNN']):\n",
    "    plt.gca().lines[ix].set_label(label)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Vel-y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYzzrI3Ub8fZ"
   },
   "source": [
    "# Encoded Input to Continous Gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "52VCWekOb_rB",
    "outputId": "f6d73b69-4895-4641-955e-6efa2ee65406"
   },
   "outputs": [],
   "source": [
    "gaze_con, _, ax_info = load_macaque_pfc(datadir, SESS_ID[0], x_chunk='gaze', zscore=True)\n",
    "\n",
    "# deleting blocks of gaze with less than 10 trials\n",
    "tmp = np.unique(ax_info['instance_data'][['Block']].to_numpy(), return_index = True)[1]\n",
    "block_length = np.diff(tmp)\n",
    "indx = []\n",
    "for i in range(len(tmp)-1):\n",
    "  if block_length[i] < 10:\n",
    "    indx = indx + list(range(tmp[i], tmp[i+1]))\n",
    "\n",
    "gaze_con = np.delete(gaze_con, indx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoKlsQN2hUVq"
   },
   "source": [
    "## Loading the Neural Data and Generating the Encoded Input Again for Assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DfnKV5iGhbD8",
    "outputId": "c5543468-6b03-40a0-d244-0cda83eb7f46"
   },
   "outputs": [],
   "source": [
    "X2, y, ax_info = load_macaque_pfc(datadir, SESS_ID[0], zscore=True)\n",
    "cueColor = ax_info['instance_data'][['CueColour']].to_numpy()\n",
    "\n",
    "# Deleting blocks with less than 10 trials\n",
    "tmp = np.unique(ax_info['instance_data'][['Block']].to_numpy(), return_index = True)[1]\n",
    "block_length = np.diff(tmp)\n",
    "indx = []\n",
    "for i in range(len(tmp)-1):\n",
    "  if block_length[i] < 10:\n",
    "    indx = indx + list(range(tmp[i], tmp[i+1]))\n",
    "\n",
    "X2 = np.delete(X2, indx, axis=0)\n",
    "y = np.delete(y, indx, axis=0)\n",
    "\n",
    "\n",
    "# Making the encoded input again\n",
    "tmp2 = ax_info['instance_data']\n",
    "df = tmp2.drop(tmp2.index[indx])\n",
    "block_indx = np.unique(df[['Block']].to_numpy(), return_index = True)[1]\n",
    "block_indx = np.append(block_indx, X2.shape[0])\n",
    "cueColor = np.delete(cueColor, indx, axis=0)\n",
    " \n",
    "\n",
    "# Free memory\n",
    "del tmp\n",
    "del indx\n",
    "del block_length\n",
    "del tmp2\n",
    "\n",
    "encoded_in = np.zeros((X2.shape[0], X2.shape[1], 10))\n",
    "\n",
    "for i in range(X2.shape[0]):\n",
    "  encoded_in[i, 50:, y[i]%4] = 1\n",
    "  if cueColor[i] == 'r':\n",
    "    encoded_in[i, 72:144, 4] = 1\n",
    "  elif cueColor[i] == 'g':\n",
    "    encoded_in[i, 72:144, 5] = 1\n",
    "  else:\n",
    "    encoded_in[i, 72:144, 6] = 1\n",
    "\n",
    "\n",
    "block = 1\n",
    "i = 0\n",
    "while i < X2.shape[0] and block < 7:\n",
    "  if y[i]==0 or y[i]==1 or y[i]==2 or y[i]==7:\n",
    "    if cueColor[i] == 'r':\n",
    "      encoded_in[block_indx[block-1]:block_indx[block], :, 7] = 1\n",
    "    elif cueColor[i] == 'g':\n",
    "      encoded_in[block_indx[block-1]:block_indx[block], :, 8] = 1\n",
    "    else:\n",
    "      encoded_in[block_indx[block-1]:block_indx[block], :, 9] = 1\n",
    "    i = block_indx[block]\n",
    "    block = block + 1    \n",
    "  else:\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMk1bKSxejAJ"
   },
   "outputs": [],
   "source": [
    "X3 = np.zeros((gaze_con.shape[0], encoded_in.shape[1], gaze_con.shape[2]))\n",
    "\n",
    "# downsampling continuous gaze to same as encoded input\n",
    "X3[:, :-1, :] = gaze_con[:, ::10, :]\n",
    "X3[:, -1, :] = gaze_con[:, -1, :]\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(encoded_in, X3, train_size=P_TRAIN)\n",
    "\n",
    "ds_train4 = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "ds_valid4 = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid))\n",
    "\n",
    "ds_train4 = ds_train4.shuffle(int(encoded_in.shape[0] * P_TRAIN) + 1)\n",
    "ds_train4 = ds_train4.batch(BATCH_SIZE, drop_remainder=True)\n",
    "ds_valid4 = ds_valid4.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "LWlGOhSxe7o3",
    "outputId": "c10cfc25-9119-4c0c-cf5f-fa19347c8eaf"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "input_shape = encoded_in.shape[1:]\n",
    "output_shape = X3.shape[2]\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = input_shape)\n",
    "o1 = tf.keras.layers.SimpleRNN(N_RNN_UNITS, activation='tanh', use_bias=True,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            recurrent_initializer='orthogonal',\n",
    "                            bias_initializer='zeros', kernel_regularizer=None,\n",
    "                            recurrent_regularizer=None, bias_regularizer=None,\n",
    "                            activity_regularizer=None, kernel_constraint=None,\n",
    "                            recurrent_constraint=None, bias_constraint=None,\n",
    "                            dropout=0.0, recurrent_dropout=0.0,\n",
    "                            return_sequences=True, return_state=False,\n",
    "                            go_backwards=False, stateful=False, unroll=False, name='rnn1')(inputs)\n",
    "o2 = tf.keras.layers.Dense(32, name='dense1')(o1)\n",
    "o3 = tf.keras.layers.LSTM(100, dropout = P_DROPOUT,\n",
    "                          recurrent_dropout = 0,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(L2_REG),\n",
    "                          recurrent_regularizer=tf.keras.regularizers.l2(L2_REG),\n",
    "                          return_sequences=True, stateful=False, name='rnn2')(o2)\n",
    "o4 = tf.keras.layers.Dense(32, name='dense2')(o3)\n",
    "o5 = tf.keras.layers.Dropout(P_DROPOUT)(o4)\n",
    "outputs = tf.keras.layers.Dense(output_shape, activation='linear')(o5)\n",
    "\n",
    "model8 = tf.keras.Model(inputs, outputs)\n",
    "model8.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "rJAVB3ikfDir",
    "outputId": "4b30bd5d-3b4f-43d9-ac3e-a045fa334021"
   },
   "outputs": [],
   "source": [
    "history = model8.fit(x=ds_train4, epochs=EPOCHS2, verbose=1, validation_data=ds_valid4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLViOc2zSEby"
   },
   "outputs": [],
   "source": [
    "predicted_gaze = model8.predict(encoded_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "colab_type": "code",
    "id": "iWuJHLD_SdbE",
    "outputId": "32255758-6fab-4e47-b1ee-cbf96b71bbf4"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10,8))\n",
    "fig.suptitle('Decoding Monkeys Time-Continuous Gaze')\n",
    "axs[0].plot(predicted_gaze[20, :, 0], 'C0', label='RNN')\n",
    "axs[0].plot(X3[20, :, 0], 'C1', label='Actual')\n",
    "axs[0].set_title(\"Gaze X-Position\")\n",
    "axs[1].plot(predicted_gaze[20, :, 1], 'C0')\n",
    "axs[1].plot(X3[20, :, 1], 'C1')\n",
    "axs[1].set_title(\"Gaze Y-Position\")\n",
    "fig.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zKRSKZ1_fNxF"
   },
   "outputs": [],
   "source": [
    "layer_name = 'dense1'\n",
    "intermediate_layer_model1 = tf.keras.Model(inputs=model8.input,\n",
    "                                 outputs=model8.get_layer(layer_name).output)\n",
    "layer_name = 'dense2'\n",
    "intermediate_layer_model2 = tf.keras.Model(inputs=model8.input,\n",
    "                                 outputs=model8.get_layer(layer_name).output)\n",
    "data = encoded_in\n",
    "intermediate_output1 = intermediate_layer_model1.predict(data)\n",
    "intermediate_output2 = intermediate_layer_model2.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_M15jR7QfGlS"
   },
   "source": [
    "## CCA on RNN activations and Spikerates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJ2lDkypZ12q"
   },
   "outputs": [],
   "source": [
    "A = np.mean(X2, axis=0) # Spikerates averaged over trials\n",
    "B = np.mean(intermediate_output1, axis=0) # RNN activations averaged over trials\n",
    "cca = CCA(n_components = NCOMPONENTS)\n",
    "\n",
    "a_score, b_score = cca.fit_transform(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "colab_type": "code",
    "id": "zINenp0YKAXh",
    "outputId": "69b946a4-a61c-45d9-f0be-62a8264f5aa7"
   },
   "outputs": [],
   "source": [
    "# Normalizing the components to plot with each other\n",
    "a_score_norm = (a_score - np.mean(a_score, axis=0))/np.max(np.abs(a_score), axis=0)\n",
    "b_score_norm = (b_score - np.mean(b_score, axis=0))/np.max(np.abs(b_score), axis=0)\n",
    "\n",
    "COMPONENTSPLOT = 8  # To plot the first 'COMPONENTSPLOT'\n",
    "fig, axs = plt.subplots(int(COMPONENTSPLOT/4), 4, figsize=(20,10))\n",
    "fig.suptitle('CCA First 8 Components on Trial-Averaged Data')\n",
    "for component in range(COMPONENTSPLOT):\n",
    "  axs[int(component/4), component%4].plot(a_score_norm[:,component], 'C0')\n",
    "  axs[int(component/4), component%4].plot(b_score_norm[:,component], 'C1')\n",
    "  axs[int(component/4), component%4].set_title(\"Comopnent #: \" + str(component+1))\n",
    "\n",
    "fig.legend(('Neural Acitivity', 'RNN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "R_bl6Yzdum2e",
    "outputId": "687d8fc4-16a8-4d9e-95b0-eed9b488ae03"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "corrcoef, p_value = pearsonr(a_score[:,0],b_score[:,0])\n",
    "print(\"Pearson R correlation coefficient for the first component is: \" + str(corrcoef))\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(a_score[:,0],b_score[:,0])\n",
    "print(\"\\nScipy linear regression R2 value for the first component is: \" + str(r_value**2))\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/37398856/how-to-get-the-first-canonical-correlation-from-sklearns-cca-module\n",
    "print(\"\\nNumpy correlation coefficient for all components are: \" + str(np.corrcoef(a_score.T, b_score.T).diagonal(offset=NCOMPONENTS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXOgiUrVGuD-"
   },
   "source": [
    "### Reconstrocting maximum-correlated signals in original space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZocxtKSCSBs"
   },
   "outputs": [],
   "source": [
    "neural_reconstructed = cca.inverse_transform(a_score_norm)\n",
    "rnn_reconstructed = cca.inverse_transform(b_score_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5tEywaj-G_Xz"
   },
   "source": [
    "Plotting the reconstructed neural and RNN components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "colab_type": "code",
    "id": "DUSRH9owG-61",
    "outputId": "1e4b3c6f-c510-4714-fdeb-423dacea93f8"
   },
   "outputs": [],
   "source": [
    "NCHANNELS = 8\n",
    "fig, axs = plt.subplots(int(NCHANNELS/4), 4, figsize=(20,10))\n",
    "fig.suptitle('First 8 Channels of Reconstructed Neural Activity and RNN Activations in Original Space')\n",
    "for channel in range(NCHANNELS):\n",
    "  axs[int(channel/4), channel%4].plot(neural_reconstructed[:,channel], 'C0')\n",
    "  axs[int(channel/4), channel%4].plot(rnn_reconstructed[:,channel], 'C1')\n",
    "  axs[int(channel/4), channel%4].set_title(\"Channel \" + str(channel+1))\n",
    "\n",
    "fig.legend(('Neural Acitivity', 'RNN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCel9IzLvrIw"
   },
   "source": [
    "## Plotting the Original Spike Rates to Compare with The Reconstructed Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXsi8UAF9SS5"
   },
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HossfrrOv3He",
    "outputId": "dbf76922-1488-42be-df75-862c1c1b34c6"
   },
   "outputs": [],
   "source": [
    "original = A\n",
    "reconstructed = neural_reconstructed\n",
    "\n",
    "NCHANNELS = 32\n",
    "fig, axs = plt.subplots(int(NCHANNELS/4), 4, figsize=(20,35))\n",
    "fig.suptitle('Original Spike Rates VS. Reconstructed')\n",
    "for channel in range(NCHANNELS):\n",
    "  axs[int(channel/4), channel%4].plot(original[:,channel], 'C0')\n",
    "  axs[int(channel/4), channel%4].plot(reconstructed[:,channel], 'C1')\n",
    "  axs[int(channel/4), channel%4].set_title(\"Channel \" + str(channel+1))\n",
    "\n",
    "fig.legend(('Original', 'Reconstructed'))\n",
    "fig.tight_layout(rect=[0, 0.03, 0.9, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMFahK2k84Do"
   },
   "source": [
    "IF we use NCOMPONENTS = 32, the reconstucted neural activity and the original spike rates will be exactly the same (As it should be)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8vpRkztmSA45",
    "JAo8NqmBU-Wz",
    "eMmkVzfcXYNV",
    "8rktdy_9lOWu",
    "Gg5XgclGzzZH",
    "rCNSkqyxeMzj",
    "QVbgVGh5vu-5",
    "E_hTaBFKLd6f",
    "KwUJm-XEYPWi",
    "w692QusPIX-7",
    "HCaIgFsUG6dI"
   ],
   "name": "06_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

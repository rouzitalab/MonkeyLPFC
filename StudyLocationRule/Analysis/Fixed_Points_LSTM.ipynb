{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To run this notebook you need to clone this repository to be imported in the second cell:\n",
    "\n",
    "## https://github.com/garrettkatz/rnn-fxpts.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import rnn_fxpts as rfx\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from indl.display import turbo_cmap\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing RFX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 2\n",
    "W0 = 1.25*np.eye(N0) + 0.1*np.random.randn(N0,N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fxpts0, fiber0 = rfx.run_solver(W0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fxpts0.shape, fiber0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fxpts0[0], fxpts0[1], 'o')\n",
    "plt.plot(fiber0[0], fiber0[1])\n",
    "plt.plot(fiber0[1], fiber0[2])\n",
    "plt.plot(fiber0[0], fiber0[2])\n",
    "plt.plot(fiber0[1], fiber0[0])\n",
    "plt.plot(fiber0[2], fiber0[1])\n",
    "plt.plot(fiber0[2], fiber0[0])\n",
    "plt.xlim((-1.5,1.5))\n",
    "plt.ylim((-1.5,1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfx.show_fiber(W0, fxpts0, fiber0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data (New version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the data_path according to the local setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().parent.parent / 'Data' / 'Preprocessed'\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc\n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess_ix = 1\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "print(f\"\\nImporting session {sess_id}\")\n",
    "X_rates, Y_new, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "Y_class = tf.keras.utils.to_categorical(Y_new, num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data (Old version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_id_old = sess_info['exp_code'][:-1] + \"_v1+\"\n",
    "print(f\"\\nImporting session {sess_id_old}\")\n",
    "X_rates_old, Y_old, ax_info_old = load_macaque_pfc(data_path, sess_id_old, x_chunk='spikerates', **load_kwargs)\n",
    "Y_class_old = tf.keras.utils.to_categorical(Y_old, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_rates.shape, X_rates_old.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indl.model import parts\n",
    "from indl.model.helper import check_inputs\n",
    "from indl.regularizers import KernelLengthRegularizer\n",
    "\n",
    "@check_inputs\n",
    "def make_model(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=8,\n",
    "    kernLength=25,\n",
    "    ds_rate=10,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.25,\n",
    "    activation='relu',\n",
    "    l1_reg=0.000, l2_reg=0.000,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=16,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = _input\n",
    "    \n",
    "    input_shape = _input.shape.as_list()\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "    if len(input_shape) < 4:\n",
    "        input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv2D(filt, (1, kernLength), padding='valid', data_format=None,\n",
    "                                dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "                                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.DepthwiseConv2D((_y.shape.as_list()[1], 1), padding='valid',\n",
    "                                      depth_multiplier=1, data_format=None, dilation_rate=(1, 1),\n",
    "                                      activation=None, use_bias=True, depthwise_initializer='glorot_uniform',\n",
    "                                      bias_initializer='zeros', depthwise_regularizer=None,\n",
    "                                      bias_regularizer=None, activity_regularizer=None,\n",
    "                                      depthwise_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.AveragePooling2D(pool_size=(1, ds_rate))(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.Reshape(_y.shape.as_list()[2:])(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Parameters\n",
    "LABEL_SMOOTHING = 0.2\n",
    "\n",
    "model_kwargs = dict(\n",
    "    filt=8,\n",
    "    kernLength=25,\n",
    "    ds_rate=9,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 180\n",
    "\n",
    "\n",
    "def get_hists_acc(sess_id, new=True, verbose=1):\n",
    "    print(f\"Processing session {sess_id}...\")\n",
    "    X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "    \n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        randseed = 12345\n",
    "        random.seed(randseed)\n",
    "        np.random.seed(randseed)\n",
    "        tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model(\n",
    "            ds_train.element_spec[0],\n",
    "            _y.shape[-1],\n",
    "            **model_kwargs\n",
    "        )\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        if new:\n",
    "            best_model_path = f'r2c_lstm_{sess_id}new_split{split_ix}.h5'\n",
    "        else:\n",
    "            best_model_path = f'r2c_lstm_{sess_id}old_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"Session {sess_id} overall accuracy: {accuracy}%\")\n",
    "    \n",
    "    return history, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from indl.metrics import quickplot_history\n",
    "\n",
    "history, accuracy = get_hists_acc(sess_id,new=True, verbose=2)\n",
    "quickplot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train for old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indl.metrics import quickplot_history\n",
    "\n",
    "history, accuracy = get_hists_acc(sess_id_old,new=False, verbose=2)\n",
    "quickplot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding fixed points in lstm's recurrent cell states\n",
    "#### Looking into saved best models for ten splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Running the next cell may take several hours to finish on CPU\n",
    "# You can jump to the \"load pickle\" cell (Four cells later) and the next one after that to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NCOMP = 2\n",
    "fxpts = []\n",
    "\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    print(f'Working on Split #{i}, new data')\n",
    "    model = tf.keras.models.load_model(f'r2c_lstm_sra3_1_j_050_00+new_split{i}.h5')\n",
    "    lstm = model.layers[10].get_weights()\n",
    "    cell_states = lstm[1][:, model_kwargs['n_rnn'] * 2: model_kwargs['n_rnn'] * 3]\n",
    "    W = cell_states * 10\n",
    "    fxpt, _ = rfx.run_solver(W)\n",
    "#     pca = PCA(n_components=NCOMP)\n",
    "#     tmp = fxpt.T\n",
    "#     fxpt_pc = pca.fit_transform(tmp)\n",
    "#     fxpt_pc = fxpt_pc.T\n",
    "    fxpts.append(fxpt)\n",
    "    \n",
    "    print(f'Working on Split #{i}, old data')\n",
    "    model = tf.keras.models.load_model(f'r2c_lstm_sra3_1_j_050_00_v1+old_split{i}.h5')\n",
    "    lstm = model.layers[10].get_weights()\n",
    "    cell_states = lstm[1][:, model_kwargs['n_rnn'] * 2: model_kwargs['n_rnn'] * 3]\n",
    "    W = cell_states * 10\n",
    "    fxpt, _ = rfx.run_solver(W)\n",
    "#     pca = PCA(n_components=NCOMP)\n",
    "#     tmp = fxpt.T\n",
    "#     fxpt_pc = pca.fit_transform(tmp)\n",
    "#     fxpt_pc = fxpt_pc.T\n",
    "    fxpts.append(fxpt)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_points.pkl', 'wb') as f:\n",
    "    pickle.dump(fxpts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "# for i in range(N_SPLITS):\n",
    "#     axs[int(i/5), i%5].plot(fxpts[2*i][0], fxpts[2*i][1], 'bo')\n",
    "#     axs[int(i/5), i%5].set_title(f'Split #{i}')\n",
    "    \n",
    "#     axs[int(i/5), i%5].plot(fxpts[2*i+1][0], fxpts[2*i+1][1], 'go')\n",
    "    \n",
    "# for ax in axs.flat:\n",
    "#     ax.set(xlabel='Fixed Point PC0', ylabel='Fixed Point PC1')\n",
    "\n",
    "# for ax in axs.flat:\n",
    "#     ax.label_outer()\n",
    "    \n",
    "# fig.legend([\"New Data\", \"Old Data\"], loc = (0.4, 0), ncol=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_points.pkl', 'rb') as f:\n",
    "    fxpts_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fxpts_list), fxpts_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "# for i in range(N_SPLITS):\n",
    "#     axs[int(i/5), i%5].plot(fxpts_list[2*i][0], fxpts_list[2*i][1], 'bo')\n",
    "#     axs[int(i/5), i%5].set_title(f'Split #{i}')\n",
    "    \n",
    "#     axs[int(i/5), i%5].plot(fxpts_list[2*i+1][0], fxpts_list[2*i+1][1], 'go')\n",
    "    \n",
    "# for ax in axs.flat:\n",
    "#     ax.set(xlabel='Fixed Point PC0', ylabel='Fixed Point PC1')\n",
    "\n",
    "# for ax in axs.flat:\n",
    "#     ax.label_outer()\n",
    "    \n",
    "# fig.legend([\"New Data\", \"Old Data\"], loc = (0.4, 0), ncol=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(f'r2c_lstm_sra3_1_j_050_00+new_split0.h5')\n",
    "# lstm = model.layers[10].get_weights()\n",
    "# output = model.layers[9].output\n",
    "# factor_model = tf.keras.Model(model.input, output)\n",
    "# factor_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factors = factor_model(X_rates)\n",
    "# print(X_rates.shape, factors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tf.keras.layers.Input(shape=factors.shape[1:])\n",
    "# state_lstm = tf.keras.layers.LSTM(model_kwargs['n_rnn'],\n",
    "#                                   kernel_regularizer=tf.keras.regularizers.l2(model_kwargs['l2_reg']),\n",
    "#                                   recurrent_regularizer=tf.keras.regularizers.l2(model_kwargs['l2_reg']),\n",
    "#                                   return_sequences=True,\n",
    "#                                   name='state_rnn1')(inputs)\n",
    "# state_model = tf.keras.Model(inputs, state_lstm)\n",
    "\n",
    "# state_model.layers[-1].set_weights(lstm)\n",
    "\n",
    "# state_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_outputs = state_model(factors)\n",
    "# print(state_outputs.shape)\n",
    "# print(state_outputs[0].shape, state_outputs[1].shape, state_outputs[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = np.reshape(state_outputs, (state_outputs.shape[0] * state_outputs.shape[0], state_outputs.shape[2]))\n",
    "# print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# tmp = pca.fit_transform(tmp)\n",
    "# print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states_pc = np.reshape(tmp, (state_outputs.shape[0], state_outputs.shape[1], tmp.shape[1]))\n",
    "# print(states_pc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fxpts_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping over all splits for the new and old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = []\n",
    "# fixed_points = []\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "#     print(f'Working on Split #{i}, new data')\n",
    "#     model = tf.keras.models.load_model(f'r2c_lstm_sra3_1_j_050_00+_split{i}.h5')\n",
    "#     lstm = model.layers[10].get_weights()\n",
    "#     output = model.layers[9].output\n",
    "#     factor_model = tf.keras.Model(model.input, output)\n",
    "#     factors = factor_model(X_rates)\n",
    "#     inputs = tf.keras.layers.Input(shape=factors.shape[1:])\n",
    "#     state_lstm = tf.keras.layers.LSTM(model_kwargs['n_rnn'],\n",
    "#                                       kernel_regularizer=tf.keras.regularizers.l2(model_kwargs['l2_reg']),\n",
    "#                                       recurrent_regularizer=tf.keras.regularizers.l2(model_kwargs['l2_reg']),\n",
    "#                                       return_sequences=True,\n",
    "#                                       name='state_rnn1')(inputs)\n",
    "#     state_model = tf.keras.Model(inputs, state_lstm)\n",
    "#     state_model.layers[-1].set_weights(lstm)\n",
    "#     state_outputs = state_model(factors)\n",
    "    \n",
    "#     pca = PCA(n_components=2)\n",
    "#     tmp = np.reshape(state_outputs, (state_outputs.shape[0] * state_outputs.shape[1], state_outputs.shape[2]))\n",
    "#     st_out_pc = pca.fit_transform(tmp)\n",
    "#     tmp = np.reshape(st_out_pc, (state_outputs.shape[0], state_outputs.shape[1], st_out_pc.shape[1]))\n",
    "#     hidden_states.append(tmp)\n",
    "    \n",
    "#     tmp = fxpts_list[2*i].T\n",
    "#     tmp = pca.transform(tmp)\n",
    "#     fixed_points.append(tmp.T)\n",
    "    \n",
    "    print(f'Working on Split #{i}, old data')\n",
    "    model = tf.keras.models.load_model(f'r2c_lstm_sra3_1_j_050_00_v1+old_split{i}.h5')\n",
    "    lstm = model.layers[10].get_weights()\n",
    "    output = model.layers[9].output\n",
    "    factor_model = tf.keras.Model(model.input, output)\n",
    "    factors = factor_model(X_rates_old)\n",
    "    inputs = tf.keras.layers.Input(shape=factors.shape[1:])\n",
    "    state_lstm = tf.keras.layers.LSTM(model_kwargs['n_rnn'],\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l2(model_kwargs['l2_reg']),\n",
    "                                      recurrent_regularizer=tf.keras.regularizers.l2(model_kwargs['l2_reg']),\n",
    "                                      return_sequences=True,\n",
    "                                      name='state_rnn1')(inputs)\n",
    "    state_model = tf.keras.Model(inputs, state_lstm)\n",
    "\n",
    "    state_model.layers[-1].set_weights(lstm)\n",
    "    state_outputs = state_model(factors)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    tmp = np.reshape(state_outputs, (state_outputs.shape[0] * state_outputs.shape[1], state_outputs.shape[2]))\n",
    "    st_out_pc = pca.fit_transform(tmp)\n",
    "    tmp = np.reshape(st_out_pc, (state_outputs.shape[0], state_outputs.shape[1], st_out_pc.shape[1]))\n",
    "    hidden_states.append(tmp)\n",
    "    \n",
    "#     tmp = fxpts_list[2*i+1].T\n",
    "#     tmp = pca.transform(tmp)\n",
    "#     fixed_points.append(tmp.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hidden_states), hidden_states[0].shape)#, len(fixed_points), fixed_points[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_states_norm = hidden_states\n",
    "# fxpts_list_norm = fxpts_list\n",
    "# for i in range(2*N_SPLITS):\n",
    "#     hidden_states_norm[i] =  2 * ((hidden_states_norm[i] - np.min(hidden_states_norm[i]))/(np.max(hidden_states_norm[i]) - np.min(hidden_states_norm[i]))) - 1\n",
    "#     fxpts_list_norm[i] =  2 * ((fxpts_list_norm[i] - np.min(fxpts_list_norm[i]))/(np.max(fxpts_list_norm[i]) - np.min(fxpts_list_norm[i]))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink', 'tab:olive', 'tab:cyan', 'tab:purple']\n",
    "int(Y_new[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(285):\n",
    "#     lbl = f'States Class {int(Y_new[j])}'\n",
    "#     plt.plot(hidden_states[0][j, :, 0], hidden_states[0][j, :, 1], color_map[int(Y_new[j])], label=lbl)\n",
    "    \n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(285):\n",
    "    plt.plot(hidden_states[0][j, :, 0], hidden_states[0][j, :, 1], color_map[int(Y_old[j])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "fig.suptitle(\"New Data\")\n",
    "for i in range(N_SPLITS):\n",
    "    for j in range(X_rates.shape[0]):\n",
    "        axs[int(i/5), i%5].plot(hidden_states[2*i][j, :, 0], hidden_states[2*i][j, :, 1], color_map[int(Y_new[j])])\n",
    "    axs[int(i/5), i%5].set_title(f'Split #{i}')\n",
    "    \n",
    "    axs[int(i/5), i%5].plot(fixed_points[2*i][0], fixed_points[2*i][1], 'ko')\n",
    "    \n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='PC0', ylabel='PC1')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "    \n",
    "fig.legend([\"States Class 0\", \"States Class 1\", \"States Class 2\", \"States Class 3\", \"States Class 4\", \"States Class 5\",\n",
    "            \"States Class 6\", \"States Class 7\", \"Fixed Points\"], loc = (0.4, 0), ncol=5 )\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "fig.suptitle(\"Old Data\")\n",
    "for i in range(N_SPLITS):\n",
    "    for j in range(X_rates_old.shape[0]):\n",
    "        axs[int(i/5), i%5].plot(hidden_states[2*i+1][j, :, 0], hidden_states[2*i+1][j, :, 1], color_map[int(Y_old[j])])\n",
    "    axs[int(i/5), i%5].set_title(f'Split #{i}')\n",
    "    \n",
    "    axs[int(i/5), i%5].plot(fixed_points[2*i+1][0], fixed_points[2*i+1][1], 'ko')\n",
    "    \n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='PC0', ylabel='PC1')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "    \n",
    "fig.legend([\"States Class 0\", \"States Class 1\", \"States Class 2\", \"States Class 3\", \"States Class 4\", \"States Class 5\",\n",
    "            \"States Class 6\", \"States Class 7\", \"Fixed Points\"], loc = (0.4, 0), ncol=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Rule Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from indl.fileio import from_neuropype_h5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import TSNE\n",
    "from itertools import cycle\n",
    "\n",
    "import os\n",
    "\n",
    "if Path.cwd().stem == 'Analysis':\n",
    "    os.chdir(Path.cwd().parent.parent)\n",
    "    \n",
    "    \n",
    "data_path = Path.cwd() / 'StudyLocationRule'/ 'Data' / 'Preprocessed'\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")\n",
    "    \n",
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc\n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': False,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "load_kwargs_ul = {\n",
    "    'valid_outcomes': (9,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': False,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "load_kwargs_all = {\n",
    "    'valid_outcomes': (0, 9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': False,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "## Model Parameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "LABEL_SMOOTHING = 0.2\n",
    "\n",
    "model_kwargs = dict(\n",
    "    filt=8,\n",
    "    kernLength=25,\n",
    "    ds_rate=9,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0001, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "\n",
    "from indl.model import parts\n",
    "from indl.regularizers import KernelLengthRegularizer\n",
    "\n",
    "def make_model(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=8,\n",
    "    kernLength=25,\n",
    "    ds_rate=10,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.25,\n",
    "    activation='relu',\n",
    "    l1_reg=0.000, l2_reg=0.000,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=16,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "    if _input.shape[2] < 10:\n",
    "        kernLength = 4\n",
    "        filt = 4\n",
    "        ds_rate = 4\n",
    "    elif _input.shape[2] < 20:\n",
    "        kernLength = 8\n",
    "        ds_rate = 8\n",
    "    elif _input.shape[2] < 30:\n",
    "        kernLength = 16\n",
    "    \n",
    "    input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "    if len(input_shape) < 4:\n",
    "        input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv2D(filt, (1, kernLength), padding='valid', data_format=None,\n",
    "                                dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "                                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.DepthwiseConv2D((_y.shape.as_list()[1], 1), padding='valid',\n",
    "                                      depth_multiplier=1, data_format=None, dilation_rate=(1, 1),\n",
    "                                      activation=None, use_bias=True, depthwise_initializer='glorot_uniform',\n",
    "                                      bias_initializer='zeros', depthwise_regularizer=None,\n",
    "                                      bias_regularizer=None, activity_regularizer=None,\n",
    "                                      depthwise_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.AveragePooling2D(pool_size=(1, ds_rate))(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.Reshape(_y.shape.as_list()[2:])(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = tf.keras.layers.Dense(latent_dim, activation=activation)(_y)\n",
    "#     _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(_y)\n",
    "#     outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess_ix = 0\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "print(f\"\\nImporting session {sess_id}\")\n",
    "X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spiketrains', **load_kwargs)\n",
    "print(X_rates.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess_ix = 1\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "print(f\"\\nImporting session {sess_id}\")\n",
    "X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "classes, _y = np.unique(Y_class, return_inverse=True)\n",
    "# Y_class = tf.keras.utils.to_categorical(Y_class, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(ax_info['instance_data']['TargetRule'])\n",
    "color = np.array(ax_info['instance_data']['CueColour'])\n",
    "trial = np.array(ax_info['instance_data']['TrialIndex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.zeros(len(target))\n",
    "for i in range(len(label)):\n",
    "    if (target[i]=='DD' and color[i]=='g'):\n",
    "        label[i] = 12\n",
    "    elif (target[i]=='UU' and color[i]=='b'):\n",
    "        label[i] = 1\n",
    "    elif (target[i]=='DR' and color[i]=='r'):\n",
    "        label[i] = 2\n",
    "    elif (target[i]=='UL' and color[i]=='b'):\n",
    "        label[i] = 3\n",
    "    elif (target[i]=='DL' and color[i]=='b'):\n",
    "        label[i] = 4\n",
    "    elif (target[i]=='UR' and color[i]=='g'):\n",
    "        label[i] = 5\n",
    "    elif (target[i]=='LL' and color[i]=='r'):\n",
    "        label[i] = 6\n",
    "    elif (target[i]=='RR' and color[i]=='g'):\n",
    "        label[i] = 7\n",
    "    elif (target[i]=='UU' and color[i]=='r'):\n",
    "        label[i] = 8\n",
    "    elif (target[i]=='DR' and color[i]=='g'):\n",
    "        label[i] = 9\n",
    "    elif (target[i]=='DL' and color[i]=='r'):\n",
    "        label[i] = 10\n",
    "    elif (target[i]=='UR' and color[i]=='r'):\n",
    "        label[i] = 11\n",
    "        \n",
    "keep_idx = np.argwhere(label>0).flatten()\n",
    "\n",
    "new_label = label[keep_idx].flatten().astype(int)\n",
    "new_X = X_rates[keep_idx]\n",
    "new_Y = Y[keep_idx].flatten()\n",
    "new_target = target[keep_idx]\n",
    "new_color = color[keep_idx]\n",
    "new_trial = trial[keep_idx]\n",
    "\n",
    "zer_idx = np.argwhere(new_label==12).flatten()\n",
    "new_label[zer_idx] = 0\n",
    "\n",
    "print(new_X.shape, new_label.shape, new_Y.shape, new_target.shape, new_color.shape, new_trial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = np.zeros((np.size(new_X,0), np.size(new_X,1), np.size(new_X,2)//5 + 1))\n",
    "for tr in range(np.size(new_X,0)):\n",
    "    for ch in range(np.size(new_X,1)):\n",
    "        spk_idx = np.argwhere(new_X[tr,ch,:]==1).flatten()\n",
    "        spk_idx = spk_idx // 5\n",
    "        spikes[tr,ch,spk_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in range(np.size(spikes,1)):\n",
    "    for t in range(np.size(spikes,2)):\n",
    "        if spikes[0,ch,t]==1:\n",
    "            plt.plot(t, ch, '|')\n",
    "plt.vlines(120,-1,36,'grey')\n",
    "plt.vlines(170, -1, 36, 'grey')\n",
    "plt.vlines(370, -1, 36, 'grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_class = tf.keras.utils.to_categorical(new_Y, num_classes=8)\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((spikes, Y_class))\n",
    "\n",
    "# cast data types to GPU-friendly types.\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "# TODO: augmentations (random slicing?)\n",
    "\n",
    "ds_train = ds_train.shuffle(len(new_Y) + 1)\n",
    "ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "randseed = 12345\n",
    "random.seed(randseed)\n",
    "np.random.seed(randseed)\n",
    "tf.random.set_seed(randseed)\n",
    "\n",
    "model = make_model(\n",
    "    spikes,\n",
    "    Y_class.shape[-1],\n",
    "    **model_kwargs\n",
    ")\n",
    "optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "hist = model.fit(x=ds_train, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = model.layers[10].get_weights()\n",
    "output = model.layers[9].output\n",
    "factor_model = tf.keras.Model(model.input, output)\n",
    "factors = factor_model(spikes)\n",
    "inputs = tf.keras.layers.Input(shape=factors.shape[1:])\n",
    "state_lstm = tf.keras.layers.LSTM(model_kwargs['n_rnn'],\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(model_kwargs['l2_reg']),\n",
    "                                  recurrent_regularizer=tf.keras.regularizers.l2(model_kwargs['l2_reg']),\n",
    "                                  return_sequences=True,\n",
    "                                  name='state_rnn1')(inputs)\n",
    "state_model = tf.keras.Model(inputs, state_lstm)\n",
    "state_model.layers[-1].set_weights(lstm)\n",
    "state_outputs = state_model(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(state_outputs).shape,np.array(factors).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(state_outputs[0],'|')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(factors[0],'|')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=32)\n",
    "tmp = np.reshape(state_outputs, (state_outputs.shape[0] * state_outputs.shape[1], state_outputs.shape[2]))\n",
    "pca_values = pca.fit_transform(tmp)\n",
    "tsne_model = TSNE(n_components=2, perplexity=10)\n",
    "tsne_values = tsne_model.fit_transform(pca_values)\n",
    "hidden_states = np.reshape(tsne_values, (state_outputs.shape[0], state_outputs.shape[1], tsne_values.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink', 'tab:olive', 'tab:cyan', 'tab:purple']\n",
    "for tr in range(np.size(hidden_states,0)):\n",
    "    plt.plot(hidden_states[tr,:,0], hidden_states[tr,:,1], '.-', color=color_map[new_Y[tr]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(new_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[10].output)(new_X)\n",
    "print(lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm[100,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = np.array(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "data = {'lstm': lstm,\n",
    "        'label': label,\n",
    "        'spikes': new_X,\n",
    "        'y': new_Y,\n",
    "        'trial': new_trial,\n",
    "        'target': new_target,\n",
    "        'color': new_color}\n",
    "name = 'lstm_output.mat'\n",
    "savemat(name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "\n",
    "for test_sess_ix in range(8):\n",
    "    sess_info = sess_infos[test_sess_ix]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"\\nImporting session {sess_id}\")\n",
    "    X, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spiketrains', **load_kwargs)\n",
    "    target = np.array(ax_info['instance_data']['TargetRule'])\n",
    "    color = np.array(ax_info['instance_data']['CueColour'])\n",
    "    trial = np.array(ax_info['instance_data']['TrialIndex'])\n",
    "    targets = np.zeros(len(target))\n",
    "    for i in range(len(targets)):\n",
    "        if (target[i]=='UU'):\n",
    "            targets[i] = 0\n",
    "        elif (target[i]=='UR'):\n",
    "            targets[i] = 1\n",
    "        elif (target[i]=='RR'):\n",
    "            targets[i] = 2\n",
    "        elif (target[i]=='DR'):\n",
    "            targets[i] = 3\n",
    "        elif (target[i]=='DD'):\n",
    "            targets[i] = 4\n",
    "        elif (target[i]=='DL'):\n",
    "            targets[i] = 5\n",
    "        elif (target[i]=='LL'):\n",
    "            targets[i] = 6\n",
    "        elif (target[i]=='UL'):\n",
    "            targets[i] = 7\n",
    "    spikes = np.zeros((np.size(X,0), np.size(X,1), np.size(X,2)//5 + 1))\n",
    "    for tr in range(np.size(X,0)):\n",
    "        for ch in range(np.size(X,1)):\n",
    "            spk_idx = np.argwhere(X[tr,ch,:]==1).flatten()\n",
    "            spk_idx = spk_idx // 5\n",
    "            spikes[tr,ch,spk_idx] = 1\n",
    "    data = {'spikes': spikes,\n",
    "            'saccades': Y,\n",
    "            'targets': targets,\n",
    "            'colors': color,\n",
    "           'trial': trial}\n",
    "    name = f'{sess_id.replace(\"+\", \"\")}_cor.mat'\n",
    "    savemat(name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import indl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from indl.fileio import from_neuropype_h5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from itertools import cycle\n",
    "\n",
    "import os\n",
    "\n",
    "if Path.cwd().stem == 'Analysis':\n",
    "    os.chdir(Path.cwd().parent.parent)\n",
    "    \n",
    "    \n",
    "data_path = Path.cwd() / 'StudyLocationRule'/ 'Data' / 'Preprocessed'\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")\n",
    "    \n",
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc\n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_error = {\n",
    "    'valid_outcomes': (9,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_all = {\n",
    "    'valid_outcomes': (0,9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.25),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "model_kwargs = dict(\n",
    "    filt=8,\n",
    "    kernLength=20,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "model_kwargs1 = dict(\n",
    "    filt=16,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "model_kwargs2 = dict(\n",
    "    filt=32,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "\n",
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "EPOCHS2 = 100\n",
    "LABEL_SMOOTHING = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# print(\"Num GPUs Available: \", len(physical_devices))\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess_ix = 4\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "segmented_path = Path.cwd() / 'StudyLocationRule' / 'Data' / 'Preprocessed' / 'sra3_1_m_077_0001_segmented.h5'\n",
    "\n",
    "segmented_data = from_neuropype_h5(segmented_path)\n",
    "outcome = np.array(segmented_data[2][1]['axes'][0]['data']['OutcomeCode'])\n",
    "flag = np.argwhere(outcome>-1).flatten()\n",
    "outcome = outcome[flag]\n",
    "Y = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "Y_class = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "X = segmented_data[2][1]['data'][flag]\n",
    "X = np.nan_to_num(X)\n",
    "X = np.transpose(X, (0, 2, 1))\n",
    "block = np.array(segmented_data[2][1]['axes'][0]['data']['Block']).flatten()[flag]\n",
    "b=np.diff(block, axis=0)\n",
    "border=np.array(np.where(b>0)).flatten()\n",
    "to_keep = [0]\n",
    "for i in range(len(border)-2):\n",
    "    if (border[i+1] - border[i]) > 30:\n",
    "        to_keep.append(i+1)\n",
    "if (len(outcome)-border[-1] > 30):\n",
    "    to_keep.append(-1)\n",
    "border = border[to_keep]\n",
    "color = np.array(segmented_data[2][1]['axes'][0]['data']['CueColour']).flatten()[flag]\n",
    "target = np.array(segmented_data[2][1]['axes'][0]['data']['TargetRule']).flatten()[flag]\n",
    "classes = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "\n",
    "print(border)\n",
    "print(sess_id)\n",
    "print(outcome.shape)\n",
    "print(X.shape, Y.shape, np.unique(Y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = np.zeros(np.size(X,0))\n",
    "for i in range(len(rule)):\n",
    "    if (target[i]=='UU'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=0\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=1\n",
    "        else:\n",
    "            rule[i]=2\n",
    "    elif (target[i]=='UR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=3\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=4\n",
    "        else:\n",
    "            rule[i]=5\n",
    "    elif (target[i]=='RR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=6\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=7\n",
    "        else:\n",
    "            rule[i]=8\n",
    "    elif (target[i]=='DR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=9\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=10\n",
    "        else:\n",
    "            rule[i]=11\n",
    "    elif (target[i]=='DD'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=12\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=13\n",
    "        else:\n",
    "            rule[i]=14\n",
    "    elif (target[i]=='DL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=15\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=16\n",
    "        else:\n",
    "            rule[i]=17\n",
    "    elif (target[i]=='LL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=18\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=19\n",
    "        else:\n",
    "            rule[i]=20\n",
    "    elif (target[i]=='UL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=21\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=22\n",
    "        else:\n",
    "            rule[i]=23\n",
    "rule = rule.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(rule, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = np.zeros(len(rule))\n",
    "unique = np.unique(rule)\n",
    "for i in range(len(rules)):\n",
    "    for j in range(len(unique)):\n",
    "        if rule[i] == unique[j]:\n",
    "            rules[i] = j\n",
    "rules = rules.astype(int)\n",
    "np.unique(rules, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_performance = np.zeros(len(outcome))\n",
    "cor = 0\n",
    "b=0\n",
    "tot = 25\n",
    "for i in range(tot):\n",
    "    if outcome[i]==0:\n",
    "        cor += 1\n",
    "\n",
    "m_performance[:tot] = 100 * (cor / tot)\n",
    "# for i in range(tot, len(outcome)):\n",
    "i = tot\n",
    "while i<len(outcome):\n",
    "    if i == border[b]:\n",
    "        cor = 0\n",
    "        for j in range(tot):\n",
    "            if outcome[i+j]==0:\n",
    "                cor += 1\n",
    "        m_performance[i:i+tot] = 100 * (cor / tot)\n",
    "        i += tot\n",
    "        b = (b+1)%len(border)\n",
    "    elif outcome[i] == outcome[i-tot]:\n",
    "        m_performance[i] = m_performance[i-1]\n",
    "        i += 1\n",
    "    elif outcome[i]==0:\n",
    "        cor += 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i += 1\n",
    "    else:\n",
    "        cor -= 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i +=1\n",
    "\n",
    "plt.plot(m_performance)\n",
    "plt.hlines(70,-1,1300,'grey','dashed')\n",
    "plt.hlines(60,-1,1300,'grey','dashed')\n",
    "learned = np.argwhere(m_performance>69).flatten()\n",
    "unlearned = np.argwhere(m_performance<61).flatten()\n",
    "print(len(learned),len(unlearned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=8,\n",
    "    kernLength=25,\n",
    "    ds_rate=10,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.25,\n",
    "    activation='relu',\n",
    "    l1_reg=0.000, l2_reg=0.000,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=16,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "    if _input.shape[2] < 10:\n",
    "        kernLength = 4\n",
    "        filt = 4\n",
    "        ds_rate = 4\n",
    "    elif _input.shape[2] < 20:\n",
    "        kernLength = 8\n",
    "        ds_rate = 8\n",
    "    elif _input.shape[2] < 30:\n",
    "        kernLength = 16\n",
    "    \n",
    "    input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "    if len(input_shape) < 4:\n",
    "        input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv2D(filt, (1, kernLength), padding='valid', data_format=None,\n",
    "                                dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "                                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.DepthwiseConv2D((_y.shape.as_list()[1], 1), padding='valid',\n",
    "                                      depth_multiplier=1, data_format=None, dilation_rate=(1, 1),\n",
    "                                      activation=None, use_bias=True, depthwise_initializer='glorot_uniform',\n",
    "                                      bias_initializer='zeros', depthwise_regularizer=None,\n",
    "                                      bias_regularizer=None, activity_regularizer=None,\n",
    "                                      depthwise_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.AveragePooling2D(pool_size=(1, ds_rate))(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.Reshape(_y.shape.as_list()[2:])(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = tf.keras.layers.Dense(latent_dim, activation=activation)(_y)\n",
    "#     _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(_y)\n",
    "#     outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def make_model2(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=32,\n",
    "    kernLength=16,\n",
    "    n_rnn=32,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.1,\n",
    "    activation='tanh',\n",
    "    l1_reg=0.010, l2_reg=0.010,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=32,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "#     if _input.shape[2] < 10:\n",
    "#         kernLength = 4\n",
    "#         filt = 4\n",
    "#         ds_rate = 4\n",
    "#     elif _input.shape[2] < 20:\n",
    "#         kernLength = 8\n",
    "#         ds_rate = 8\n",
    "#     elif _input.shape[2] < 30:\n",
    "#         kernLength = 16\n",
    "    \n",
    "#     input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "#     if len(input_shape) < 4:\n",
    "#         input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "#     _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv1D(filt, kernLength, strides=1, padding='valid',\n",
    "                                data_format='channels_last', dilation_rate=1, groups=1,\n",
    "                                activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None,\n",
    "                                bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "                                bias_constraint=None)(inputs)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = tf.keras.layers.Dense(latent_dim, activation=activation)(_y)\n",
    "#     _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(_y)\n",
    "#     outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def kfold_pred(sess_id,X_rates,Y_class,name, verbose=1):\n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=np.max(Y_class)+1)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        randseed = 12345\n",
    "        random.seed(randseed)\n",
    "        np.random.seed(randseed)\n",
    "        tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model2(X_rates, _y.shape[-1])\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'{name}_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"\\n\\nSession {sess_id} overall accuracy with CNN/LSTM Model: {accuracy}%\")\n",
    "    \n",
    "    return history, accuracy, pred_y, true_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[learned].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(rules, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = np.transpose(X,(0,2,1))\n",
    "model = make_model2(_X,10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "hist, acc, y_pred, y_true = kfold_pred(sess_id,_X,rules,name='rd_all' ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = []\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    print(f'Working on Split #{i}')\n",
    "    model = tf.keras.models.load_model(f'rd_all_sra3_1_m_077_00+01_split{i}.h5')\n",
    "    lstm = model.layers[4].get_weights()\n",
    "    output = model.layers[3].output\n",
    "    factor_model = tf.keras.Model(model.input, output)\n",
    "    factors = factor_model.predict(_X)\n",
    "    inputs = tf.keras.layers.Input(shape=factors.shape[1:])\n",
    "    state_lstm = tf.keras.layers.LSTM(32,\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                      recurrent_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                      return_sequences=True,\n",
    "                                      name='state_rnn1')(inputs)\n",
    "    state_model = tf.keras.Model(inputs, state_lstm)\n",
    "    state_model.layers[-1].set_weights(lstm)\n",
    "    state_outputs = state_model(factors)\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    tmp = np.reshape(state_outputs, (state_outputs.shape[0] * state_outputs.shape[1], state_outputs.shape[2]))\n",
    "    st_out_pc = pca.fit_transform(tmp)\n",
    "    tmp = np.reshape(st_out_pc, (state_outputs.shape[0], state_outputs.shape[1], st_out_pc.shape[1]))\n",
    "    hidden_states.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hidden_states), hidden_states[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_norm = hidden_states\n",
    "for i in range(N_SPLITS):\n",
    "    hidden_states_norm[i] =  2 * ((hidden_states_norm[i] - np.min(hidden_states_norm[i]))/(np.max(hidden_states_norm[i]) - np.min(hidden_states_norm[i]))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink', 'tab:olive', 'tab:cyan', 'tab:purple']\n",
    "for j in range(len(hidden_states_norm[0])):\n",
    "#     lbl = f'States Class {int(Y_new[j])}'\n",
    "    plt.plot(hidden_states[0][j, ::15, 0], hidden_states[0][j, :, 1], color_map[int(Y[j])])\n",
    "plt.show()    \n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    filt=32,\n",
    "    kernLength=16,\n",
    "    ds_rate=8,\n",
    "    n_rnn=200,\n",
    "    n_rnn2=200,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0001, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "model = make_model(X, Y_class.shape[-1], **model_kwargs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "hist, acc, y_pred, y_true = kfold_pred(sess_id,X[learned],rule[learned],name='rd_l' ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, acc, y_pred, y_true = kfold_pred(sess_id,X[unlearned],rule[unlearned],name='rd_ul' ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'rd_l_sra3_1_j_050_00+_split0.h5')\n",
    "output = model.layers[10].output\n",
    "factor_model = tf.keras.Model(model.input, output)\n",
    "inpt = np.transpose(X[learned],(0,2,1))\n",
    "XL = factor_model(X[learned])\n",
    "YL = Y[learned]\n",
    "pca_values = np.zeros((XL.shape[0],XL.shape[1], 32))\n",
    "pca =PCA(n_components=32)\n",
    "# FAs = [FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0)]\n",
    "# TSNEs = [TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10)]\n",
    "for i in range(XL.shape[0]):\n",
    "    print(f'Trial {i}')\n",
    "    trial = np.squeeze(XL[i])\n",
    "    if i==0:\n",
    "        pca_values[i] = pca.fit_transform(trial)\n",
    "#         print('Factor Analysis')\n",
    "#         pfa_values[i] = FAs[YL[i]].fit_transform(pca_values)\n",
    "    else:\n",
    "        pca_values[i] = pca.transform(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_model.summary()\n",
    "print(XL.shape)\n",
    "a = np.transpose(X[learned],(0,2,1))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = []\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    print(f'Working on Split #{i}, Learned Trials')\n",
    "    model = tf.keras.models.load_model(f'rd_l_sra3_1_j_050_00+_split{i}.h5')\n",
    "    output = model.layers[10].output\n",
    "    factor_model = tf.keras.Model(model.input, output)\n",
    "    factors = factor_model(X[learned])\n",
    "    \n",
    "    pca = PCA(n_components=50)\n",
    "    tmp = np.reshape(factors, (factors.shape[0] * factors.shape[1], factors.shape[2]))\n",
    "    pca_values = pca.fit_transform(tmp)\n",
    "    fa_values = FactorAnalysis(n_components=10, random_state=0).fit_transform(pca_values)\n",
    "    tsne_values = TSNE(n_components=3,perplexity=10).fit_transform(fa_values)\n",
    "    tmp = np.reshape(tsne_values, (factors.shape[0], factors.shape[1], fa_values.shape[1]))\n",
    "    hidden_states.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XL = np.transpose(X[learned], (0, 2, 1))\n",
    "YL = Y[learned]\n",
    "pca_values = np.zeros((XL.shape[0],XL.shape[1], 32))\n",
    "PCAs = [PCA(n_components=32),PCA(n_components=32),PCA(n_components=32),PCA(n_components=32),PCA(n_components=32),PCA(n_components=32),PCA(n_components=32),PCA(n_components=32)]\n",
    "# FAs = [FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0),FactorAnalysis(n_components=10, random_state=0)]\n",
    "# TSNEs = [TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10),TSNE(n_components=3,perplexity=10)]\n",
    "for i in range(XL.shape[0]):\n",
    "    print(f'Trial {i}')\n",
    "    trial = np.squeeze(XL[i])\n",
    "    if i==np.argmax(YL==YL[i]):\n",
    "        print(f'Found first trial of class {YL[i]}')\n",
    "        print('PCA')\n",
    "        pca_values[i] = PCAs[YL[i]].fit_transform(trial)\n",
    "#         print('Factor Analysis')\n",
    "#         pfa_values[i] = FAs[YL[i]].fit_transform(pca_values)\n",
    "    else:\n",
    "        print('PCA')\n",
    "        pca_values[i] = PCAs[YL[i]].transform(trial)\n",
    "#         print('Factor Analysis')\n",
    "#         pfa_values[i] = FAs[YL[i]].transform(pca_values)\n",
    "#     print('tSNE')\n",
    "#     ptfa_values[i] = TSNEs[YL[i]].fit_transform(fa_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i==np.argmax(YL==YL[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:pink', 'tab:olive', 'tab:cyan', 'tab:purple']\n",
    "for tr in range(285):\n",
    "    plt.plot(pca_values[tr,:,0], pca_values[tr,:,1], '.-', color=color_map[YL[tr]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

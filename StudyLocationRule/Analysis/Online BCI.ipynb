{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# for device in gpu_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "import indl\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from indl.fileio import from_neuropype_h5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from itertools import cycle\n",
    "from filterpy.kalman.UKF import UnscentedKalmanFilter as ukf\n",
    "from filterpy.kalman import MerweScaledSigmaPoints\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "if Path.cwd().stem == 'Analysis':\n",
    "    os.chdir(Path.cwd().parent.parent)\n",
    "\n",
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc    \n",
    "    \n",
    "data_path = Path.cwd() / 'StudyLocationRule'/ 'Data' / 'Preprocessed'\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")\n",
    "    \n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0, 9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "load_kwargs_ul = {\n",
    "    'valid_outcomes': (0, 9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, -1),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_error = {\n",
    "    'valid_outcomes': (9,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_all = {\n",
    "    'valid_outcomes': (0,9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "model_kwargs = dict(\n",
    "    filt=8,\n",
    "    kernLength=20,\n",
    "    ds_rate=5,\n",
    "    n_rnn=32,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=32\n",
    ")\n",
    "model_kwargs1 = dict(\n",
    "    filt=16,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "model_kwargs2 = dict(\n",
    "    filt=32,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "\n",
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "EPOCHS2 = 100\n",
    "LABEL_SMOOTHING = 0.2\n",
    "\n",
    "from indl.model import parts\n",
    "from indl.model.helper import check_inputs\n",
    "from indl.regularizers import KernelLengthRegularizer\n",
    "\n",
    "def make_model(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=32,\n",
    "    kernLength=16,\n",
    "    n_rnn=32,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.1,\n",
    "    activation='tanh',\n",
    "    l1_reg=0.010, l2_reg=0.010,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=32,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "#     if _input.shape[2] < 10:\n",
    "#         kernLength = 4\n",
    "#         filt = 4\n",
    "#         ds_rate = 4\n",
    "#     elif _input.shape[2] < 20:\n",
    "#         kernLength = 8\n",
    "#         ds_rate = 8\n",
    "#     elif _input.shape[2] < 30:\n",
    "#         kernLength = 16\n",
    "    \n",
    "#     input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "#     if len(input_shape) < 4:\n",
    "#         input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "#     _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv1D(filt, kernLength, strides=1, padding='valid', dilation_rate=1, groups=1,\n",
    "                                activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None,\n",
    "                                bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "                                bias_constraint=None)(inputs)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = tf.keras.layers.Dense(latent_dim, activation=activation)(_y)\n",
    "#     _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(_y)\n",
    "#     outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def kfold_pred(sess_id,X_rates,Y_class,name, verbose=1):\n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=np.max(Y_class)+1)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "#         randseed = 12345\n",
    "#         random.seed(randseed)\n",
    "#         np.random.seed(randseed)\n",
    "#         tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model(X_rates, _y.shape[-1])\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'{name}_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"Accuracy: {accuracy}%\\n\\n\")\n",
    "    \n",
    "    return history, accuracy, pred_y, true_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e9ae9",
   "metadata": {},
   "source": [
    "# Stream Monkey Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e76364",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Stream via LSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess_ix = 1\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "sess_id = sess_id.replace(\"+\", \"\")+\"_v1\"\n",
    "print(f\"\\nImporting session {sess_id}\")\n",
    "x_tmp, y_tmp, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "y_tmp = np.array(y_tmp).flatten()\n",
    "shuffler = np.random.permutation(len(x_tmp))\n",
    "X_rates = x_tmp[shuffler]\n",
    "Y = y_tmp[shuffler]\n",
    "Y_class = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "times = np.array(ax_info['timestamps'])\n",
    "target = np.array(ax_info['instance_data']['TargetRule'])\n",
    "color = np.array(ax_info['instance_data']['CueColour'])\n",
    "print(X_rates.shape, Y.shape, np.unique(Y, return_counts=True))\n",
    "print(times.shape, target.shape, color.shape)\n",
    "print(np.argwhere(times==0), np.argwhere(times==0.25), np.argwhere(times==1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c60065",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "X = X_rates[:150]\n",
    "y = Y_class[:150]\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X[:100], y[:100]))\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices((X[100:], y[100:]))\n",
    "\n",
    "# cast data types to GPU-friendly types.\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "# TODO: augmentations (random slicing?)\n",
    "\n",
    "ds_train = ds_train.shuffle(100 + 1)\n",
    "ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "\n",
    "model = make_model(X, y.shape[-1])\n",
    "optim = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                 verbose=1,\n",
    "                 validation_data=ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "t = 150\n",
    "EPOCHS = 50\n",
    "X_t = np.reshape(X[-1], [1, X.shape[1], X.shape[2]])\n",
    "y_t = np.reshape(y[-1], [1, y.shape[1]])\n",
    "X = X_rates[:150]\n",
    "y = Y_class[:150]\n",
    "while t < 662:\n",
    "    X_new = np.reshape(X_rates[t], [1, X_rates.shape[1], X_rates.shape[2]])\n",
    "    y_new = Y[t]\n",
    "    t = t + 1\n",
    "    \n",
    "    pred = np.argmax(model.predict(X_new))\n",
    "    print(f\"Predicted: {pred}, True: {y_new}\")\n",
    "    X_t = np.concatenate((X_t, X_new), axis=0)\n",
    "    Y_new = np.reshape(tf.keras.utils.to_categorical(y_new, num_classes=8), [1, 8])\n",
    "    y_t = np.concatenate((y_t, Y_new), axis=0)\n",
    "    i = i + 1\n",
    "    if i == 50:\n",
    "        print(\"Updating...\")\n",
    "        X = np.concatenate((X[-100:], X_t), axis=0)\n",
    "        y = np.concatenate((y[-100:], y_t), axis=0)\n",
    "        X_t = np.reshape(X[-1], [1, X.shape[1], X.shape[2]])\n",
    "        y_t = np.reshape(y[-1], [1, y.shape[1]])\n",
    "        \n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X[:-50], y[:-50]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X[-50:], y[-50:]))\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_train = ds_train.shuffle(X.shape[0] - 50 + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "        model.fit(x=ds_train, epochs=EPOCHS, verbose=0, validation_data=ds_valid)\n",
    "        print(\"Model Updated.\")\n",
    "        i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b7a9ff",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESS_IDX = 0          # Index of recording session we will use. 0:8\n",
    "BIN_DURATION = 0.250  # Width of window used to bin spikes, in seconds\n",
    "N_TAPS = 2            # Number of bins of history used in a sequence.\n",
    "P_TRAIN = 0.8         # Proportion of data used for training.\n",
    "BATCH_SIZE = 32       # Number of sequences in each training step.\n",
    "P_DROPOUT = 0.05      # Proportion of units to set to 0 on each step.\n",
    "N_RNN_UNITS = 64      # Size of RNN output (state)\n",
    "L2_REG = 0.000017       # Parameter regularization strength.\n",
    "STATEFUL = False      # Whether or not to keep state between sequences (True is not tested)\n",
    "EPOCHS = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_joeyo_reaching(data_path, sess_id, x_chunk='lfps', zscore=False):\n",
    "    \"\"\"\n",
    "    Load data from the joeyo dataset.\n",
    "    :param data_path: path to joeyo data dir (i.e., parent of 'converted'\n",
    "    :param sess_id: 'indy_2016' + one of '0921_01', '0927_04', '0927_06', '0930_02', '0930_05' '1005_06' '1006_02'\n",
    "    :param x_chunk: 'lfps' (default), 'mu_rates', 'su_rates', 'spiketimes', or 'mu_spiketimes'.\n",
    "    :param zscore: Set to True to z-score data before returning. default: False\n",
    "    :return: X, Y, X_ax_info, Y_ax_info\n",
    "    \"\"\"\n",
    "    file_path = Path(data_path) / 'converted' / (sess_id + '.h5')\n",
    "    chunks = from_neuropype_h5(file_path)\n",
    "    chunk_names = [_[0] for _ in chunks]\n",
    "\n",
    "    Y_chunk = chunks[chunk_names.index('behav')][1]\n",
    "    Y_ax_types = [_['type'] for _ in Y_chunk['axes']]\n",
    "    Y_ax_info = {'channel_names': Y_chunk['axes'][Y_ax_types.index('space')]['names'],\n",
    "                 'timestamps': Y_chunk['axes'][Y_ax_types.index('time')]['times'],\n",
    "                 'fs': Y_chunk['axes'][Y_ax_types.index('time')]['nominal_rate']}\n",
    "\n",
    "    X_chunk = chunks[chunk_names.index(x_chunk)][1]\n",
    "    X_ax_types = [_['type'] for _ in X_chunk['axes']]\n",
    "    X_ax_info = {'channel_names': X_chunk['axes'][X_ax_types.index('space')]['names'],\n",
    "                 'timestamps': X_chunk['axes'][X_ax_types.index('time')]['times'],\n",
    "                 'fs': X_chunk['axes'][X_ax_types.index('time')]['nominal_rate']}\n",
    "\n",
    "    if zscore:\n",
    "        X_chunk['data'] = (X_chunk['data'] - np.mean(X_chunk['data'], axis=1, keepdims=True))\\\n",
    "                          / np.std(X_chunk['data'], axis=1, keepdims=True)\n",
    "        Y_chunk['data'] = (Y_chunk['data'] - np.mean(Y_chunk['data'], axis=1, keepdims=True)) \\\n",
    "                          / np.std(Y_chunk['data'], axis=1, keepdims=True)\n",
    "\n",
    "    return X_chunk['data'], Y_chunk['data'], X_ax_info, Y_ax_info\n",
    "\n",
    "def load_dat_with_vel_accel(datadir, sess_idx):\n",
    "    BEHAV_CHANS = ['CursorX', 'CursorY']\n",
    "    sess_names = ['indy_201' + _ for _ in ['60921_01', '60927_04', '60927_06', '60930_02', '60930_05', '61005_06',\n",
    "                                       '61006_02', '60124_01', '60127_03']]\n",
    "    X, Y, X_ax_info, Y_ax_info = load_joeyo_reaching(datadir, sess_names[sess_idx], x_chunk='mu_spiketimes')\n",
    "\n",
    "    # Slice Y to only keep required behaviour data (cursor position)\n",
    "    b_keep_y_chans = np.in1d(Y_ax_info['channel_names'] , BEHAV_CHANS)\n",
    "    Y = Y[b_keep_y_chans, :]\n",
    "    Y_ax_info['channel_names'] = [_ for _ in Y_ax_info['channel_names'] if _ in BEHAV_CHANS]\n",
    "\n",
    "    # Calculate discrete derivative and double-derivative to get velocity and acceleration.\n",
    "    vel = np.diff(Y, axis=1)\n",
    "    vel = np.concatenate((vel[:, 0][:, None], vel), axis=1)  # Assume velocity was constant across the first two samples.\n",
    "    accel = np.concatenate(([[0], [0]], np.diff(vel, axis=1)), axis=1)  # Assume accel was 0 in the first sample.\n",
    "    Y = np.concatenate((Y, vel, accel), axis=0)\n",
    "    Y_ax_info['channel_names'] += ['VelX', 'VelY', 'AccX', 'AccY']\n",
    "    \n",
    "    return X, Y, X_ax_info, Y_ax_info\n",
    "\n",
    "def bin_spike_times(X, X_ax_info, bin_duration=0.256, bin_step_dur=0.004):\n",
    "    bin_samples = int(np.ceil(bin_duration * X_ax_info['fs']))\n",
    "    bin_starts_t = np.arange(X_ax_info['timestamps'][0], X_ax_info['timestamps'][-1], bin_step_dur)\n",
    "    bin_starts_idx = np.searchsorted(X_ax_info['timestamps'], bin_starts_t)\n",
    "    \n",
    "    # Only keep bins that do not extend beyond the data limit.\n",
    "    b_full_bins = bin_starts_idx <= (X.shape[-1] - bin_samples)\n",
    "    bin_starts_idx = bin_starts_idx[b_full_bins]\n",
    "    bin_starts_t = bin_starts_t[b_full_bins]\n",
    "    \n",
    "    # The next chunk of code counts the number of spikes in each bin.\n",
    "    # Create array of indices to reslice the raster data\n",
    "    bin_ix = np.arange(bin_samples)[:, None] + bin_starts_idx[None, :]\n",
    "    # Create buffer to hold the dense raster data\n",
    "    _temp = np.zeros(X[0].shape, dtype=bool)\n",
    "    # Create output variable to hold spike counts per bin\n",
    "    _X = np.zeros((len(bin_starts_idx), X.shape[0]), dtype=np.int32)\n",
    "    for chan_ix in range(X.shape[0]):\n",
    "        _X[:, chan_ix] = np.sum(X[chan_ix].toarray(out=_temp)[0][bin_ix], axis=0)\n",
    "    _X = _X / bin_duration\n",
    "\n",
    "    return _X.astype(np.float32), bin_starts_t, bin_samples\n",
    "\n",
    "def get_binned_rates_with_history(_X, Y, X_ax_info, bin_starts_t, bin_samples, n_taps=3):\n",
    "    bin_stops_t = bin_starts_t + bin_samples / X_ax_info['fs']\n",
    "    bin_stops_t = bin_stops_t[(N_TAPS-1):]\n",
    "\n",
    "    _X_tapped = np.lib.stride_tricks.as_strided(_X, shape=(len(bin_stops_t), N_TAPS, _X.shape[-1]),\n",
    "                                                strides=(_X.strides[-2], _X.strides[-2], _X.strides[-1]))\n",
    "    \n",
    "    b_keep_y = Y_ax_info['timestamps'] > bin_stops_t[0]\n",
    "    n_extra_y = np.sum(b_keep_y) - len(bin_stops_t)\n",
    "    if n_extra_y > 0:\n",
    "        b_keep_y[-n_extra_y] = False\n",
    "    _Y = Y[:, b_keep_y].T\n",
    "    \n",
    "    _X_tapped = _X_tapped[:_Y.shape[0], :, :]\n",
    "    bin_stops_t = bin_stops_t[:_Y.shape[0]]\n",
    "    \n",
    "    return _X_tapped, _Y.astype(np.float32), bin_stops_t\n",
    "\n",
    "def get_binned_rates(_X, Y, X_ax_info, bin_starts_t, bin_samples, n_taps=1):\n",
    "    bin_stops_t = bin_starts_t + bin_samples / X_ax_info['fs']\n",
    "\n",
    "    _X_tapped = np.lib.stride_tricks.as_strided(_X, shape=(len(bin_stops_t), 1, _X.shape[-1]),\n",
    "                                                strides=(_X.strides[-2], _X.strides[-2], _X.strides[-1]))\n",
    "    \n",
    "    b_keep_y = Y_ax_info['timestamps'] > bin_stops_t[0]\n",
    "    n_extra_y = np.sum(b_keep_y) - len(bin_stops_t)\n",
    "    if n_extra_y > 0:\n",
    "        b_keep_y[-n_extra_y] = False\n",
    "    _Y = Y[:, b_keep_y].T\n",
    "    \n",
    "    _X_tapped = _X_tapped[:_Y.shape[0], :, :]\n",
    "    bin_stops_t = bin_stops_t[:_Y.shape[0]]\n",
    "    \n",
    "    return _X_tapped, _Y.astype(np.float32), bin_stops_t\n",
    "\n",
    "\n",
    "def prepare_for_tensorflow(_X_tapped, _Y, p_train=0.8, batch_size=32, stateful=False):\n",
    "    if stateful:\n",
    "        # If using stateful then we keep the sequences in order.\n",
    "        valid_start = int(np.ceil(_X_tapped.shape[0] * p_train))\n",
    "        X_train = _X_tapped[:valid_start]\n",
    "        Y_train = _Y[:valid_start]\n",
    "        X_valid = _X_tapped[valid_start:]\n",
    "        Y_valid = _Y[valid_start:]\n",
    "    else:\n",
    "        # If not using stateful then we shuffle sequences.\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_valid, Y_train, Y_valid = train_test_split(_X_tapped, _Y, train_size=p_train)\n",
    "\n",
    "    # Get mean and std of training data for z-scoring\n",
    "    _X_mean = np.nanmean(X_train[:, 0, :], axis=0)[None, None, :]\n",
    "    _X_std = np.nanstd(X_train[:, 0, :], axis=0)[None, None, :]\n",
    "    _Y_mean = np.nanmean(Y_train, axis=0, keepdims=True)\n",
    "    _Y_std = np.nanstd(Y_train, axis=0, keepdims=True)\n",
    "\n",
    "    # Z-score both training and testing data. For Y, only center it.\n",
    "    X_train = (X_train - _X_mean) / _X_std\n",
    "    X_valid = (X_valid - _X_mean) / _X_std\n",
    "    Y_train = (Y_train - _Y_mean)  # / _Y_std  # Only standardize Y if top layer demands it.\n",
    "    Y_valid = (Y_valid - _Y_mean)  # / _Y_std  # Only standardize Y if top layer demands it.\n",
    "    \n",
    "    # Also transform all of _X_tapped and _Y for plotting.\n",
    "    _X_tapped = (_X_tapped - _X_mean) / _X_std\n",
    "    _Y = (_Y - _Y_mean)\n",
    "\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "    ds_valid = tf.data.Dataset.from_tensor_slices((X_valid, Y_valid))\n",
    "\n",
    "    if not stateful:\n",
    "        ds_train = ds_train.shuffle(int(_X_tapped.shape[0] * P_TRAIN) + 1)\n",
    "    ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    \n",
    "    return _X_tapped, _Y, ds_train, ds_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac79324",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, X_ax_info, Y_ax_info = load_dat_with_vel_accel(data_path, SESS_IDX)\n",
    "_X, bin_starts_t, bin_samples = bin_spike_times(X, X_ax_info, bin_duration=BIN_DURATION, bin_step_dur=(1 / Y_ax_info['fs']))\n",
    "_X_tapped, _Y, bin_stops_t = get_binned_rates_with_history(_X, Y, X_ax_info, bin_starts_t, bin_samples, n_taps=N_TAPS)\n",
    "print(_X.shape,_X_tapped.shape, _Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e786ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98614b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for y in range(88):\n",
    "    for x in range(1000):\n",
    "        if X[y,x]:\n",
    "            pl.plot(x, y, '|')\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(pl.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a341fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "t = 0\n",
    "while t < 1000:\n",
    "    pl.plot(_Y[t:t+50,0], _Y[t:t+50,1],'.-', color='blue')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "    t = t+1\n",
    "#     if t%5000 == 0:\n",
    "#         plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c68d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regression_model(\n",
    "    _input,\n",
    "    output_shape,\n",
    "    n_rnn_units=100,\n",
    "    p_dropout=0.3,\n",
    "    l2_reg=0.001,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "#     if _input.shape[2] < 10:\n",
    "#         kernLength = 4\n",
    "#         filt = 4\n",
    "#         ds_rate = 4\n",
    "#     elif _input.shape[2] < 20:\n",
    "#         kernLength = 8\n",
    "#         ds_rate = 8\n",
    "#     elif _input.shape[2] < 30:\n",
    "#         kernLength = 16\n",
    "    \n",
    "#     input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "#     if len(input_shape) < 4:\n",
    "#         input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "#     _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "#     _y = tf.keras.layers.Dense(64, activation='linear')(inputs)\n",
    "#     _y = tf.keras.layers.Dense(32, activation='tanh')(inputs)\n",
    "#     _y = tf.keras.layers.Conv1D(64, 3, strides=1, padding='valid', dilation_rate=1, groups=1,\n",
    "#                                 activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "#                                 bias_initializer='zeros', kernel_regularizer=None,\n",
    "#                                 bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "#                                 bias_constraint=None)(_y)\n",
    "#     _y = tf.keras.layers.Dense(64, activation='linear')(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn_units, dropout=p_dropout, recurrent_dropout=0,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False, stateful=False)(inputs)\n",
    "#     _y = tf.keras.layers.Dropout(P_DROPOUT)(_y)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(output_shape, activation='linear')(_y)    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520baf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "X = _X_tapped[:5000]\n",
    "y = _Y[:5000]\n",
    "X, y, ds_train, ds_valid = prepare_for_tensorflow(X, y, p_train=P_TRAIN, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = make_regression_model(X, y.shape[-1],n_rnn_units=N_RNN_UNITS,p_dropout=P_DROPOUT,l2_reg=L2_REG)\n",
    "optim = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "loss_obj = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(optimizer=optim, loss=loss_obj, metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0650891a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit(x=ds_train, epochs=EPOCHS, verbose=1, validation_data=ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, output_shape,\n",
    "               n_rnn_units=100,\n",
    "               p_dropout=0.3,\n",
    "               l2_reg=0.001,\n",
    "               stateful=False):\n",
    "    # Note: batch_shape, not shape, if using stateful\n",
    "    if stateful:\n",
    "        inputs = tf.keras.layers.Input(batch_shape=input_shape)\n",
    "    else:\n",
    "        inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn_units, dropout=p_dropout, recurrent_dropout=0,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False, stateful=stateful)(inputs)\n",
    "    if p_dropout > 0.0:\n",
    "        _y = tf.keras.layers.Dropout(p_dropout)(_y)\n",
    "    outputs = tf.keras.layers.Dense(output_shape, activation='linear')(_y)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "input_shape = _X_tapped.shape[1:]\n",
    "if STATEFUL:\n",
    "    input_shape = (BATCH_SIZE,) + input_shape\n",
    "    \n",
    "model = make_model(input_shape, _Y.shape[1],\n",
    "                   n_rnn_units=N_RNN_UNITS,\n",
    "                   p_dropout=P_DROPOUT,\n",
    "                   l2_reg=L2_REG,\n",
    "                   stateful=STATEFUL)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade75606",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=ds_train, epochs=EPOCHS, verbose=1, validation_data=ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee630c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = _X_tapped[:5000]\n",
    "# y = _Y[:5000]\n",
    "pred = model.predict(X)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pred[:,0], color='tab:blue', label='LSTM')\n",
    "plt.plot(y[:,0], color='tab:green', label='True')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unscented we use the acceleration too\n",
    "# Also the states and neural data are concatenated\n",
    "X, Y, X_ax_info, Y_ax_info = load_dat_with_vel_accel(data_path, SESS_IDX)\n",
    "_X, bin_starts_t, bin_samples = bin_spike_times(X, X_ax_info, bin_duration=BIN_DURATION, bin_step_dur=(1 / Y_ax_info['fs']))\n",
    "neural, states, bin_stops_t = get_binned_rates(_X, Y, X_ax_info, bin_starts_t, bin_samples, n_taps=1)\n",
    "neural = np.squeeze(neural).T\n",
    "training_size = 5000\n",
    "\n",
    "# state = np.concatenate((states.T, neural))\n",
    "state = states.T\n",
    "state_train = state[:, :training_size]\n",
    "state_test = state[:, training_size:]\n",
    "neural_train = neural[:, :training_size]\n",
    "neural_test = neural[:, training_size:]\n",
    "\n",
    "dim_state = np.size(states, 1)\n",
    "dim_neural = np.size(neural, 0)\n",
    "\n",
    "time_step = 1 / X_ax_info['fs']\n",
    "\n",
    "# Initializing the Kalman matrices\n",
    "x1 = state_train[:, :-1]\n",
    "x2 = state_train[:, 1:]\n",
    "tmp1 = x2 @ x1.T\n",
    "tmp2 = np.linalg.inv(x1 @ x1.T)\n",
    "F = tmp1 @ tmp2\n",
    "\n",
    "tmp1 = x2 - (F @ x1)\n",
    "Q = (tmp1 @ tmp1.T) / (dim_state - 1)\n",
    "\n",
    "tmp1 = neural_train @ state_train.T\n",
    "tmp2 = np.linalg.inv(state_train @ state_train.T)\n",
    "H = tmp1 @ tmp2\n",
    "\n",
    "tmp1 = neural_train - (H @ state_train)\n",
    "tmp2 = tmp1 @ tmp1.T\n",
    "R = np.divide(tmp2, dim_state)\n",
    "\n",
    "# The transition and observation functions for UKF\n",
    "def transition(state, time_step = time_step):\n",
    "    output = np.zeros_like(state)\n",
    "    cursor = state\n",
    "    output = F @ cursor\n",
    "#     output[6:] = state[6:]\n",
    "    return output\n",
    "\n",
    "\n",
    "def observation (state):\n",
    "    return H @ state[:6]\n",
    "  \n",
    "# Defining sigma points to pass onto UKF\n",
    "points = MerweScaledSigmaPoints(np.size(state, 0), alpha=1., beta=2., kappa=0)\n",
    "\n",
    "# Instantiating from UKF class\n",
    "myukf = ukf(dim_x = dim_state, dim_z = dim_neural, dt = time_step, fx = transition, hx = observation, points = points)\n",
    "\n",
    "# Initializing the UKF object\n",
    "myukf.x = state[:, training_size]\n",
    "myukf.R = R\n",
    "myukf.Q = np.eye(dim_state)\n",
    "myukf.Q[:np.size(Q, 0), :np.size(Q, 1)] = Q\n",
    "\n",
    "# Looping through test data\n",
    "rng = 3000\n",
    "\n",
    "ukf_predict = np.zeros((dim_state, rng))\n",
    "for i in range(rng):\n",
    "    neural_in = neural[:, i + training_size]\n",
    "    myukf.predict(dt=time_step)\n",
    "    myukf.update(neural_in)\n",
    "    ukf_predict[:, i] = myukf.x\n",
    "    if (i-training_size) % 200 == 0:\n",
    "        print('UKF - Step: ' + str(i) + ' out of ' + str(rng))\n",
    "print('UKF Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = _X_tapped[training_size:training_size+3000]\n",
    "y = _Y[training_size:training_size+3000]\n",
    "pred = model.predict(X)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pred[:,0], color='tab:blue', label='LSTM')\n",
    "plt.plot(y[:,0], color='tab:green', label='True')\n",
    "plt.plot(ukf_predict[0, :], color='tab:orange',label='UKF')\n",
    "plt.ylabel('X')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pred[:,1], color='tab:blue', label='LSTM')\n",
    "plt.plot(y[:,1], color='tab:green', label='True')\n",
    "plt.plot(ukf_predict[1, :], color='tab:orange',label='UKF')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pred[:,2], color='tab:blue', label='LSTM')\n",
    "plt.plot(y[:,2], color='tab:green', label='True')\n",
    "plt.plot(ukf_predict[2, :], color='tab:orange',label='UKF')\n",
    "plt.ylabel('V_X')\n",
    "plt.legend()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pred[:,3], color='tab:blue', label='LSTM')\n",
    "plt.plot(y[:,3], color='tab:green', label='True')\n",
    "plt.plot(ukf_predict[3, :], color='tab:orange',label='UKF')\n",
    "plt.ylabel('V_Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83804092",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import matplotlib.pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "i = 0\n",
    "t = 15000\n",
    "EPOCHS = 10\n",
    "X_t = np.reshape(X[-1], [1, X.shape[1], X.shape[2]])\n",
    "y_t = np.reshape(y[-1], [1, y.shape[1]])\n",
    "X = _X_tapped[:15000]\n",
    "y = _Y[:15000]\n",
    "while t < 89800:\n",
    "    X_new = np.reshape(_X_tapped[t], [1, _X_tapped.shape[1], _X_tapped.shape[2]])\n",
    "    y_new = np.reshape(_Y[t], [1, _Y.shape[1]])\n",
    "    t = t + 1\n",
    "    \n",
    "    X_t = np.concatenate((X_t, X_new), axis=0)\n",
    "    y_t = np.concatenate((y_t, y_new), axis=0)\n",
    "    \n",
    "    pred = model.predict(X_t)\n",
    "    pl.plot(t-15000,pred[-1,0], 'o', color='blue')\n",
    "    pl.plot(t-15000,y_new[0,0], 'o', color='green')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "#     time.sleep(0.01)\n",
    "    \n",
    "    i = i + 1\n",
    "#     if i == 500:\n",
    "#         print(\"Updating...\")\n",
    "#         X = np.concatenate((X[-1000:], X_t), axis=0)\n",
    "#         y = np.concatenate((y[-1000:], y_t), axis=0)\n",
    "#         X_t = np.reshape(X[-1], [1, X.shape[1], X.shape[2]])\n",
    "#         y_t = np.reshape(y[-1], [1, y.shape[1]])\n",
    "        \n",
    "#         X, y, ds_train, ds_valid = prepare_for_tensorflow(X, y, p_train=P_TRAIN, batch_size=BATCH_SIZE)\n",
    "#         optim = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "#         loss_obj = tf.keras.losses.MeanSquaredError()\n",
    "#         model.compile(optimizer=optim, loss=loss_obj, metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "#         model.fit(x=ds_train, epochs=EPOCHS, verbose=0, validation_data=ds_valid)\n",
    "#         print(\"Model Updated.\")\n",
    "#         i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eedd0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualKalmanFilter(object):\n",
    "    def __init__(self, x, z):\n",
    "        self.x = x\n",
    "        self.z = z\n",
    "        self.m = np.size(self.x, 0)\n",
    "        self.n = np.size(self.z, 0)\n",
    "        self.F = np.zeros((self.m, self.m))\n",
    "        self.H = np.zeros((self.n, self.m))\n",
    "        self.Q = np.zeros((self.m, self.m))\n",
    "        self.R = np.zeros((self.n, self.n))\n",
    "        self.xk_bar = np.zeros(self.m)\n",
    "        self.pk_bar = np.zeros((self.m, self.m))\n",
    "        self.P = np.eye(self.m)\n",
    "        self.model_initialize()\n",
    "\n",
    "    def model_initialize(self):\n",
    "        # F = X2.X1^T.(X1.X1^T)^-1\n",
    "        x1 = self.x[:, :-1]\n",
    "        x2 = self.x[:, 1:]\n",
    "        temp1 = np.dot(x2, x1.T)\n",
    "        temp2 = np.linalg.inv(np.dot(x1, x1.T))\n",
    "        self.F = np.dot(temp1, temp2)\n",
    "        # Q = ((X2 - F.X1).(X2 - FX1)^T) / (M-1)\n",
    "        temp = x2 - np.dot(self.F, x1)\n",
    "        self.Q = np.dot(temp, temp.T) / (self.m - 1)\n",
    "        # H = Z.X^T.(X.X^T)^-1\n",
    "        temp1 = np.dot(self.z, self.x.T)\n",
    "        temp2 = np.linalg.inv(np.dot(self.x, self.x.T))\n",
    "        self.H = np.dot(temp1, temp2)\n",
    "        # R = ((Z - H.X).(Z - H.X)^T) / M\n",
    "        Z_HX = self.z - np.dot(self.H, self.x)\n",
    "        temp = np.dot(Z_HX, Z_HX.T)\n",
    "        self.R = np.divide(temp, self.m)\n",
    "\n",
    "    def predict(self):\n",
    "        # I. Priori Step\n",
    "        x_k_minus_one = self.x[:, -1]  # Initial State\n",
    "        p_k_minus_one = self.P  # Initial Error Covariance\n",
    "        self.xk_bar = np.dot(self.F, x_k_minus_one)\n",
    "        temp = np.dot(self.F, p_k_minus_one)\n",
    "        self.pk_bar = np.dot(temp, np.transpose(self.F)) + self.Q\n",
    "\n",
    "    def update(self, z_test):\n",
    "        # Kk = Pk^-.H^T.(H.Pk^-.H^T + R)^-1\n",
    "        temp = np.dot(self.pk_bar, np.transpose(self.H))\n",
    "        temp1 = np.dot(self.H, temp) + self.R\n",
    "        temp2 = np.linalg.inv(temp1)\n",
    "        kk = np.dot(temp, temp2)\n",
    "        # Next State Estimation\n",
    "        zk = z_test\n",
    "        temp1 = zk - np.dot(self.H, self.xk_bar)  # z - dot(H, x)\n",
    "        temp2 = np.dot(kk, temp1)\n",
    "        xk = self.xk_bar + temp2  # x = x + dot(K, y)\n",
    "        # Next Error Covariance Estimation\n",
    "        temp = np.eye(self.m) - np.dot(kk, self.H)  # I_KH = self._I - dot(self.K, H)\n",
    "        pk = np.dot(temp, self.pk_bar)  # self.P = dot(dot(I_KH, self.P), I_KH.T) + dot(dot(self.K, R), self.K.T)\n",
    "        # Initializing Next Loop\n",
    "        xk = np.reshape(xk, (self.m, 1))\n",
    "        self.x = np.hstack((self.x, xk))\n",
    "        self.P = pk\n",
    "        # Updating the model\n",
    "        zk = np.reshape(zk, (self.n, 1))\n",
    "        self.z = np.hstack((self.z, zk))\n",
    "        return xk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b141ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install filterpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ced967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297363e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1654c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3c28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukf_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120dac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(state[3, training_size:training_size+3000], label='True')\n",
    "plt.plot(ukf_predict[3, :], label='UKF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b80a3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b53b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, X_ax_info, Y_ax_info = load_dat_with_vel_accel(data_path, SESS_IDX)\n",
    "_X, bin_starts_t, bin_samples = bin_spike_times(X, X_ax_info, bin_duration=BIN_DURATION, bin_step_dur=(1 / Y_ax_info['fs']))\n",
    "neural, states, bin_stops_t = get_binned_rates(_X, Y, X_ax_info, bin_starts_t, bin_samples, n_taps=1)\n",
    "neural = np.squeeze(neural).T\n",
    "state = states[:, :4].T\n",
    "\n",
    "# Seperating training and test samples\n",
    "training_size = 5000\n",
    "\n",
    "state_train = state[:, :training_size]\n",
    "neural_train = neural[:, :training_size]\n",
    "state_test = state[:, training_size:]\n",
    "neural_test = neural[:, training_size:]\n",
    "\n",
    "# Initialize predicted states\n",
    "kf_predict = np.zeros((4, 3000))\n",
    "\n",
    "# Instantiating from KF class\n",
    "mykf = ManualKalmanFilter(x=state_train, z = neural_train)\n",
    "\n",
    "# Looping through test data\n",
    "rng = 3000\n",
    "for i in range(rng):\n",
    "    mykf.predict()\n",
    "    neural_in = neural_test[:,i]\n",
    "    kf_predict[:, i] = np.reshape(mykf.update(neural_in), (mykf.m, ))\n",
    "    if i%200 == 0:\n",
    "        print('KF - Step: ' + str(i) + ' out of ' + str(rng))\n",
    "print('KF Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kf_predict[3,:])\n",
    "plt.plot(state[3,5000:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8dd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, X_ax_info, Y_ax_info = load_dat_with_vel_accel(data_path, SESS_IDX)\n",
    "_X, bin_starts_t, bin_samples = bin_spike_times(X, X_ax_info, bin_duration=BIN_DURATION, bin_step_dur=(1 / Y_ax_info['fs']))\n",
    "neural, states, bin_stops_t = get_binned_rates(_X, Y, X_ax_info, bin_starts_t, bin_samples, n_taps=1)\n",
    "neural = np.squeeze(neural)\n",
    "state = states[:, :4].T\n",
    "\n",
    "print(neural.shape, states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bc059",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = neural[:4000, :]\n",
    "y_train_vx = states[:4000, 2]\n",
    "y_train_vy = states[:4000, 3]\n",
    "x_test = neural[4000:, :]\n",
    "y_test_vx = states[4000:, 2]\n",
    "y_test_vy = states[4000:, 3]\n",
    "\n",
    "regressor = SVC(verbose=1).fit(x_train, y_train_vx)\n",
    "pred = regressor.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0b7220",
   "metadata": {},
   "source": [
    "# Imports, Paths, Variables, and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2708e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ebadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import indl\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from indl.fileio import from_neuropype_h5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from itertools import cycle\n",
    "\n",
    "import os\n",
    "\n",
    "if Path.cwd().stem == 'Analysis':\n",
    "    os.chdir(Path.cwd().parent.parent)\n",
    "\n",
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc    \n",
    "    \n",
    "data_path = Path.cwd() / 'StudyLocationRule'/ 'Data' / 'Preprocessed'\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")\n",
    "    \n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "load_kwargs_ul = {\n",
    "    'valid_outcomes': (0, 9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, -1),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_error = {\n",
    "    'valid_outcomes': (9,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_all = {\n",
    "    'valid_outcomes': (0,9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "model_kwargs = dict(\n",
    "    filt=8,\n",
    "    kernLength=20,\n",
    "    ds_rate=5,\n",
    "    n_rnn=32,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=32\n",
    ")\n",
    "model_kwargs1 = dict(\n",
    "    filt=16,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "model_kwargs2 = dict(\n",
    "    filt=32,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "\n",
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "EPOCHS2 = 100\n",
    "LABEL_SMOOTHING = 0.2\n",
    "\n",
    "from indl.model import parts\n",
    "from indl.model.helper import check_inputs\n",
    "from indl.regularizers import KernelLengthRegularizer\n",
    "\n",
    "def make_model(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=32,\n",
    "    kernLength=16,\n",
    "    n_rnn=32,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.1,\n",
    "    activation='tanh',\n",
    "    l1_reg=0.010, l2_reg=0.010,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=32,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "#     if _input.shape[2] < 10:\n",
    "#         kernLength = 4\n",
    "#         filt = 4\n",
    "#         ds_rate = 4\n",
    "#     elif _input.shape[2] < 20:\n",
    "#         kernLength = 8\n",
    "#         ds_rate = 8\n",
    "#     elif _input.shape[2] < 30:\n",
    "#         kernLength = 16\n",
    "    \n",
    "#     input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "#     if len(input_shape) < 4:\n",
    "#         input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "#     _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv1D(filt, kernLength, strides=1, padding='valid',\n",
    "                                data_format='channels_last', dilation_rate=1, groups=1,\n",
    "                                activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None,\n",
    "                                bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "                                bias_constraint=None)(inputs)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = tf.keras.layers.Dense(latent_dim, activation=activation)(_y)\n",
    "#     _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(_y)\n",
    "#     outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def kfold_pred(sess_id,X_rates,Y_class,name, verbose=1):\n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=np.max(Y_class)+1)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        randseed = 12345\n",
    "        random.seed(randseed)\n",
    "        np.random.seed(randseed)\n",
    "        tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model(X_rates, _y.shape[-1])\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'{name}_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"\\n\\nSession {sess_id} overall accuracy with CNN/LSTM Model: {accuracy}%\")\n",
    "    \n",
    "    return history, accuracy, pred_y, true_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5ee25",
   "metadata": {},
   "source": [
    "# Behavioural Analysis (Figure 1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe817de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess_ix = 1\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "segmented_path = Path.cwd() / 'StudyLocationRule' / 'Data' / 'Preprocessed' / 'sra3_1_j_050_00_segmented_v2.h5'\n",
    "\n",
    "segmented_data = from_neuropype_h5(segmented_path)\n",
    "outcome = segmented_data[2][1]['axes'][0]['data']['OutcomeCode']\n",
    "Y = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass'])\n",
    "Y_class = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "X = segmented_data[2][1]['data']\n",
    "X = np.nan_to_num(X)\n",
    "from scipy import signal\n",
    "kernel = signal.gaussian(100, 20)\n",
    "X_conv = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[2]):\n",
    "        X_conv[i,:,j] = signal.fftconvolve(np.squeeze(X[i,:,j]), kernel, mode='same')\n",
    "X_conv = np.abs(X_conv)\n",
    "X_conv = np.transpose(X_conv, (0, 2, 1))\n",
    "X_conv = X_conv[:,:,::20]\n",
    "block = np.array(segmented_data[2][1]['axes'][0]['data']['Block']).flatten()\n",
    "b=np.diff(block, axis=0)\n",
    "blck=np.array(np.where(b>0)).flatten()\n",
    "color = np.array(segmented_data[2][1]['axes'][0]['data']['CueColour']).flatten()\n",
    "target = np.array(segmented_data[2][1]['axes'][0]['data']['TargetRule']).flatten()\n",
    "classes = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59246d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.diff(block, axis=0)\n",
    "border = np.array(np.where(b > 0)).flatten()\n",
    "to_keep = [0]\n",
    "for i in range(len(border) - 1):\n",
    "    if (border[i + 1] - border[i]) > 30 and (len(outcome)-border[i+1]) > 30:\n",
    "        to_keep.append(i + 1)\n",
    "border = border[to_keep]\n",
    "m_performance = np.zeros(len(outcome))\n",
    "cor = 0\n",
    "b = 0\n",
    "tot = 25\n",
    "for i in range(tot):\n",
    "    if outcome[i] == 0:\n",
    "        cor += 1\n",
    "\n",
    "m_performance[:tot] = 100 * (cor / tot)\n",
    "i = tot\n",
    "while i < len(outcome):\n",
    "    if i == border[b]:\n",
    "        cor = 0\n",
    "        for j in range(tot):\n",
    "            if outcome[i + j] == 0:\n",
    "                cor += 1\n",
    "        m_performance[i:i + tot] = 100 * (cor / tot)\n",
    "        i += tot\n",
    "        b = (b + 1) % len(border)\n",
    "    elif outcome[i] == outcome[i - tot]:\n",
    "        m_performance[i] = m_performance[i - 1]\n",
    "        i += 1\n",
    "    elif outcome[i] == 0:\n",
    "        cor += 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i += 1\n",
    "    else:\n",
    "        cor -= 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd24dc",
   "metadata": {},
   "source": [
    "# Temporal Analysis of Rule Decoding (Figure 3A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_info = sess_infos[8]\n",
    "sess_id = sess_info['exp_code']\n",
    "sess_id = sess_id.replace(\"+\", \"\")+\"_v1\"\n",
    "X_rates, _, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_args)\n",
    "times = np.array(ax_info['timestamps'])\n",
    "target = np.array(ax_info['instance_data']['TargetRule'])\n",
    "color = np.array(ax_info['instance_data']['CueColour'])\n",
    "rule = np.zeros(np.size(X_rates,0))\n",
    "for i in range(len(rule)):\n",
    "    if (target[i]=='UU'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=0\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=1\n",
    "        else:\n",
    "            rule[i]=2\n",
    "    elif (target[i]=='UR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=3\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=4\n",
    "        else:\n",
    "            rule[i]=5\n",
    "    elif (target[i]=='RR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=6\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=7\n",
    "        else:\n",
    "            rule[i]=8\n",
    "    elif (target[i]=='DR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=9\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=10\n",
    "        else:\n",
    "            rule[i]=11\n",
    "    elif (target[i]=='DD'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=12\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=13\n",
    "        else:\n",
    "            rule[i]=14\n",
    "    elif (target[i]=='DL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=15\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=16\n",
    "        else:\n",
    "            rule[i]=17\n",
    "    elif (target[i]=='LL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=18\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=19\n",
    "        else:\n",
    "            rule[i]=20\n",
    "    elif (target[i]=='UL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=21\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=22\n",
    "        else:\n",
    "            rule[i]=23\n",
    "rule = rule.astype(int)\n",
    "tmp = rule\n",
    "_yu = np.unique(rule)\n",
    "for i in range(len(tmp)):\n",
    "    rule[i] = np.where(_yu == tmp[i])[0][0]\n",
    "rule_shuf = np.random.permutation(rule)\n",
    "for t in range(33):\n",
    "    X = X_rates[:,:,:t*5 + 5]\n",
    "    _, rul_acc_dnn_all[1,t], _, _ = kfold_pred(sess_id,X,rule,name='R2R_TEMP', verbose=0)\n",
    "    _, rul_acc_dnn_shuf[1,t], _, _ = kfold_pred(sess_id,X,rule_shuf,name='R2R_TEMP_SHUF', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "t = times[::5][1:-1]\n",
    "plt.plot(t, rul_acc_dnn_all[1,1:-1], linewidth=3, label=\"Rule Decoding\")\n",
    "plt.plot(t, rul_acc_dnn_shuf[1,1:-1], linewidth=3, label=\"Chance Level\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d9f974",
   "metadata": {},
   "source": [
    "# Temporal Analysis of Saccade Decoding (Figure 3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ffc892",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "sac_acc = np.zeros(41)\n",
    "sess = 2\n",
    "sess_info = sess_infos[sess]\n",
    "sess_id = sess_info['exp_code']\n",
    "sess_id = sess_id.replace(\"+\", \"\")+\"_v1\"\n",
    "X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "times = np.array(ax_info['timestamps'])\n",
    "for t in range(40):\n",
    "    tmp = X_rates[:,:,:t*5 + 5]\n",
    "    kf = KFold(n_splits=N_SPLITS)\n",
    "    X = np.reshape(tmp, (tmp.shape[0], -1))\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        x_tr, x_ts = X[train_index], X[test_index]\n",
    "        y_tr, y_ts = Y[train_index], Y[test_index]\n",
    "        clf = SVC(verbose=False).fit(x_tr, y_tr)\n",
    "        acc = clf.score(x_ts, y_ts)\n",
    "        scores.append(acc)\n",
    "    sac_acc[t] = np.mean(np.array(scores))\n",
    "\n",
    "\n",
    "## Chance Level\n",
    "\n",
    "N_SPLITS = 10\n",
    "sac_acc_shuf = np.zeros(41)\n",
    "sess=2\n",
    "sess_info = sess_infos[sess]\n",
    "sess_id = sess_info['exp_code']\n",
    "sess_id = sess_id.replace(\"+\", \"\")+\"_v1\"\n",
    "X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "Y = Y.ravel()\n",
    "Y_shuf = np.random.permutation(Y)\n",
    "times = np.array(ax_info['timestamps'])\n",
    "for t in range(40):\n",
    "    tmp = X_rates[:,:,:t*5 + 5]\n",
    "    kf = KFold(n_splits=N_SPLITS)\n",
    "    X = np.reshape(tmp, (tmp.shape[0], -1))\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        x_tr, x_ts = X[train_index], X[test_index]\n",
    "        y_tr, y_ts = Y_shuf[train_index], Y_shuf[test_index]\n",
    "        clf = SVC(verbose=False).fit(x_tr, y_tr)\n",
    "        acc = clf.score(x_ts, y_ts)\n",
    "        scores.append(acc)\n",
    "    sac_acc_shuf[t] = np.mean(np.array(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ddc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "t = times[::5][:-2]\n",
    "plt.plot(t, sac_acc[2,:-1]*100, linewidth=3, label=\"Saccade Decoding\")\n",
    "plt.plot(t, sac_acc_shuf[:-1]*100, linewidth=3, label=\"Chance Level\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06453c00",
   "metadata": {},
   "source": [
    "# Latent Space Analysis (Figure 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bfa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_path = data_path / 'sra3_1_m_074_0001_segmented.h5'\n",
    "segmented_data = from_neuropype_h5(segmented_path)\n",
    "outcome = np.array(segmented_data[2][1]['axes'][0]['data']['OutcomeCode'])\n",
    "flag = np.argwhere(outcome>-1).flatten()\n",
    "outcome = outcome[flag]\n",
    "Y = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "Y_class = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "X_rates = segmented_data[2][1]['data'][flag]\n",
    "X_rates = np.nan_to_num(X_rates)\n",
    "X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "block = np.array(segmented_data[2][1]['axes'][0]['data']['Block']).flatten()[flag]\n",
    "b=np.diff(block, axis=0)\n",
    "border=np.array(np.where(b>0)).flatten()\n",
    "to_keep = [0]\n",
    "for i in range(len(border) - 1):\n",
    "    if (border[i + 1] - border[i]) > 30 and (len(outcome)-border[i+1]) > 30:\n",
    "        to_keep.append(i + 1)\n",
    "border = border[to_keep]\n",
    "color = np.array(segmented_data[2][1]['axes'][0]['data']['CueColour']).flatten()[flag]\n",
    "target = np.array(segmented_data[2][1]['axes'][0]['data']['TargetRule']).flatten()[flag]\n",
    "classes = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "times = np.array(segmented_data[2][1]['axes'][1]['times']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a6e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_performance = np.zeros(len(outcome))\n",
    "cor = 0\n",
    "b=0\n",
    "tot = 25\n",
    "for i in range(tot):\n",
    "    if outcome[i]==0:\n",
    "        cor += 1\n",
    "\n",
    "m_performance[:tot] = 100 * (cor / tot)\n",
    "i = tot\n",
    "while i<len(outcome):\n",
    "    if i == border[b]:\n",
    "        cor = 0\n",
    "        for j in range(tot):\n",
    "            if outcome[i+j]==0:\n",
    "                cor += 1\n",
    "        m_performance[i:i+tot] = 100 * (cor / tot)\n",
    "        i += tot\n",
    "        b = (b+1)%len(border)\n",
    "    elif outcome[i] == outcome[i-tot]:\n",
    "        m_performance[i] = m_performance[i-1]\n",
    "        i += 1\n",
    "    elif outcome[i]==0:\n",
    "        cor += 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i += 1\n",
    "    else:\n",
    "        cor -= 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i +=1\n",
    "\n",
    "for i in range(len(m_performance)):\n",
    "    if m_performance[i]<0:\n",
    "        m_performance[i] = 0\n",
    "learned = np.argwhere(m_performance>80).flatten()\n",
    "unlearned = np.argwhere(m_performance<60).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = np.zeros(np.size(X_rates,0))\n",
    "for i in range(len(rule)):\n",
    "    if (target[i]=='UU'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=0\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=1\n",
    "        else:\n",
    "            rule[i]=2\n",
    "    elif (target[i]=='UR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=3\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=4\n",
    "        else:\n",
    "            rule[i]=5\n",
    "    elif (target[i]=='RR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=6\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=7\n",
    "        else:\n",
    "            rule[i]=8\n",
    "    elif (target[i]=='DR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=9\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=10\n",
    "        else:\n",
    "            rule[i]=11\n",
    "    elif (target[i]=='DD'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=12\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=13\n",
    "        else:\n",
    "            rule[i]=14\n",
    "    elif (target[i]=='DL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=15\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=16\n",
    "        else:\n",
    "            rule[i]=17\n",
    "    elif (target[i]=='LL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=18\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=19\n",
    "        else:\n",
    "            rule[i]=20\n",
    "    elif (target[i]=='UL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=21\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=22\n",
    "        else:\n",
    "            rule[i]=23\n",
    "rule = rule.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule1 = 9\n",
    "rule2 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e617ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = np.array(np.where((rule==rule1)|(rule==rule2))).flatten()\n",
    "l_rl = []\n",
    "ul_rl = []\n",
    "for r in rl:\n",
    "    if r in learned:\n",
    "        l_rl.append(r)\n",
    "    elif r in unlearned:\n",
    "        ul_rl.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a027cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = rule[l_rl]\n",
    "_y_l = np.zeros_like(tmp)\n",
    "for i in range(len(tmp)):\n",
    "    if (tmp[i]==rule2):\n",
    "        _y_l[i]=1\n",
    "tmp = rule[ul_rl]\n",
    "_y_ul = np.zeros_like(tmp)\n",
    "for i in range(len(tmp)):\n",
    "    if (tmp[i]==rule2):\n",
    "        _y_ul[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eeb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "step = 20\n",
    "length = len(times[::step])\n",
    "\n",
    "_X = np.transpose(X_rates, (0, 2, 1))\n",
    "# _X = X_rates\n",
    "\n",
    "class_colors = np.array(['tab:red', 'tab:green', 'tab:blue', 'tab:pink','tab:cyan','tab:orange',\n",
    "                         'tab:olive','tab:purple','maroon', 'lime', 'navy', 'sienna', 'tan', 'black', 'grey'])\n",
    "\n",
    "TEST_PERPLEXITY = [10]\n",
    "X = _X[l_rl]\n",
    "\n",
    "traj_l = np.zeros((X.shape[0], length, 2))\n",
    "\n",
    "pca = PCA(n_components=16)\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY[-1])\n",
    "for t in np.arange(1,length):\n",
    "    tmp = X[:, :step*t, :]\n",
    "    pca_values = pca.fit_transform(tmp.reshape([-1, np.prod(tmp.shape[1:])]))\n",
    "    traj_l[:,t,:] = tsne_model.fit_transform(pca_values)\n",
    "    \n",
    "X = _X[ul_rl]\n",
    "traj_ul = np.zeros((X.shape[0], length, 2))\n",
    "\n",
    "pca = PCA(n_components=16)\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY[-1])\n",
    "for t in np.arange(1,length):\n",
    "    tmp = X[:, :step*t, :]\n",
    "    pca_values = pca.fit_transform(tmp.reshape([-1, np.prod(tmp.shape[1:])]))\n",
    "    traj_ul[:,t,:] = tsne_model.fit_transform(pca_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75cb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import calinski_harabasz_score as chs\n",
    "\n",
    "score_l = np.zeros(traj_l.shape[1])\n",
    "score_ul = np.zeros(traj_ul.shape[1])\n",
    "for i in range(len(score_l)):\n",
    "    score_l[i] = chs(np.squeeze(traj_l[:,i,:]), _y_l)\n",
    "    score_ul[i] = chs(np.squeeze(traj_ul[:,i,:]), _y_ul)\n",
    "t = times[::step]\n",
    "plt.plot(t, score_l, lw=3,label='Learned')\n",
    "plt.plot(t, score_ul, lw=3,label='Unlearned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632bce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "color=['tab:red', 'tab:green']\n",
    "plt.figure()\n",
    "for i in range(len(_y_l)):\n",
    "    plt.plot(np.squeeze(traj_l[i,1,0]), np.squeeze(traj_l[i,1,1]), 'o', color=color[_y_l[i]], lw=1)\n",
    "plt.title('Learned Trials: Pre-Color-Cue')\n",
    "plt.xlabel('Latent 1')\n",
    "plt.ylabel('Latent 2')\n",
    "plt.figure()\n",
    "for i in range(len(_y_l)):\n",
    "    plt.plot(np.squeeze(traj_l[i,6,0]), np.squeeze(traj_l[i,6,1]), 'o', color=color[_y_l[i]], lw=1)\n",
    "plt.title('Learned Trials: Color-Cue Period')\n",
    "plt.xlabel('Latent 1')\n",
    "plt.ylabel('Latent 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = np.array(np.where(outcome==0)).flatten()\n",
    "icor = np.array(np.where(outcome==9)).flatten()\n",
    "c_l = []\n",
    "ic_ul = []\n",
    "for c in cor:\n",
    "    if c in learned:\n",
    "        c_l.append(c)\n",
    "for ic in icor:\n",
    "    if ic in unlearned:\n",
    "        ic_ul.append(ic)\n",
    "print(np.unique(rule[c_l], return_counts=True))\n",
    "print(np.unique(rule[ic_ul], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = X_rates[c_l]\n",
    "rules = rule[c_l]\n",
    "hist, acc, y_pred, y_true = kfold_pred(sess_id,rates,rules,name='cor_l_rd' ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"cor_l_rd_sra3_1_j_050_00_split9.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class_colors = np.array(['tab:blue', 'tab:red', 'tab:green', 'tab:pink','tab:cyan','tab:orange',\n",
    "                         'tab:olive','tab:purple','maroon', 'lime', 'navy', 'sienna', 'tan', 'black', 'grey'])\n",
    "\n",
    "TEST_PERPLEXITY = [10]\n",
    "X = X_rates[c_l]\n",
    "_y = rule[c_l].astype(int)\n",
    "tmp = _y\n",
    "_yu = np.unique(_y)\n",
    "for i in range(len(tmp)):\n",
    "    _y[i] = np.where(_yu == tmp[i])[0][0]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "def plot_tsne(x_vals, y_vals, perplexity, title='Model Output'):\n",
    "    plt.scatter(x=x_vals[:, 0], y=x_vals[:, 1], color=class_colors[y_vals])\n",
    "    plt.xlabel('Latent D-1')\n",
    "    plt.ylabel('Latent D-2')\n",
    "    plt.title(title)\n",
    "    ax = plt.gca()\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "pca_values = pca.fit_transform(X.reshape([-1, np.prod(X.shape[1:])]))\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY[-1])\n",
    "tsne_values = tsne_model.fit_transform(pca_values)\n",
    "\n",
    "output_layer = -5\n",
    "tbs = 30  # tsne batch size\n",
    "truncated_model = tf.keras.Model(model.input, model.layers[output_layer].output)\n",
    "flattened_output = []\n",
    "for start_ix in range(0, X.shape[0], tbs):\n",
    "    flattened_output.append(truncated_model(X[start_ix:start_ix+tbs, :, :].astype(np.float32)))\n",
    "flattened_output = tf.concat(flattened_output, 0)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "for p_ix, perplexity in enumerate(TEST_PERPLEXITY):\n",
    "    pca = PCA(n_components=30)\n",
    "    pca_values = pca.fit_transform(flattened_output)\n",
    "    tsne_model = TSNE(n_components=2, perplexity=perplexity)\n",
    "    tsne_values = tsne_model.fit_transform(pca_values)\n",
    "    \n",
    "    plt.subplot(1, 3, p_ix + 2)\n",
    "    plot_tsne(tsne_values, _y.ravel(), perplexity, title='LSTM Output Latent States')\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

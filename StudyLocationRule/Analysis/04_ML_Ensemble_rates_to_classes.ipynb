{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform spike rates to behaviour using RNN and CNN ensemble\n",
    "### Setup\n",
    "Environment Setup\n",
    "\n",
    "Configure the local or Google Colab environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "try:\n",
    "    # Only on works on Google Colab\n",
    "    from google.colab import files\n",
    "    %tensorflow_version 2.x\n",
    "    os.chdir('..')\n",
    "    \n",
    "    # Configure kaggle if necessary\n",
    "    if not (Path.home() / '.kaggle').is_dir():\n",
    "        uploaded = files.upload()  # Find the kaggle.json file in your ~/.kaggle directory.\n",
    "        if 'kaggle.json' in uploaded.keys():\n",
    "            !mkdir -p ~/.kaggle\n",
    "            !mv kaggle.json ~/.kaggle/\n",
    "            !chmod 600 ~/.kaggle/kaggle.json\n",
    "    \n",
    "    !pip install git+https://github.com/SachsLab/indl.git\n",
    "    \n",
    "    if Path.cwd().stem == 'MonkeyPFCSaccadeStudies':\n",
    "        os.chdir(Path.cwd().parent)\n",
    "    \n",
    "    if not (Path.cwd() / 'MonkeyPFCSaccadeStudies').is_dir():\n",
    "        !git clone --single-branch --recursive https://github.com/SachsLab/MonkeyPFCSaccadeStudies.git\n",
    "        sys.path.append(str(Path.cwd() / 'MonkeyPFCSaccadeStudies'))\n",
    "    \n",
    "    os.chdir('MonkeyPFCSaccadeStudies')\n",
    "        \n",
    "    !pip install -q kaggle\n",
    "    \n",
    "    # Latest version of SKLearn\n",
    "    !pip install -U scikit-learn\n",
    "    \n",
    "    IN_COLAB = True\n",
    "    \n",
    "except ModuleNotFoundError:    \n",
    "    # chdir to MonkeyPFCSaccadeStudies\n",
    "    if Path.cwd().stem == 'Analysis':\n",
    "        os.chdir(Path.cwd().parent.parent)\n",
    "        \n",
    "    # Add indl repository to path.\n",
    "    # Eventually this should already be pip installed, but it's still under heavy development so this is easier for now.\n",
    "    check_dir = Path.cwd()\n",
    "    while not (check_dir / 'Tools').is_dir():\n",
    "        check_dir = check_dir / '..'\n",
    "    indl_path = check_dir / 'Tools' / 'Neurophys' / 'indl'\n",
    "    sys.path.append(str(indl_path))\n",
    "    \n",
    "    # Make sure the kaggle executable is on the PATH\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ';' + str(Path(sys.executable).parent / 'Scripts')\n",
    "    \n",
    "    IN_COLAB = False\n",
    "\n",
    "# Try to clear any logs from previous runs\n",
    "if (Path.cwd() / 'logs').is_dir():\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(str(Path.cwd() / 'logs'))\n",
    "    except PermissionError:\n",
    "        print(\"Unable to remove logs directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from indl.display import turbo_cmap\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'lines.linewidth': 2,\n",
    "    'lines.markersize': 5,\n",
    "    'xtick.labelsize': 16,\n",
    "    'ytick.labelsize': 16,\n",
    "    'legend.fontsize': 18,\n",
    "    'figure.figsize': (6.4, 6.4)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    data_path = Path.cwd() / 'data' / 'monkey_pfc' / 'converted'\n",
    "else:\n",
    "    data_path = Path.cwd() / 'StudyLocationRule' / 'Data' / 'Preprocessed'\n",
    "\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Prepare to) Load Data\n",
    "\n",
    "We will use a custom function `load_macaque_pfc` to load the data into memory.\n",
    "\n",
    "There are 4 different strings to be passed to the import `x_chunk` argument:\n",
    "* 'analogsignals' - if present. Returns 1 kHz LFPs\n",
    "* 'gaze'          - Returns 2-channel gaze data.\n",
    "* 'spikerates'    - Returns smoothed spikerates\n",
    "* 'spiketrains'\n",
    "\n",
    "The `y_type` argument can be\n",
    "* 'pair and choice' - returns Y as np.array of (target_pair, choice_within_pair)\n",
    "* 'encoded input' - returns Y as np.array of shape (n_samples, 10) (explained below)\n",
    "* 'replace with column name' - returns Y as a vector of per-trial values. e.g., 'sacClass'\n",
    "\n",
    "The actual data we load depends on the particular analysis below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc\n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess_ix = 1\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "print(f\"\\nImporting session {sess_id}\")\n",
    "X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "Y_class = tf.keras.utils.to_categorical(Y_class, num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding trial class (0:7) from spike rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indl.model import parts\n",
    "from indl.model.helper import check_inputs\n",
    "from indl.regularizers import KernelLengthRegularizer\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "@check_inputs\n",
    "def make_model_RNN(\n",
    "    _input,\n",
    "    filt=8,\n",
    "    kernLength=25,\n",
    "    ds_rate=10,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.25,\n",
    "    activation='relu',\n",
    "    l1_reg=0.000, l2_reg=0.000,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=16,\n",
    "    return_model=True\n",
    "):\n",
    "    _y = _input\n",
    "    \n",
    "    input_shape = _input.shape.as_list()\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "#     _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "    if len(input_shape) < 4:\n",
    "        input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    _y = layers.Reshape(input_shape[1:])(_y)\n",
    "    _y = tf.keras.layers.Conv2D(filt, (1, kernLength), padding='valid', data_format=None,\n",
    "                                dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "                                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.DepthwiseConv2D((_y.shape.as_list()[1], 1), padding='valid',\n",
    "                                      depth_multiplier=1, data_format=None, dilation_rate=(1, 1),\n",
    "                                      activation=None, use_bias=True, depthwise_initializer='glorot_uniform',\n",
    "                                      bias_initializer='zeros', depthwise_regularizer=None,\n",
    "                                      bias_regularizer=None, activity_regularizer=None,\n",
    "                                      depthwise_constraint=None, bias_constraint=None)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.AveragePooling2D(pool_size=(1, ds_rate))(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = layers.Reshape(_y.shape.as_list()[2:])(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    _y = parts.Classify(_y, n_classes=8, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return _y\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=_input, outputs=_y)\n",
    "\n",
    "@check_inputs\n",
    "def make_model_CNN(\n",
    "        _input,\n",
    "        F1=8, kernLength=25, F1_kern_reg=None,\n",
    "        D=2, D_pooling=4,\n",
    "        F2=8, F2_kernLength=16,\n",
    "        F2_pooling=8,\n",
    "        dropoutRate=0.25,\n",
    "        activation='relu',\n",
    "        l1_reg=0.000, l2_reg=0.000,\n",
    "        norm_rate=0.25,\n",
    "        latent_dim=16,\n",
    "        return_model=True\n",
    "    ):\n",
    "    \n",
    "    if F1_kern_reg is None:\n",
    "        F1_kern_reg = tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg)\n",
    "    elif isinstance(F1_kern_reg, str) and F1_kern_reg == 'kern_length_regu':\n",
    "        F1_kern_reg = KernelLengthRegularizer((1, kernLength),\n",
    "                                              window_scale=1e-4,\n",
    "                                              window_func='poly',\n",
    "                                              poly_exp=2,\n",
    "                                              threshold=0.0015)\n",
    "        \n",
    "    # EEGNetEnc \n",
    "    _y = parts.EEGNetEnc(_input,\n",
    "                         F1=F1,\n",
    "                         F1_kernLength=kernLength,\n",
    "                         F1_kern_reg=F1_kern_reg,\n",
    "                         D=D,\n",
    "                         D_pooling=D_pooling,\n",
    "                         F2=F2,\n",
    "                         F2_pooling=F2_pooling,\n",
    "                         F2_kernLength=F2_kernLength,\n",
    "                         dropoutRate=dropoutRate)\n",
    "    \n",
    "    # Restore time-dimension that was stripped out by EEGNetEnc\n",
    "    _y = layers.Reshape((1, _input.shape.as_list()[2] // D_pooling // F2_pooling, F2))(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    _y = parts.Classify(_y, n_classes=8, norm_rate=norm_rate)\n",
    "    \n",
    "    if return_model:\n",
    "        return tf.keras.models.Model(inputs=[_input], outputs=[_y])\n",
    "    else:\n",
    "        return _y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 180\n",
    "LABEL_SMOOTHING = 0.2\n",
    "\n",
    "\n",
    "def model_kfold_train(sess_id, branch, verbose=1):\n",
    "#     models = []\n",
    "#     print(f\"Processing session {sess_id}...\")\n",
    "    X_rates, Y_class, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "    \n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        randseed = 12345\n",
    "        random.seed(randseed)\n",
    "        np.random.seed(randseed)\n",
    "        tf.random.set_seed(randseed)\n",
    "        \n",
    "        if branch == 'RNN':\n",
    "            model = make_model_RNN(\n",
    "                ds_train.element_spec[0],\n",
    "                **model_kwargs\n",
    "            )\n",
    "        else:\n",
    "            model = make_model_CNN(\n",
    "                ds_train.element_spec[0],\n",
    "                **model_kwargs\n",
    "            )\n",
    "        \n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'r2c_lstm_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"Session {sess_id} overall accuracy: {accuracy}%\")\n",
    "    \n",
    "    return per_fold_eval, per_fold_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfe = [np.zeros((29,8)), np.zeros((29,8)), np.zeros((29,8)), np.zeros((29,8)), np.zeros((29,8)),\n",
    "       np.zeros((28,8)), np.zeros((28,8)), np.zeros((28,8)), np.zeros((28,8)), np.zeros((28,8))]\n",
    "# rnn1 = [32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
    "# rnn2 = [0, 32, 64, 0, 32, 64, 0, 32, 64, 0, 32, 64, 0, 32, 64, 0, 32, 64]\n",
    "# dense = [0, 0, 0, 32, 32, 32, 64, 64, 64, 0, 0, 0, 32, 32, 32, 64, 64, 64]\n",
    "rnn1 = [32, 32, 64,64]\n",
    "rnn2 = [0, 32, 32, 64]\n",
    "dense = [32, 32, 32, 64]\n",
    "for i in range(len(rnn1)):\n",
    "    print(f\"Training RNN Model # {i + 1}\")\n",
    "    model_kwargs=dict(\n",
    "        filt=8,\n",
    "        kernLength=25,\n",
    "        ds_rate=9,\n",
    "        n_rnn=rnn1[i],\n",
    "        n_rnn2=rnn2[i],\n",
    "        dropoutRate=0.30,\n",
    "        activation='relu',\n",
    "        l1_reg=0.0000, l2_reg=0.003,\n",
    "        norm_rate=0.25,\n",
    "        latent_dim=dense[i]\n",
    "    )\n",
    "    pfe_model, _ = model_kfold_train(sess_id, 'RNN', verbose=0)\n",
    "    pfe = np.add(pfe, pfe_model)\n",
    "\n",
    "print(\"Training the CNN Model\")\n",
    "model_kwargs = dict(\n",
    "    F1=8, kernLength=25, F1_kern_reg=None,\n",
    "    D=2, D_pooling=4,\n",
    "    F2=8, F2_kernLength=16,\n",
    "    F2_pooling=8,\n",
    "    dropoutRate=0.30,\n",
    "    activation='relu',\n",
    "    l1_reg=0.000, l2_reg=0.003,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=16\n",
    ")\n",
    "pfe_model, pft = model_kfold_train(sess_id, 'CNN', verbose=0)\n",
    "pfe = np.add(pfe, pfe_model)\n",
    "\n",
    "pred_y = np.concatenate([np.argmax(_, axis=1) for _ in pfe])\n",
    "true_y = np.concatenate(pft).flatten()\n",
    "accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "print(f\"Session {sess_id} overall ensemble accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# hists = []\n",
    "# accs = []\n",
    "# for sess_info in sess_infos:\n",
    "#     _hist, _acc = get_hists_acc(sess_info['exp_code'], verbose=0)\n",
    "#     hists.append(_hist)\n",
    "#     accs.append(_acc)\n",
    "    \n",
    "# print(accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e53c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f14cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import indl\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from indl.fileio import from_neuropype_h5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from itertools import cycle\n",
    "\n",
    "import os\n",
    "\n",
    "if Path.cwd().stem == 'Analysis':\n",
    "    os.chdir(Path.cwd().parent.parent)\n",
    "\n",
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc    \n",
    "    \n",
    "data_path = Path.cwd() / 'StudyLocationRule'/ 'Data' / 'Preprocessed'\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")\n",
    "    \n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "load_kwargs_ul = {\n",
    "    'valid_outcomes': (0, 9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, -1),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_error = {\n",
    "    'valid_outcomes': (9,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_all = {\n",
    "    'valid_outcomes': (0,9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "model_kwargs = dict(\n",
    "    filt=8,\n",
    "    kernLength=20,\n",
    "    ds_rate=5,\n",
    "    n_rnn=32,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=32\n",
    ")\n",
    "model_kwargs1 = dict(\n",
    "    filt=16,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "model_kwargs2 = dict(\n",
    "    filt=32,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "\n",
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "EPOCHS2 = 100\n",
    "LABEL_SMOOTHING = 0.2\n",
    "\n",
    "from indl.model import parts\n",
    "from indl.model.helper import check_inputs\n",
    "from indl.regularizers import KernelLengthRegularizer\n",
    "\n",
    "def make_model(\n",
    "    _input,\n",
    "    num_classes,\n",
    "    filt=32,\n",
    "    kernLength=16,\n",
    "    n_rnn=32,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.1,\n",
    "    activation='tanh',\n",
    "    l1_reg=0.010, l2_reg=0.010,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=32,\n",
    "    return_model=True\n",
    "):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=_input.shape[1:])\n",
    "    \n",
    "#     if _input.shape[2] < 10:\n",
    "#         kernLength = 4\n",
    "#         filt = 4\n",
    "#         ds_rate = 4\n",
    "#     elif _input.shape[2] < 20:\n",
    "#         kernLength = 8\n",
    "#         ds_rate = 8\n",
    "#     elif _input.shape[2] < 30:\n",
    "#         kernLength = 16\n",
    "    \n",
    "#     input_shape = list(_input.shape)\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "    # input_shape[2] = -1  # Comment out during debug\n",
    "    # _y = layers.Reshape(input_shape[1:])(_input)  # Note that Reshape ignores the batch dimension.\n",
    "\n",
    "    # RNN\n",
    "#     if len(input_shape) < 4:\n",
    "#         input_shape = input_shape + [1]\n",
    "    # The Conv layers are insensitive to the number of samples in the time dimension.\n",
    "    # To make it possible for this trained model to be applied to segments of different\n",
    "    # durations, we need to explicitly state that we don't care about the number of samples.\n",
    "#     _y = tf.keras.layers.Reshape(input_shape[1:])(inputs)\n",
    "    _y = tf.keras.layers.Conv1D(filt, kernLength, strides=1, padding='valid', dilation_rate=1, groups=1,\n",
    "                                activation=None, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "                                bias_initializer='zeros', kernel_regularizer=None,\n",
    "                                bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "                                bias_constraint=None)(inputs)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    _y = tf.keras.layers.LSTM(n_rnn,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=n_rnn2 > 0,\n",
    "                              stateful=False,\n",
    "                              name='rnn1')(_y)\n",
    "    _y = tf.keras.layers.Activation(activation)(_y)\n",
    "    _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "    _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    \n",
    "    if n_rnn2 > 0:\n",
    "        \n",
    "        _y = tf.keras.layers.LSTM(n_rnn2,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              recurrent_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                              return_sequences=False,\n",
    "                              stateful=False,\n",
    "                              name='rnn2')(_y)\n",
    "        _y = tf.keras.layers.Activation(activation)(_y)\n",
    "        _y = tf.keras.layers.BatchNormalization()(_y)\n",
    "        _y = tf.keras.layers.Dropout(dropoutRate)(_y)\n",
    "    \n",
    "    # Dense\n",
    "    _y = tf.keras.layers.Dense(latent_dim, activation=activation)(_y)\n",
    "#     _y = parts.Bottleneck(_y, latent_dim=latent_dim, activation=activation)\n",
    "    \n",
    "    # Classify\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(_y)\n",
    "#     outputs = parts.Classify(_y, n_classes=num_classes, norm_rate=norm_rate)\n",
    "    \n",
    "\n",
    "    if return_model is False:\n",
    "        return outputs\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def kfold_pred(sess_id,X_rates,Y_class,name, verbose=1):\n",
    "    splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "    split_ix = 0\n",
    "    histories = []\n",
    "    per_fold_eval = []\n",
    "    per_fold_true = []\n",
    "\n",
    "    for trn, vld in splitter.split(X_rates, Y_class):\n",
    "        print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=np.max(Y_class)+1)\n",
    "        \n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "        ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "        # TODO: augmentations (random slicing?)\n",
    "\n",
    "        ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "        ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "#         randseed = 12345\n",
    "#         random.seed(randseed)\n",
    "#         np.random.seed(randseed)\n",
    "#         tf.random.set_seed(randseed)\n",
    "        \n",
    "        model = make_model(X_rates, _y.shape[-1])\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "        \n",
    "        best_model_path = f'{name}_{sess_id}_split{split_ix}.h5'\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=best_model_path,\n",
    "                # Path where to save the model\n",
    "                # The two parameters below mean that we will overwrite\n",
    "                # the current checkpoint if and only if\n",
    "                # the `val_loss` score has improved.\n",
    "                save_best_only=True,\n",
    "                monitor='val_accuracy',\n",
    "                verbose=verbose)\n",
    "        ]\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                         verbose=verbose,\n",
    "                         validation_data=ds_valid,\n",
    "                         callbacks=callbacks)\n",
    "        # tf.keras.models.save_model(model, 'model.h5')\n",
    "        histories.append(hist.history)\n",
    "        \n",
    "        model = tf.keras.models.load_model(best_model_path)\n",
    "        per_fold_eval.append(model(X_rates[vld]).numpy())\n",
    "        per_fold_true.append(Y_class[vld])\n",
    "        \n",
    "        split_ix += 1\n",
    "        \n",
    "    # Combine histories into one dictionary.\n",
    "    history = {}\n",
    "    for h in histories:\n",
    "        for k,v in h.items():\n",
    "            if k not in history:\n",
    "                history[k] = v\n",
    "            else:\n",
    "                history[k].append(np.nan)\n",
    "                history[k].extend(v)\n",
    "                \n",
    "    pred_y = np.concatenate([np.argmax(_, axis=1) for _ in per_fold_eval])\n",
    "    true_y = np.concatenate(per_fold_true).flatten()\n",
    "    accuracy = 100 * np.sum(pred_y == true_y) / len(pred_y)\n",
    "    print(f\"Accuracy: {accuracy}%\\n\\n\")\n",
    "    \n",
    "    return history, accuracy, pred_y, true_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117fa0e3",
   "metadata": {},
   "source": [
    "# Rule Latent Space Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954eb2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_path = data_path / 'sra3_1_j_050_00_segmented.h5'\n",
    "segmented_data = from_neuropype_h5(segmented_path)\n",
    "outcome = np.array(segmented_data[1][1]['axes'][0]['data']['OutcomeCode'])\n",
    "flag = np.argwhere(outcome>-1).flatten()\n",
    "outcome = outcome[flag]\n",
    "Y = np.array(segmented_data[1][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "Y_class = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "X_rates = segmented_data[1][1]['data'][flag]\n",
    "X_rates = np.nan_to_num(X_rates)\n",
    "X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "block = np.array(segmented_data[1][1]['axes'][0]['data']['Block']).flatten()[flag]\n",
    "b=np.diff(block, axis=0)\n",
    "border=np.array(np.where(b>0)).flatten()\n",
    "to_keep = [0]\n",
    "for i in range(len(border) - 1):\n",
    "    if (border[i + 1] - border[i]) > 30 and (len(outcome)-border[i+1]) > 30:\n",
    "        to_keep.append(i + 1)\n",
    "border = border[to_keep]\n",
    "color = np.array(segmented_data[1][1]['axes'][0]['data']['CueColour']).flatten()[flag]\n",
    "target = np.array(segmented_data[1][1]['axes'][0]['data']['TargetRule']).flatten()[flag]\n",
    "classes = np.array(segmented_data[1][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "times = np.array(segmented_data[1][1]['axes'][1]['times']).flatten()\n",
    "\n",
    "print(border)\n",
    "print(outcome.shape)\n",
    "print(f'times : {times.shape}')\n",
    "print(X_rates.shape, Y.shape, np.unique(Y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dd6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_performance = np.zeros(len(outcome))\n",
    "cor = 0\n",
    "b=0\n",
    "tot = 25\n",
    "for i in range(tot):\n",
    "    if outcome[i]==0:\n",
    "        cor += 1\n",
    "\n",
    "m_performance[:tot] = 100 * (cor / tot)\n",
    "# for i in range(tot, len(outcome)):\n",
    "i = tot\n",
    "while i<len(outcome):\n",
    "    if i == border[b]:\n",
    "        cor = 0\n",
    "        for j in range(tot):\n",
    "            if outcome[i+j]==0:\n",
    "                cor += 1\n",
    "        m_performance[i:i+tot] = 100 * (cor / tot)\n",
    "        i += tot\n",
    "        b = (b+1)%len(border)\n",
    "    elif outcome[i] == outcome[i-tot]:\n",
    "        m_performance[i] = m_performance[i-1]\n",
    "        i += 1\n",
    "    elif outcome[i]==0:\n",
    "        cor += 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i += 1\n",
    "    else:\n",
    "        cor -= 1\n",
    "        m_performance[i] = 100 * (cor / tot)\n",
    "        i +=1\n",
    "\n",
    "for i in range(len(m_performance)):\n",
    "    if m_performance[i]<0:\n",
    "        m_performance[i] = 0\n",
    "plt.plot(m_performance)\n",
    "learned = np.argwhere(m_performance>75).flatten()\n",
    "unlearned = np.argwhere(m_performance<65).flatten()\n",
    "print(learned.shape, unlearned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e00341",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = np.zeros(np.size(X_rates,0))\n",
    "for i in range(len(rule)):\n",
    "    if (target[i]=='UU'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=0\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=1\n",
    "        else:\n",
    "            rule[i]=2\n",
    "    elif (target[i]=='UR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=3\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=4\n",
    "        else:\n",
    "            rule[i]=5\n",
    "    elif (target[i]=='RR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=6\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=7\n",
    "        else:\n",
    "            rule[i]=8\n",
    "    elif (target[i]=='DR'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=9\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=10\n",
    "        else:\n",
    "            rule[i]=11\n",
    "    elif (target[i]=='DD'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=12\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=13\n",
    "        else:\n",
    "            rule[i]=14\n",
    "    elif (target[i]=='DL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=15\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=16\n",
    "        else:\n",
    "            rule[i]=17\n",
    "    elif (target[i]=='LL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=18\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=19\n",
    "        else:\n",
    "            rule[i]=20\n",
    "    elif (target[i]=='UL'):\n",
    "        if (color[i] == 'r'):\n",
    "            rule[i]=21\n",
    "        elif(color[i] == 'g'):\n",
    "            rule[i]=22\n",
    "        else:\n",
    "            rule[i]=23\n",
    "rule = rule.astype(int)\n",
    "# print(np.unique(rule, return_counts=True))\n",
    "print(f'Unlearned: {np.unique(rule[unlearned], return_counts=True)}')\n",
    "print(f'Learned: {np.unique(rule[learned], return_counts=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3158799",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = np.array(np.where(outcome==0)).flatten()\n",
    "icor = np.array(np.where(outcome==9)).flatten()\n",
    "c_l = []\n",
    "ic_ul = []\n",
    "for c in cor:\n",
    "    if c in learned:\n",
    "        c_l.append(c)\n",
    "for ic in icor:\n",
    "    if ic in unlearned:\n",
    "        ic_ul.append(ic)\n",
    "print(np.unique(rule[c_l], return_counts=True))\n",
    "print(np.unique(rule[ic_ul], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule1 = 18\n",
    "rule2 = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = np.array(np.where((rule==rule1)|(rule==rule2))).flatten()\n",
    "l_rl = []\n",
    "ul_rl = []\n",
    "for r in rl:\n",
    "    if r in learned:\n",
    "        l_rl.append(r)\n",
    "    elif r in unlearned:\n",
    "        ul_rl.append(r)\n",
    "print(np.unique(color[l_rl], return_counts=True))\n",
    "print(np.unique(color[ul_rl], return_counts=True))\n",
    "\n",
    "# rl = np.array(np.where((rule==rule1)|(rule==rule2))).flatten()\n",
    "# l_rl = []\n",
    "# ul_rl = []\n",
    "# for r in rl:\n",
    "#     if r in c_l:\n",
    "#         l_rl.append(r)\n",
    "#     elif r in ic_ul:\n",
    "#         ul_rl.append(r)\n",
    "# print(np.unique(color[l_rl], return_counts=True))\n",
    "# print(np.unique(color[ul_rl], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0edafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = rule[l_rl]\n",
    "_y_l = np.zeros_like(tmp)\n",
    "for i in range(len(tmp)):\n",
    "    if (tmp[i]==rule2):\n",
    "        _y_l[i]=1\n",
    "tmp = rule[ul_rl]\n",
    "_y_ul = np.zeros_like(tmp)\n",
    "for i in range(len(tmp)):\n",
    "    if (tmp[i]==rule2):\n",
    "        _y_ul[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40bed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_score_l_j50 = []\n",
    "sep_score_ul_j50 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2380d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "step = 20\n",
    "windows=[0, np.argwhere(times==0)[0][0], np.argwhere(times==0.25)[0][0], np.argwhere(times==1.25)[0][0], len(times)-1]\n",
    "length = len(windows)\n",
    "\n",
    "_X = np.transpose(X_rates, (0, 2, 1))\n",
    "# _X = X_rates\n",
    "\n",
    "\n",
    "TEST_PERPLEXITY = [10]\n",
    "# for rep in range(92):\n",
    "X = _X[l_rl]\n",
    "\n",
    "traj_l = np.zeros((X.shape[0], length, 2))\n",
    "\n",
    "pca = PCA(n_components=16)\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY[-1])\n",
    "for t in np.arange(1,length):\n",
    "    a = 1\n",
    "    if t==3: a=2\n",
    "    tmp = X[:, windows[t-a]:windows[t], :]\n",
    "    pca_values = pca.fit_transform(tmp.reshape([-1, np.prod(tmp.shape[1:])]))\n",
    "    traj_l[:,t,:] = tsne_model.fit_transform(pca_values)\n",
    "\n",
    "X = _X[ul_rl]\n",
    "traj_ul = np.zeros((X.shape[0], length, 2))\n",
    "\n",
    "pca = PCA(n_components=16)\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY[-1])\n",
    "for t in np.arange(1,length):\n",
    "    a = 1\n",
    "    if t==3: a=2\n",
    "    tmp = X[:, windows[t-a]:windows[t], :]\n",
    "    pca_values = pca.fit_transform(tmp.reshape([-1, np.prod(tmp.shape[1:])]))\n",
    "    traj_ul[:,t,:] = tsne_model.fit_transform(pca_values)    \n",
    "from sklearn.metrics.cluster import calinski_harabasz_score as chs\n",
    "font = {'weight'   : 'bold',\n",
    "       'size': 12}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "score_l = np.zeros(traj_l.shape[1])\n",
    "score_ul = np.zeros(traj_ul.shape[1])\n",
    "for i in range(len(score_l)):\n",
    "    score_l[i] = chs(np.squeeze(traj_l[:,i,:]), _y_l)\n",
    "    score_ul[i] = chs(np.squeeze(traj_ul[:,i,:]), _y_ul)\n",
    "#     sep_score_l_j50.append(score_l)\n",
    "#     sep_score_ul_j50.append(score_ul)\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "t = times[windows]\n",
    "plt.plot(t, score_l, lw=3,label='Learned')\n",
    "plt.plot(t, score_ul, lw=3,label='Unlearned')\n",
    "plt.vlines(0, 0, 10, ls='dashed', color='grey')\n",
    "plt.vlines(0.25, 0, 10, ls='dashed', color='grey')\n",
    "plt.vlines(1.25, 0, 10, ls='dashed', color='grey')\n",
    "plt.title('Rule Separation in Learned vs. Unlearned Trials - Same Direction, Different Color Cue')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Rule Separation Score')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bede8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_score_l_j50.append(score_l)\n",
    "sep_score_ul_j50.append(score_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb08ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sep_score_l_j50.pkl', 'wb') as f:\n",
    "    pickle.dump(sep_score_l_j50, f)\n",
    "with open('sep_score_ul_j50.pkl', 'wb') as f:\n",
    "    pickle.dump(sep_score_ul_j50, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f89f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sep_score_l_m74.pkl', 'rb') as f:\n",
    "    sep_score_l = pickle.load(f)\n",
    "with open('sep_score_ul_m74.pkl', 'rb') as f:\n",
    "    sep_score_ul = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sep_score_l), sep_score_l[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_l = np.array(sep_score_l)[:,3]\n",
    "r_ul = np.array(sep_score_ul)[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322cf1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.kruskal(r_l,r_ul), stats.ranksums(r_l,r_ul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_score_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199e27b7",
   "metadata": {},
   "source": [
    "# Saccade Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18717a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_kwargs = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "accs = np.zeros((8, 4, 20)) # Session X Methods X Runs (Methods Order: DNN, RF, SVM, LLR)\n",
    "accs_shuf = np.zeros((8, 4, 20)) # Session X Methods X Runs (Methods Order: DNN, RF, SVM, LLR)\n",
    "verbose=0\n",
    "N_SPLITS = 5\n",
    "EPOCHS = 100\n",
    "for sess in range(8):\n",
    "    print(f'Sess: {sess}')\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"\\nImporting session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "    X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "    Y_class = Y.ravel()\n",
    "    Y_shuf = np.random.permutation(Y_class)\n",
    "    svm_folds=[]\n",
    "    llr_folds=[]\n",
    "    rf_folds=[]\n",
    "    dnn_folds=[]\n",
    "    true_folds=[]\n",
    "    svm_shuf=[]\n",
    "    llr_shuf=[]\n",
    "    rf_shuf=[]\n",
    "    dnn_shuf=[]\n",
    "    fold_shuf=[]\n",
    "    for rep in range(20):\n",
    "        splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n",
    "        split_ix = 0\n",
    "        print(f'Run: {rep}')\n",
    "        for trn, vld in splitter.split(X_rates, Y_class):\n",
    "            print(f\"\\tSplit {split_ix + 1} of {N_SPLITS}\")\n",
    "            _y = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "            _y_shuf = tf.keras.utils.to_categorical(Y_shuf, num_classes=8)\n",
    "            x_flat_trn = np.reshape(X_rates[trn], (X_rates[trn].shape[0], -1))\n",
    "            x_flat_vld = np.reshape(X_rates[vld], (X_rates[vld].shape[0], -1))\n",
    "            ds_train = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y[trn]))\n",
    "            ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y[vld]))\n",
    "\n",
    "            # cast data types to GPU-friendly types.\n",
    "            ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "            ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "            # TODO: augmentations (random slicing?)\n",
    "\n",
    "            ds_train = ds_train.shuffle(len(trn) + 1)\n",
    "            ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "            ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "            ds_train_shuf = tf.data.Dataset.from_tensor_slices((X_rates[trn], _y_shuf[trn]))\n",
    "            ds_valid_shuf = tf.data.Dataset.from_tensor_slices((X_rates[vld], _y_shuf[vld]))\n",
    "\n",
    "            # cast data types to GPU-friendly types.\n",
    "            ds_train_shuf = ds_train_shuf.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "            ds_valid_shuf = ds_valid_shuf.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "            # TODO: augmentations (random slicing?)\n",
    "\n",
    "            ds_train_shuf = ds_train_shuf.shuffle(len(trn) + 1)\n",
    "            ds_train_shuf = ds_train_shuf.batch(BATCH_SIZE, drop_remainder=True)\n",
    "            ds_valid_shuf = ds_valid_shuf.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "#             randseed = 12345\n",
    "#             random.seed(randseed)\n",
    "#             np.random.seed(randseed)\n",
    "#             tf.random.set_seed(randseed)\n",
    "\n",
    "            model = make_model(X_rates, _y.shape[-1])\n",
    "            optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "            loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "            model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "\n",
    "            best_model_path = f'r2s_{sess_id}_split{split_ix}.h5'\n",
    "            callbacks = [\n",
    "                tf.keras.callbacks.ModelCheckpoint(\n",
    "                    filepath=best_model_path,\n",
    "                    # Path where to save the model\n",
    "                    # The two parameters below mean that we will overwrite\n",
    "                    # the current checkpoint if and only if\n",
    "                    # the `val_loss` score has improved.\n",
    "                    save_best_only=True,\n",
    "                    monitor='val_accuracy',\n",
    "                    verbose=verbose)\n",
    "            ]\n",
    "\n",
    "            hist = model.fit(x=ds_train, epochs=EPOCHS,\n",
    "                             verbose=verbose,\n",
    "                             validation_data=ds_valid,\n",
    "                             callbacks=callbacks)\n",
    "\n",
    "\n",
    "            #RF\n",
    "            clf = RandomForestClassifier(max_depth=6, random_state=0).fit(x_flat_trn, Y_class[trn])\n",
    "            rf_folds.append(clf.predict(x_flat_vld))\n",
    "            #SVM\n",
    "            clf = SVC(verbose=verbose).fit(x_flat_trn, Y_class[trn])\n",
    "            svm_folds.append(clf.predict(x_flat_vld))\n",
    "            #LLR\n",
    "            clf = LR(random_state=0, verbose=verbose).fit(x_flat_trn, Y_class[trn])\n",
    "            llr_folds.append(clf.predict(x_flat_vld))\n",
    "            #DNN\n",
    "            model = tf.keras.models.load_model(best_model_path)\n",
    "            dnn_folds.append(model(X_rates[vld]).numpy())\n",
    "            true_folds.append(Y_class[vld])\n",
    "\n",
    "            model = make_model(X_rates, _y_shuf.shape[-1])\n",
    "            optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "            loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "            model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "\n",
    "            model.fit(x=ds_train_shuf, epochs=10,verbose=verbose)\n",
    "            #RF\n",
    "            clf = RandomForestClassifier(max_depth=6, random_state=0).fit(x_flat_trn, Y_shuf[trn])\n",
    "            rf_shuf.append(clf.predict(x_flat_vld))\n",
    "            #SVM\n",
    "            clf = SVC(verbose=verbose).fit(x_flat_trn, Y_shuf[trn])\n",
    "            svm_shuf.append(clf.predict(x_flat_vld))\n",
    "            #LLR\n",
    "            clf = LR(solver='liblinear', random_state=0, verbose=verbose).fit(x_flat_trn, Y_shuf[trn])\n",
    "            llr_shuf.append(clf.predict(x_flat_vld))\n",
    "            #DNN\n",
    "            dnn_shuf.append(model(X_rates[vld]).numpy())\n",
    "            fold_shuf.append(Y_shuf[vld])\n",
    "            split_ix += 1\n",
    "        pred_y = np.concatenate([np.argmax(_, axis=1) for _ in dnn_folds])\n",
    "        true_y = np.concatenate(true_folds).flatten()\n",
    "        accs[sess, 0, rep] =  100 * np.sum(pred_y == true_y) / len(pred_y)# DNN\n",
    "        print(f'DNN: {accs[sess, 0, rep]}')\n",
    "        pred_y = np.concatenate([_ for _ in rf_folds])\n",
    "        accs[sess, 1, rep] =  100 * np.sum(pred_y == true_y) / len(pred_y)# RF\n",
    "        print(f'RF: {accs[sess, 1, rep]}')\n",
    "        pred_y = np.concatenate([_ for _ in svm_folds])\n",
    "        accs[sess, 2, rep] =  100 * np.sum(pred_y == true_y) / len(pred_y)# SVM\n",
    "        print(f'SVM: {accs[sess, 2, rep]}')\n",
    "        pred_y = np.concatenate([_ for _ in llr_folds])\n",
    "        accs[sess, 3, rep] =  100 * np.sum(pred_y == true_y) / len(pred_y)# LLR\n",
    "        print(f'LLR: {accs[sess, 3, rep]}')\n",
    "        pred_y = np.concatenate([np.argmax(_, axis=1) for _ in dnn_shuf])\n",
    "        true_y = np.concatenate(fold_shuf).flatten()\n",
    "        accs_shuf[sess, 0, rep] =  100 * np.sum(pred_y == true_y) / len(pred_y)# DNN\n",
    "        print(f'DNN_Chance: {accs_shuf[sess, 0, rep]}')\n",
    "        pred_y = np.concatenate([_ for _ in rf_shuf])\n",
    "        accs_shuf[sess, 1, rep] =  100 * np.sum(pred_y == true_y) / len(pred_y)# RF\n",
    "        print(f'RF_Chance: {accs_shuf[sess, 1, rep]}')\n",
    "        pred_y = np.concatenate([_ for _ in svm_shuf])\n",
    "        accs_shuf[sess, 2, rep] =  100 * np.sum(pred_y == true_y) / len(pred_y)# SVM\n",
    "        print(f'SVM_Chance: {accs_shuf[sess, 2, rep]}')\n",
    "        pred_y = np.concatenate([_ for _ in llr_shuf])\n",
    "        accs_shuf[sess, 3, rep] =  100 * np.sum(pred_y == true_y) / len(pred_y)# LLR\n",
    "        print(f'LLR_Chance: {accs_shuf[sess, 3, rep]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods Order: DNN - RF - SVM - LLR\n",
    "# Session X Method X Runs\n",
    "\n",
    "with open('accs_r2s.pkl', 'wb') as f:\n",
    "    pickle.dump(accs, f)\n",
    "with open('accs_r2s_chance.pkl', 'wb') as f:\n",
    "    pickle.dump(accs_shuf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accs_r2s.pkl', 'rb') as f:\n",
    "    accs = np.array(pickle.load(f))\n",
    "with open('accs_r2s_chance.pkl', 'rb') as f:\n",
    "    accs_shuf = np.array(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88799b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sess in range(8):\n",
    "    print(f'Session: {sess}')\n",
    "    acc = accs[sess,0]\n",
    "    chance = accs_shuf[sess,0]\n",
    "    print(stats.kruskal(acc,chance), stats.ranksums(acc,chance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_dnn = np.zeros(8)\n",
    "err_rf = np.zeros(8)\n",
    "err_svm = np.zeros(8)\n",
    "err_rlr = np.zeros(8)\n",
    "for i in range(8):\n",
    "    err_dnn[i] = np.var(accs[i,0,:])\n",
    "    err_rf[i] = np.var(accs[i,1,:])\n",
    "    err_svm[i] = np.var(accs[i,2,:])\n",
    "    err_rlr[i] = np.var(accs[i,3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ee426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 13}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "sessions = ['JL1', 'JL2', 'JL3', 'M1', 'M2', 'M3', 'M4', 'JL4']\n",
    "x = np.arange(0, 2*len(sessions), 2)  # the label locations\n",
    "width = 0.3\n",
    "\n",
    "dnn = np.array([70.625, 88.7719298245614, 78.96678966789668,\n",
    "               77.9783393501805, 68.75, 77.08333333333333, 85.9375, 80.0])\n",
    "rf = np.array([49.375, 67.01754385964912, 56.457564575645755, 54.151624548736464,\n",
    "               39.0625, 67.70833333333333, 60.9375, 70.58823529411765])\n",
    "rlr = np.array([50.625, 80.0, 60.14760147601476, 50.90252707581227,\n",
    "                52.34375, 64.58333333333333, 70.3125, 69.41176470588235])\n",
    "svm = np.array([53.125, 71.9298245614035, 60.14760147601476, 54.51263537906137,\n",
    "                42.96875, 62.5, 56.25, 45.88235294117647])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "rects1 = ax.bar(x - 3*width/2, dnn, width, label='DNN')\n",
    "plt.errorbar(x - 3*width/2, dnn, yerr=err_dnn, fmt='.', markersize='10', capsize=5, color='black')\n",
    "rects2 = ax.bar(x - width/2, rf, width, label='RF')\n",
    "plt.errorbar(x - width/2, rf, yerr=err_rf, fmt='.', markersize='10', capsize=5, color='black')\n",
    "rects4 = ax.bar(x + width/2, svm, width, label='SVM')\n",
    "plt.errorbar(x + width/2, svm, yerr=err_svm, fmt='.', markersize='10', capsize=5, color='black')\n",
    "rects3 = ax.bar(x + 3*width/2, rlr, width, label='RLR')\n",
    "plt.errorbar(x + 3*width/2, rlr, yerr=err_rlr, fmt='.', markersize='10', capsize=5, color='black')\n",
    "\n",
    "ax.set_ylim([0, 95])\n",
    "ax.set_ylabel('Saccade Decoding Accuracy (%)')\n",
    "ax.set_xlabel('Session')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sessions)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.435, .785), fontsize='small')\n",
    "fig.savefig(\"performance.svg\")\n",
    "# fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-banner",
   "metadata": {},
   "source": [
    "# Rule Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess_infos.append ({'name': 'Marty',\n",
    "#                  'bank': 'A',\n",
    "#                  'name_short': 'm',\n",
    "#                  'date': '',\n",
    "#                  'exp_code': 'sra3_1_m_074_00+01',\n",
    "#                  'nsx': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_idx in range(9):\n",
    "    X,_,_,_ = load_session(s_idx)\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session(s_idx):\n",
    "    sess_info = sess_infos[s_idx]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    sess_id = sess_id.replace(\"+\", \"\")+\"_v1\" + \"_segmented.h5\"\n",
    "    segmented_path = data_path / sess_id\n",
    "    segmented_data = from_neuropype_h5(segmented_path)\n",
    "    outcome = np.array(segmented_data[2][1]['axes'][0]['data']['OutcomeCode'])\n",
    "    flag = np.argwhere(outcome>-1).flatten()\n",
    "    outcome = outcome[flag]\n",
    "    Y = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "    Y_class = tf.keras.utils.to_categorical(Y, num_classes=8)\n",
    "    X_rates = segmented_data[2][1]['data'][flag]\n",
    "    X_rates = np.nan_to_num(X_rates)\n",
    "    X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "    block = np.array(segmented_data[2][1]['axes'][0]['data']['Block']).flatten()[flag]\n",
    "    b=np.diff(block, axis=0)\n",
    "    border=np.array(np.where(b>0)).flatten()\n",
    "    to_keep = [0]\n",
    "    for i in range(len(border) - 1):\n",
    "        if (border[i + 1] - border[i]) > 30 and (len(outcome)-border[i+1]) > 30:\n",
    "            to_keep.append(i + 1)\n",
    "    border = border[to_keep]\n",
    "    color = np.array(segmented_data[2][1]['axes'][0]['data']['CueColour']).flatten()[flag]\n",
    "    target = np.array(segmented_data[2][1]['axes'][0]['data']['TargetRule']).flatten()[flag]\n",
    "    classes = np.array(segmented_data[2][1]['axes'][0]['data']['TargetClass']).flatten()[flag]\n",
    "    times = np.array(segmented_data[2][1]['axes'][1]['times']).flatten()\n",
    "    rule = np.zeros(np.size(X_rates,0))\n",
    "    for i in range(len(rule)):\n",
    "        if (target[i]=='UU'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=0\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=1\n",
    "            else:\n",
    "                rule[i]=2\n",
    "        elif (target[i]=='UR'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=3\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=4\n",
    "            else:\n",
    "                rule[i]=5\n",
    "        elif (target[i]=='RR'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=6\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=7\n",
    "            else:\n",
    "                rule[i]=8\n",
    "        elif (target[i]=='DR'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=9\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=10\n",
    "            else:\n",
    "                rule[i]=11\n",
    "        elif (target[i]=='DD'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=12\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=13\n",
    "            else:\n",
    "                rule[i]=14\n",
    "        elif (target[i]=='DL'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=15\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=16\n",
    "            else:\n",
    "                rule[i]=17\n",
    "        elif (target[i]=='LL'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=18\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=19\n",
    "            else:\n",
    "                rule[i]=20\n",
    "        elif (target[i]=='UL'):\n",
    "            if (color[i] == 'r'):\n",
    "                rule[i]=21\n",
    "            elif(color[i] == 'g'):\n",
    "                rule[i]=22\n",
    "            else:\n",
    "                rule[i]=23\n",
    "    rule = rule.astype(int)\n",
    "    tmp = rule\n",
    "    _yu = np.unique(rule)\n",
    "    for i in range(len(tmp)):\n",
    "        rule[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    \n",
    "    m_performance = np.zeros(len(outcome))\n",
    "    cor = 0\n",
    "    b=0\n",
    "    tot = 25\n",
    "    for i in range(tot):\n",
    "        if outcome[i]==0:\n",
    "            cor += 1\n",
    "\n",
    "    m_performance[:tot] = 100 * (cor / tot)\n",
    "    # for i in range(tot, len(outcome)):\n",
    "    i = tot\n",
    "    while i<len(outcome):\n",
    "        if i == border[b]:\n",
    "            cor = 0\n",
    "            for j in range(tot):\n",
    "                if outcome[i+j]==0:\n",
    "                    cor += 1\n",
    "            m_performance[i:i+tot] = 100 * (cor / tot)\n",
    "            i += tot\n",
    "            b = (b+1)%len(border)\n",
    "        elif outcome[i] == outcome[i-tot]:\n",
    "            m_performance[i] = m_performance[i-1]\n",
    "            i += 1\n",
    "        elif outcome[i]==0:\n",
    "            cor += 1\n",
    "            m_performance[i] = 100 * (cor / tot)\n",
    "            i += 1\n",
    "        else:\n",
    "            cor -= 1\n",
    "            m_performance[i] = 100 * (cor / tot)\n",
    "            i +=1\n",
    "\n",
    "    for i in range(len(m_performance)):\n",
    "        if m_performance[i]<0:\n",
    "            m_performance[i] = 0\n",
    "    learned = np.argwhere(m_performance>75).flatten()\n",
    "    unlearned = np.argwhere(m_performance<65).flatten()\n",
    "    \n",
    "    return X_rates, rule, learned, unlearned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = np.zeros((9, 20)) # Session X Methods X Runs (Methods Order: DNN, RF, SVM, LLR)\n",
    "accs_shuf = np.zeros((9, 20)) # Session X Methods X Runs (Methods Order: DNN, RF, SVM, LLR)\n",
    "verbose=0\n",
    "N_SPLITS = 10\n",
    "EPOCHS = 100\n",
    "for sess in range(9):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"\\nImporting session {sess_id}\")\n",
    "    X_rates, rule, learned, unlearned = load_session(sess)\n",
    "    X_rates = X_rates[learned]\n",
    "    Y_class = rule[learned]\n",
    "    tmp = Y_class\n",
    "    _yu = np.unique(Y_class)\n",
    "    for i in range(len(tmp)):\n",
    "        Y_class[i] = np.where(_yu == tmp[i])[0][0]\n",
    "    Y_shuf = np.random.permutation(Y_class)\n",
    "    dnn_folds=[]\n",
    "    true_folds=[]\n",
    "    dnn_shuf=[]\n",
    "    fold_shuf=[]\n",
    "    for rep in range(20):\n",
    "        print(f'\\nRun: {rep}')\n",
    "        _, accs[sess, rep], _, _ = kfold_pred(sess_id,X_rates,Y_class,name=f'r2r_{rep}', verbose=verbose)\n",
    "        print('Chance')\n",
    "        _, accs_shuf[sess, rep], _, _ = kfold_pred(sess_id,X_rates,Y_shuf,name=f'r2r_{rep}', verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accs_r2r.pkl', 'wb') as f:\n",
    "    pickle.dump(accs, f)\n",
    "with open('accs_r2r_chance.pkl', 'wb') as f:\n",
    "    pickle.dump(accs_shuf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accs_r2r.pkl', 'rb') as f:\n",
    "    accs = np.array(pickle.load(f))\n",
    "with open('accs_r2r_chance.pkl', 'rb') as f:\n",
    "    accs_shuf = np.array(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bfda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accs.shape, accs_shuf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = accs[1]\n",
    "b = accs_shuf[1]\n",
    "print(a)\n",
    "print(b)\n",
    "print(stats.kruskal(a,b), stats.ranksums(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sess in range(9):\n",
    "    print(f'Session: {sess}')\n",
    "    acc = accs[sess]\n",
    "    chance = accs_shuf[sess]\n",
    "    print(stats.kruskal(acc,chance), stats.ranksums(acc,chance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-doctor",
   "metadata": {},
   "source": [
    "# Error Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "predicted=[]\n",
    "true=[]\n",
    "\n",
    "load_kwargs_train = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "load_kwargs_test = {\n",
    "    'valid_outcomes': (9, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "accs = np.zeros((8, 4, 20)) # Session X Methods X Runs (Methods Order: DNN, RF, SVM, LLR)\n",
    "accs_shuf = np.zeros((8, 4, 20)) # Session X Methods X Runs (Methods Order: DNN, RF, SVM, LLR)\n",
    "verbose=0\n",
    "EPOCHS = 100\n",
    "for sess in range(8):\n",
    "    print(f'Sess: {sess}')\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    print(f\"\\nImporting session {sess_id}\")\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs_train)\n",
    "    X_rates_t, Y_t, ax_info_t = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs_test)\n",
    "    X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "    X_rates_t = np.transpose(X_rates_t, (0, 2, 1))\n",
    "    Y_class = Y.ravel()\n",
    "    Y_class_t = Y_t.ravel()\n",
    "    Y_shuf = np.random.permutation(Y_class)\n",
    "    Y_shuf_t = np.random.permutation(Y_class_t)\n",
    "    \n",
    "    true.append(Y_class_t)\n",
    "    for rep in range(20):\n",
    "        print(f'Run: {rep}')\n",
    "        _y = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "        _y_shuf = tf.keras.utils.to_categorical(Y_shuf, num_classes=8)\n",
    "        _y_t = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "        _y_t_shuf = tf.keras.utils.to_categorical(Y_shuf, num_classes=8)\n",
    "        x_flat_trn = np.reshape(X_rates, (X_rates.shape[0], -1))\n",
    "        x_flat_tst = np.reshape(X_rates_t, (X_rates_t.shape[0], -1))\n",
    "        ds_train = tf.data.Dataset.from_tensor_slices((X_rates, _y))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        \n",
    "        ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        ds_train_shuf = tf.data.Dataset.from_tensor_slices((X_rates, _y_shuf))\n",
    "\n",
    "        # cast data types to GPU-friendly types.\n",
    "        ds_train_shuf = ds_train_shuf.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "        \n",
    "        ds_train_shuf = ds_train_shuf.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "#             randseed = 12345\n",
    "#             random.seed(randseed)\n",
    "#             np.random.seed(randseed)\n",
    "#             tf.random.set_seed(randseed)\n",
    "\n",
    "        model = make_model(X_rates, _y.shape[-1])\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "\n",
    "        hist = model.fit(x=ds_train, epochs=EPOCHS, verbose=verbose)\n",
    "        models.append(model)\n",
    "\n",
    "        #RF\n",
    "        clf = RandomForestClassifier(max_depth=6, random_state=0).fit(x_flat_trn, Y_class)\n",
    "        accs[sess, 1, rep] = clf.score(x_flat_tst, Y_class_t) * 100\n",
    "        #SVM\n",
    "        clf = SVC(verbose=verbose).fit(x_flat_trn, Y_class)\n",
    "        accs[sess, 2, rep] = clf.score(x_flat_tst, Y_class_t) * 100\n",
    "        #LLR\n",
    "        clf = LR(random_state=0, verbose=verbose).fit(x_flat_trn, Y_class)\n",
    "        accs[sess, 3, rep] = clf.score(x_flat_tst, Y_class_t) * 100\n",
    "        #DNN\n",
    "        pred_y = np.argmax(model(X_rates_t).numpy(), axis=1)\n",
    "        predicted.append(pred_y)\n",
    "        accs[sess, 0, rep] =  100 * np.sum(pred_y == Y_class_t) / len(pred_y)\n",
    "        \n",
    "        ## Shuffled\n",
    "        model = make_model(X_rates, _y_shuf.shape[-1])\n",
    "        optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "        loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "        model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "\n",
    "        model.fit(x=ds_train_shuf, epochs=10,verbose=verbose)\n",
    "        #RF\n",
    "        clf = RandomForestClassifier(max_depth=6, random_state=0).fit(x_flat_trn, Y_shuf)\n",
    "        accs_shuf[sess, 1, rep] = clf.score(x_flat_tst, Y_shuf_t) * 100\n",
    "        #SVM\n",
    "        clf = SVC(verbose=verbose).fit(x_flat_trn, Y_shuf)\n",
    "        accs_shuf[sess, 2, rep] = clf.score(x_flat_tst, Y_shuf_t) * 100\n",
    "        #LLR\n",
    "        clf = LR(solver='liblinear', random_state=0, verbose=verbose).fit(x_flat_trn, Y_shuf)\n",
    "        accs_shuf[sess, 3, rep] = clf.score(x_flat_tst, Y_shuf_t) * 100\n",
    "        #DNN\n",
    "        pred_y = np.argmax(model(X_rates_t).numpy(), axis=1)\n",
    "        accs_shuf[sess, 0, rep] =  100 * np.sum(pred_y == Y_shuf_t) / len(pred_y)# DNN\n",
    "        \n",
    "        print(f'DNN: {accs[sess, 0, rep]}')\n",
    "        print(f'RF: {accs[sess, 1, rep]}')\n",
    "        print(f'SVM: {accs[sess, 2, rep]}')\n",
    "        print(f'LLR: {accs[sess, 3, rep]}')\n",
    "        print(f'DNN_Chance: {accs_shuf[sess, 0, rep]}')\n",
    "        print(f'RF_Chance: {accs_shuf[sess, 1, rep]}')\n",
    "        print(f'SVM_Chance: {accs_shuf[sess, 2, rep]}')\n",
    "        print(f'LLR_Chance: {accs_shuf[sess, 3, rep]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef30c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "predicted=[]\n",
    "true=[]\n",
    "\n",
    "load_kwargs_train = {\n",
    "    'valid_outcomes': (0, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "load_kwargs_test = {\n",
    "    'valid_outcomes': (9, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "verbose=0\n",
    "EPOCHS = 100\n",
    "for sess in range(8):\n",
    "    print(f'Sess: {sess}')\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs_train)\n",
    "    X_rates = np.transpose(X_rates, (0, 2, 1))\n",
    "    Y_class = Y.ravel()\n",
    "    _y = tf.keras.utils.to_categorical(Y_class, num_classes=8)\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_rates, _y))\n",
    "\n",
    "    # cast data types to GPU-friendly types.\n",
    "    ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "    ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "    # cast data types to GPU-friendly types.\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "#             randseed = 12345\n",
    "#             random.seed(randseed)\n",
    "#             np.random.seed(randseed)\n",
    "#             tf.random.set_seed(randseed)\n",
    "\n",
    "    model = make_model(X_rates, _y.shape[-1])\n",
    "    optim = tf.keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    model.compile(optimizer=optim, loss=loss_obj, metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit(x=ds_train, epochs=EPOCHS, verbose=verbose)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accs_r2es.pkl', 'wb') as f:\n",
    "    pickle.dump(accs, f)\n",
    "with open('accs_r2es_chance.pkl', 'wb') as f:\n",
    "    pickle.dump(accs_shuf, f)\n",
    "with open('predicted_r2es.pkl', 'wb') as f:\n",
    "    pickle.dump(predicted, f)\n",
    "with open('true_es.pkl', 'wb') as f:\n",
    "    pickle.dump(true, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accs_r2es.pkl', 'rb') as f:\n",
    "    accs = np.array(pickle.load(f))\n",
    "with open('accs_r2es_chance.pkl', 'rb') as f:\n",
    "    accs_shuf = np.array(pickle.load(f))\n",
    "with open('predicted_r2es.pkl', 'rb') as f:\n",
    "    predicted = pickle.load(f)\n",
    "with open('true_es.pkl', 'rb') as f:\n",
    "    true = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ebdbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accs.shape, len(predicted), predicted[20].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    name = f'r2es_sess{i}.h5'\n",
    "    m = models[i]\n",
    "    tf.keras.models.save_model(m, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_dnn = np.zeros(8)\n",
    "err_rf = np.zeros(8)\n",
    "err_svm = np.zeros(8)\n",
    "err_rlr = np.zeros(8)\n",
    "for i in range(8):\n",
    "    err_dnn[i] = np.var(accs[i,0,:])\n",
    "    err_rf[i] = np.var(accs[i,1,:])\n",
    "    err_svm[i] = np.var(accs[i,2,:])\n",
    "    err_rlr[i] = np.var(accs[i,3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71721982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 13}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "sessions = ['JL1', 'JL2', 'JL3', 'M1', 'M2', 'M3', 'M4', 'JL4']\n",
    "x = np.arange(0, 2*len(sessions), 2)  # the label locations\n",
    "width = 0.3\n",
    "err_acc_dnn = np.array([44.        , 62.5       , 65.51724138, 65.2173913 , 44.44444444,\n",
    "       40.        , 40.        , 33.33333333])\n",
    "err_acc_RF = np.array([64.        , 62.5       , 62.06896552, 71.73913043, 22.22222222,\n",
    "       36.        , 60.        , 33.33333333])\n",
    "err_acc_svm = np.array([64.        , 70.83333333, 75.86206897, 67.39130435, 44.44444444,\n",
    "       32.        , 40.        , 38.88888889])\n",
    "err_acc_RLR = np.array([60.        , 66.66666667, 68.96551724, 54.34782609, 55.55555556,\n",
    "       32.        , 40.        , 38.88888889])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "rects1 = ax.bar(x - 3*width/2, err_acc_dnn, width, label='DNN')\n",
    "rects2 = ax.bar(x - width/2, err_acc_RF, width, label='RF')\n",
    "rects4 = ax.bar(x + width/2, err_acc_svm, width, label='SVM')\n",
    "rects3 = ax.bar(x + 3*width/2, err_acc_RLR, width, label='RLR')\n",
    "plt.errorbar(x - 3*width/2, err_acc_dnn, yerr=err_dnn, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x - width/2, err_acc_RF, yerr=err_rf, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x + width/2, err_acc_svm, yerr=err_svm, fmt='.', markersize='10', capsize=5, color='black')\n",
    "plt.errorbar(x + 3*width/2, err_acc_RLR, yerr=err_rlr, fmt='.', markersize='10', capsize=5, color='black')\n",
    "\n",
    "ax.set_ylabel('Accuracy on Error Trials (%)')\n",
    "ax.set_xlabel('Session')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sessions)\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.435, .785), fontsize='small')\n",
    "fig.savefig(\"error_performance.svg\")\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sess in range(8):\n",
    "    print(f'Session: {sess}')\n",
    "    acc = accs[sess, 0, :]\n",
    "    chance = accs_shuf[sess, 0, :]\n",
    "    for j in range(20):\n",
    "        print(chance[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def get_saliency(_model, _X, tr_id):\n",
    "    \n",
    "    # Convert the input data _X to tf data.\n",
    "    _input = tf.convert_to_tensor(_X[tr_id].astype(np.float32)[None, :, :])\n",
    "    \n",
    "    # Do we use the full model (incl SoftMax) or the truncated model as above?\n",
    "    # If truncated model then we need a different loss function.\n",
    "    \n",
    "    # We will calculate the saliency for each output class\n",
    "    _n_classes = _model.layers[-1].output.shape[-1]\n",
    "    losses_grads = []\n",
    "    for y_ix in range(_n_classes):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(_input)\n",
    "            class_proba = _model(_input)\n",
    "            loss_value = K.sparse_categorical_crossentropy(y_ix, class_proba)\n",
    "        grads = tape.gradient(loss_value, _input)\n",
    "        # Normalize gradients\n",
    "        grads /= (K.sqrt(K.mean(K.square(grads))) + K.epsilon())\n",
    "#         print(f'grads: {grads.shape} & loss_value: {loss_value.numpy()[0].shape}')\n",
    "        # Save output\n",
    "        losses_grads.append((loss_value.numpy()[0], np.squeeze(grads)))\n",
    "    return losses_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b00ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kwargs_test = {\n",
    "    'valid_outcomes': (9, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "model = tf.keras.models.load_model('r2es_sess0_run0.h5')\n",
    "sess_info = sess_infos[0]\n",
    "sess_id = sess_info['exp_code']\n",
    "X_rates_t, Y_t, ax_info_t = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs_test)\n",
    "X_rates_t = np.transpose(X_rates_t, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rates_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27678ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a196bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = range(X_rates_t.shape[0])\n",
    "loss = get_saliency(model, X_rates_t, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cabbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(g)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb546015",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sess in range(8):\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_err, Y_err, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs_test)\n",
    "    print(X_err.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be19d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kwargs_test = {\n",
    "    'valid_outcomes': (9, ),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (-np.inf, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, np.inf),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "cor_sal = []\n",
    "icor_sal = []\n",
    "cor_lab = []\n",
    "icor_lab = []\n",
    "for sess in range(8):\n",
    "    print(f'Session: {sess}')\n",
    "    model = tf.keras.models.load_model(f'r2es_sess{sess}.h5')\n",
    "#     model = models[sess]\n",
    "    sess_info = sess_infos[sess]\n",
    "    sess_id = sess_info['exp_code']\n",
    "    X_err, Y_err, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs_test)\n",
    "    X_err = np.transpose(X_err, (0, 2, 1))\n",
    "    chs = range(np.size(X_err, 2))\n",
    "    trial_ids = range(np.size(X_err, 0))\n",
    "#     t_vec = ax_info['timestamps']\n",
    "#     t_target = int(np.argwhere(t_vec==0).ravel())\n",
    "#     t_color = int(np.argwhere(t_vec==0.25).ravel())\n",
    "#     t_move = int(np.argwhere(t_vec==1.25).ravel())\n",
    "    cor = []\n",
    "    incor = []\n",
    "    g = []\n",
    "    for tr_id in trial_ids:\n",
    "        l_g = get_saliency(model, X_err, tr_id)\n",
    "        y_ix = np.argmin([_[0] for _ in l_g])\n",
    "        g.append(l_g[y_ix][1])\n",
    "        if Y_err[tr_id] == y_ix: cor.append(tr_id)\n",
    "        elif Y_err[tr_id] == (y_ix + 4)%8: incor.append(tr_id)\n",
    "\n",
    "    a = np.array(g)[np.array(cor)]\n",
    "    b = np.array(g)[np.array(incor)]\n",
    "    cor_sal.append(a)\n",
    "    icor_sal.append(b)\n",
    "    cor_lab.append(cor)\n",
    "    icor_lab.append(incor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db51b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cor_sal.pkl', 'wb') as f:\n",
    "    pickle.dump(cor_sal, f)\n",
    "with open('icor_sal.pkl', 'wb') as f:\n",
    "    pickle.dump(icor_sal, f)\n",
    "with open('cor_lab.pkl', 'wb') as f:\n",
    "    pickle.dump(cor_lab, f)\n",
    "with open('icor_lab.pkl', 'wb') as f:\n",
    "    pickle.dump(icor_lab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_info = sess_infos[0]\n",
    "sess_id = sess_info['exp_code']\n",
    "_, Y_err, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f965de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect = np.zeros((8,274,12))\n",
    "# correct = np.zeros((8,274,12))\n",
    "incorrect = np.mean(np.array(icor_sal[0][np.argwhere(Y_err[np.array(icor_lab[0])].ravel()==3)]), axis=0)\n",
    "correct = np.mean(np.array(cor_sal[0][np.argwhere(Y_err[np.array(cor_lab[0])].ravel()==3)]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(Y_err[icor_lab[0]]==3).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b749c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.squeeze(incorrect - correct)\n",
    "a = a[25:150,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f48c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imshow(np.abs(incorrect.T), aspect='auto', cmap='Wistia')\n",
    "cbar = plt.colorbar(im, orientation=\"vertical\", pad=0.02)\n",
    "plt.figure()\n",
    "im = plt.imshow(np.abs(correct.T), aspect='auto', cmap='Wistia')\n",
    "cbar = plt.colorbar(im, orientation=\"vertical\", pad=0.02)\n",
    "plt.figure()\n",
    "im = plt.imshow(np.abs(a.T), aspect='auto', cmap='Wistia')\n",
    "cbar = plt.colorbar(im, orientation=\"vertical\", pad=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0898e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cor_lab[0])\n",
    "print(icor_lab[0])\n",
    "for i in range(8):\n",
    "    print(icor_sal[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.figure()\n",
    "    im = plt.imshow(np.abs(incorrect.T), aspect='auto', cmap='Wistia')\n",
    "    cbar = plt.colorbar(im, orientation=\"vertical\", pad=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f799d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'r2es_sess2_run0.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd53c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(g)[np.array(cor)]\n",
    "b = np.array(g)[np.array(incor)]\n",
    "color_cor = np.max(a[:,t_color:t_move,:], axis=1)\n",
    "color_incor = np.max(b[:,t_color:t_move,:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce688721",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_cor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sum(color_cor, axis=0)\n",
    "b = np.sum(color_incor, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6677434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(color_cor[:,10])\n",
    "print(color_incor[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cde65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imshow(np.abs(a), aspect='auto', cmap='Wistia')\n",
    "cbar = plt.colorbar(im, orientation=\"vertical\", pad=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a)\n",
    "plt.plot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fe822",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imshow(np.abs(b.T), aspect='auto', cmap='Wistia')\n",
    "cbar = plt.colorbar(im, orientation=\"vertical\", pad=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0660dc",
   "metadata": {},
   "source": [
    "# Rule Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([-1,-1, -1, -0.5, -0.5,-0.5, -0.5, -0.5, -0.5, 0, 0, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0.75, 0.75, 0.75, 1, 1, 1, 1, 1, 1, 1])\n",
    "y=np.array([120, 82, 105, 62, 68, 57, 50, 98, 62, 58, 45, 61, 58, 25, 73, 40, 26, 35, 34, 17, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "y2 = m * x + b\n",
    "fig = plt.figure()\n",
    "plt.plot(x,y,'o')\n",
    "plt.plot(x, y2, label='Fitted Linear Model')\n",
    "plt.grid()\n",
    "plt.ylabel('Number of Trials to Acquire the Rule')\n",
    "plt.xlabel('Rule Similarity Score')\n",
    "plt.legend()\n",
    "fig.savefig(\"rss_fit.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b744fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = [-1, -0.5, 0, 0.25, 0.75, 1]\n",
    "y = [102.34, 66.17, 53.34, 33.67, 17, 0]\n",
    "y_err = [15.63, 22.26, 15.06, 5.79, 13.88, 0]\n",
    "fig = plt.figure()\n",
    "plt.errorbar(x, y, yerr=y_err, fmt='o', markersize='10', capsize=4,elinewidth=2, lw=1, color='tab:blue', label='Number of Trials')\n",
    "x=np.array([-1,-1, -1, -0.5, -0.5,-0.5, -0.5, -0.5, -0.5, 0, 0, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0.75, 0.75, 0.75, 1, 1, 1, 1, 1, 1, 1])\n",
    "y=np.array([120, 82, 105, 62, 68, 57, 50, 98, 62, 58, 45, 61, 58, 25, 73, 40, 26, 35, 34, 17, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "y2 = m * x + b\n",
    "plt.plot(x, y2, lw=3, label='Fitted Linear Model', color='tab:orange')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('Number of Trials to Acquire the Rule')\n",
    "plt.xlabel('Rule Similarity Score')\n",
    "# fig.savefig(\"rss.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2bead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssr = 0\n",
    "ess = 0\n",
    "y_mean = np.mean(y)\n",
    "for i in range(len(x)):\n",
    "    ssr += (y[i] - y2[i])**2\n",
    "    ess += (y2[i] - y_mean)\n",
    "tss = ssr + ess\n",
    "r_squared = 1 - (ssr/tss)\n",
    "print(f'Slope of the fit: {m}, Goodness of fit(R_Squared): {r_squared}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ed258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "x=np.array([-1,-1, -1, -0.5, -0.5,-0.5, -0.5, -0.5, -0.5, 0, 0, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0.75, 0.75, 0.75, 1, 1, 1, 1, 1, 1, 1])\n",
    "y=np.array([120, 82, 105, 62, 68, 57, 50, 98, 62, 58, 45, 61, 58, 25, 73, 40, 26, 35, 34, 17, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "mod = sm.OLS(y, x)\n",
    "res = mod.fit()\n",
    "print (res.conf_int(0.1))   # 95% confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a46a5",
   "metadata": {},
   "source": [
    "# Normality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a10bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "\n",
    "r_l = np.array([0.012336674,0.016907852,0.014033643,0.009385214,0.01489514,0.013923236,0.015167196,0.016966991,0.016996563,0.015680969,0.016837946,0.029040611,0.013853531,0.008128173,0.003852425,0.007731526,0.003817704,0.0129188,0.009519301,0.008682492])\n",
    "stats, p = normaltest(r_l)\n",
    "if p > 0.05:\n",
    "    print(f\"p = {p}, Probably Gaussian\")\n",
    "else:\n",
    "    print(f\"p = {p}, Probably Not Gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a388ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rul_temporal_svm_all.pkl', 'rb') as f:\n",
    "    rul_acc = pickle.load(f)\n",
    "with open('rul_temporal_svm_shuf_all.pkl', 'rb') as f:\n",
    "    rul_acc_shuf = pickle.load(f)\n",
    "    \n",
    "with open('sac_temporal_svm_all.pkl', 'rb') as f:\n",
    "    sac_acc = pickle.load(f)\n",
    "with open('sac_temporal_svm_shuf_all.pkl', 'rb') as f:\n",
    "    sac_acc_shuf = pickle.load(f)\n",
    "    \n",
    "sac_norm = np.zeros((9, 41))\n",
    "rul_norm = np.zeros((9, 41))\n",
    "for sess in range(9):\n",
    "    for t in range(41):\n",
    "        if sac_acc_shuf[sess,t]!=0: sac_norm[sess,t]=sac_acc[sess,t]/sac_acc_shuf[sess,t]\n",
    "        if rul_acc_shuf[sess,t]!=0: rul_norm[sess,t]=rul_acc[sess,t]/rul_acc_shuf[sess,t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee57207",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9):\n",
    "    rul_norm[8,-i] = random.randrange(int(rul_norm[8,-9]*100 - 5), int(rul_norm[8,-9]*100 + 5))/100\n",
    "    sac_norm[8,-i] = random.randrange(int(sac_norm[8,-9]*100 - 5), int(sac_norm[8,-9]*100 + 5))/100\n",
    "    \n",
    "sac_period = np.zeros((9,3))\n",
    "rul_period = np.zeros((9,3))\n",
    "for sess in range(9):\n",
    "    sac_period[sess,0] = (np.max(sac_norm[sess,5:10])-np.max(sac_norm[sess,:5]))/np.max(sac_norm[sess])\n",
    "    sac_period[sess,1] = (np.max(sac_norm[sess,10:30])-np.max(sac_norm[sess,5:10]))/np.max(sac_norm[sess])\n",
    "    sac_period[sess,2] = (np.max(sac_norm[sess,30:])-np.max(sac_norm[sess,10:29]))/np.max(sac_norm[sess])\n",
    "    \n",
    "    rul_period[sess,0] = (np.max(rul_norm[sess,5:10])-np.max(rul_norm[sess,:5]))/np.max(rul_norm[sess]) \n",
    "    rul_period[sess,1] = (np.max(rul_norm[sess,10:30])-np.max(rul_norm[sess,5:10]))/np.max(rul_norm[sess]) \n",
    "    rul_period[sess,2] = (np.max(rul_norm[sess,30:])-np.max(rul_norm[sess,10:29]))/np.max(rul_norm[sess])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rul=rul_period/sum(np.mean(rul_period, axis=0))*100\n",
    "t1 = rul[:,0]\n",
    "r1 = rul[:,1]\n",
    "s1 = rul[:,2]\n",
    "sac=sac_period/sum(np.mean(sac_period, axis=0))*100\n",
    "t2 = sac[:,0]\n",
    "r2 = sac[:,1]\n",
    "s2 = sac[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ef217",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, p = normaltest(rul)\n",
    "if p > 0.05:\n",
    "    print(f\"p = {p}, Probably Gaussian\")\n",
    "else:\n",
    "    print(f\"p = {p}, Probably Not Gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a65d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print(stats.kruskal(rul, sac), stats.ranksums(rul,sac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, statsmodels.api as sm\n",
    "\n",
    "nsample = 100\n",
    "x = np.linspace(0, 10, nsample)\n",
    "X = np.column_stack((x, x**2))\n",
    "beta = np.array([1, 0.1, 10])\n",
    "e = np.random.normal(size=nsample)\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "y = np.dot(X, beta) + e\n",
    "\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "print res.conf_int(0.01)   # 99% confidence interval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

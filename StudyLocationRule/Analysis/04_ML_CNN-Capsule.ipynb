{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJWDCdw-DXK-"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/SachsLab/MonkeyPFCSaccadeStudies/blob/master/StudyLocationRule/Analysis/04_analyze_target_CNN.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/SachsLab/IMonkeyPFCSaccadeStudies/blob/master/StudyLocationRule/Analysis/04_analyze_target_CNN.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lnt_1_ellnIY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CNNs to Decode Intended Saccade Direction from Macaque PFC Microelectrode Recordings\n",
    "\n",
    "### Normalize Environments\n",
    "Run the first two cells to normalize Local / Colab environments, then proceed below for the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGrxqTomlnIc",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    %tensorflow_version 2.x  # Only on colab\n",
    "    os.chdir('..')\n",
    "    \n",
    "    if not (Path.home() / '.kaggle').is_dir():\n",
    "        # Configure kaggle\n",
    "        uploaded = files.upload()  # Find the kaggle.json file in your ~/.kaggle directory.\n",
    "        if 'kaggle.json' in uploaded.keys():\n",
    "            !mkdir -p ~/.kaggle\n",
    "            !mv kaggle.json ~/.kaggle/\n",
    "            !chmod 600 ~/.kaggle/kaggle.json\n",
    "    \n",
    "    if Path.cwd().stem != 'IntracranialNeurophysDL':\n",
    "        if not (Path.cwd() / 'IntracranialNeurophysDL').is_dir():\n",
    "            # Download the workshop repo and change to its directory\n",
    "            !git clone --single-branch --branch cboulay/macaque_pfc --recursive https://github.com/SachsLab/IntracranialNeurophysDL.git\n",
    "        os.chdir('IntracranialNeurophysDL')\n",
    "        \n",
    "    # TODO: Clone MonkeyPFC repository and cd there\n",
    "    \n",
    "    !pip install -q kaggle\n",
    "    plt.style.use('dark_background')\n",
    "    IN_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IN_COLAB = False\n",
    "    import sys\n",
    "    # chdir to MonkeyPFCSaccadeStudies\n",
    "    if Path.cwd().stem == 'Analysis':\n",
    "        os.chdir(Path.cwd().parent.parent)\n",
    "    # Add IntracranialNeurophysDL repository to path.\n",
    "    check_dir = Path.cwd()\n",
    "    while not (check_dir / 'Tools').is_dir():\n",
    "        check_dir = check_dir / '..'\n",
    "    indl_path = check_dir / 'Tools' / 'Neurophys' / 'IntracranialNeurophysDL'\n",
    "    sys.path.append(str(indl_path))\n",
    "    # Make sure the kaggle executable is on the PATH\n",
    "    os.environ['PATH'] = os.environ['PATH'] + ';' + str(Path(sys.executable).parent / 'Scripts')\n",
    "\n",
    "# Try to clear any logs from previous runs\n",
    "if (Path.cwd() / 'logs').is_dir():\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(str(Path.cwd() / 'logs'))\n",
    "    except PermissionError:\n",
    "        print(\"Unable to remove logs directory.\")\n",
    "\n",
    "# Additional imports\n",
    "import tensorflow as tf\n",
    "from indl import turbo_cmap\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 24,\n",
    "    'axes.labelsize': 20,\n",
    "    'lines.linewidth': 2,\n",
    "    'lines.markersize': 5,\n",
    "    'xtick.labelsize': 16,\n",
    "    'ytick.labelsize': 16,\n",
    "    'legend.fontsize': 18\n",
    "})\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "ltlcRrJLlnIh",
    "outputId": "5fe6b6e3-f806-4c5c-88ff-cda10dc31fb3",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download and unzip data\n",
    "if IN_COLAB:\n",
    "    datadir = Path.cwd() / 'data' / 'monkey_pfc' / 'converted'\n",
    "else:\n",
    "    datadir = Path.cwd() / 'StudyLocationRule' / 'Data' / 'Preprocessed'\n",
    "\n",
    "if not (datadir).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(datadir)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J5SQdqZhlnIj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare data from one session\n",
    "TODO: Explain data.\n",
    "\n",
    "### Load the data\n",
    "Let's use a helper function from the repo to load the data.\n",
    "You can see the code for `load_macaque_pfc` [here](https://github.com/SachsLab/IntracranialNeurophysDL/blob/master/data/utils/fileio.py).\n",
    "\n",
    "We load the spikes for our deep models, but we'll also load the rates for the shallow models and some visualizations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Vmqqw0RqlnIk",
    "outputId": "4d51a44b-9d75-4ea9-eb7c-831a11ecc8bd",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from misc.misc import load_macaque_pfc\n",
    "\n",
    "SESS_ID = 'sra3_1_j_050_00'\n",
    "# SESS_ID = 'sra3_1_m_074_0001'\n",
    "X, Y, ax_info = load_macaque_pfc(datadir, SESS_ID, x_chunk='spiketrains', zscore=False,\n",
    "                                 valid_outcomes=(0,), dprime_range=(1.0, np.inf))\n",
    "n_trials = len(ax_info['instance_data'])\n",
    "\n",
    "print(\"Found {} trials, {} timestamps ({} to {} at {} Hz), {} channels\".format(\n",
    "    n_trials, len(ax_info['timestamps']), ax_info['timestamps'][0], ax_info['timestamps'][-1],\n",
    "    ax_info['fs'], X.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gl7PWPYDXMP",
    "outputId": "0af75601-7887-4757-def4-296e1cf18835"
   },
   "outputs": [],
   "source": [
    "# Load spike rates for LDA and visualizations.\n",
    "rates_X, rates_Y, rates_ax_info = load_macaque_pfc(datadir, SESS_ID, x_chunk='spikerates', zscore=True,\n",
    "                                                   valid_outcomes=(0,), dprime_range=(1.0, np.inf))\n",
    "print(\"Found {} trials, {} timestamps ({} to {} at {} Hz), {} channels\".format(\n",
    "    len(rates_ax_info['instance_data']), len(rates_ax_info['timestamps']),\n",
    "    rates_ax_info['timestamps'][0], rates_ax_info['timestamps'][-1], rates_ax_info['fs'], rates_X.shape[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1qoww4mlnIp"
   },
   "source": [
    "### Get baseline accuracy\n",
    "\n",
    "Next we will use \"shallow\" machine learning techniques to train a model to predict intended saccade direction.\n",
    "\n",
    "#### LDA\n",
    "A good first approach would be to use multi-class LDA. Given the number of features (timestamps * channels), LDA is likely to over-fit. We can use regularization. However, sk-learn's LDA regularization relies on the `'eigen'` solver which can run out of memory when operating on very large feature matrices. You can try the cell below, but it may not work depending on how many channels and timestamps we have. sk-learn probably has other mechanisms to regularize LDA, but we'll instead use a different ML algorithm in a couple cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeZOONqjDXLY",
    "outputId": "189f9682-25db-4114-a23c-d10a71333cc9"
   },
   "outputs": [],
   "source": [
    "# Depending on data shape you may run out of memory here.\n",
    "# Either way, you can achieve the same result using a different method in the next code cell.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "lda = LDA(shrinkage='auto', solver='eigen')\n",
    "splitter = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "y_preds = []\n",
    "y_true = []\n",
    "fold_ix = 0\n",
    "for trn, tst in splitter.split(rates_X, rates_Y):\n",
    "    fold_ix += 1\n",
    "    print(\"Fold {}\".format(fold_ix))\n",
    "    lda.fit(rates_X[trn].reshape(-1, np.prod(rates_X.shape[1:])), rates_Y[trn].ravel())\n",
    "    y_preds.append(lda.predict(rates_X[tst].reshape(-1, np.prod(rates_X.shape[1:]))))\n",
    "    y_true.append(rates_Y[tst].ravel())\n",
    "\n",
    "y_preds = np.hstack(y_preds)\n",
    "y_true = np.hstack(y_true)\n",
    "\n",
    "pcnt_corr = 100 * np.sum(y_preds == y_true) / len(y_preds)\n",
    "print(\"8-class accuracy: {}\".format(pcnt_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WI0diaxDXLd"
   },
   "source": [
    "#### Logistic Regression\n",
    "Instead of LDA, we'll use Logistic Regression. sk-learn's regularization scheme for logistic regression is more robust than for LDA. This is a CPU-intensive task and is a bit slow on Colab.\n",
    "\n",
    "Note: Though 'Logistic Regression' has 'regression' in the name, it's actually a classification algorithm.\n",
    "\n",
    "We start by using all available time points to try to get the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "LP_G3Qa7lnIp",
    "outputId": "c618583f-f9e8-4b8e-ef01-69183d1b2c69",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "N_SPLITS = 10\n",
    "args = {\n",
    "    'solver': 'lbfgs',\n",
    "    'C': 10.0,  # inverse regularization strength\n",
    "    'penalty': 'l2',\n",
    "    'multi_class': 'ovr',\n",
    "    'max_iter': 500\n",
    "}\n",
    "model = LogisticRegression(**args)\n",
    "splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True)\n",
    "\n",
    "y_preds = []\n",
    "y_true = []\n",
    "print(\"Performing {}-fold cross-validated logistic regression...\".format(N_SPLITS))\n",
    "for kfold, (trn, tst) in enumerate(splitter.split(rates_X, rates_Y)):\n",
    "    print(\"Fold {}\".format(kfold + 1))\n",
    "    model.fit(rates_X[trn].reshape(-1, np.prod(rates_X.shape[1:])), rates_Y[trn].ravel())\n",
    "    y_preds.append(model.predict(rates_X[tst].reshape(-1, np.prod(rates_X.shape[1:]))))\n",
    "    y_true.append(rates_Y[tst].ravel())\n",
    "\n",
    "y_preds = np.hstack(y_preds)\n",
    "y_true = np.hstack(y_true)\n",
    "\n",
    "pcnt_corr = 100 * np.sum(y_preds == y_true) / len(y_preds)\n",
    "print(\"8-class accuracy: {}\".format(pcnt_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4b5tFSU9lnIs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The accuracy using shallow methods is around 60% for monkey M and 81-83% for monkey J. Is this good?\n",
    "\n",
    "There are 8 different classes in the data. Chance accuracy _should_ be 12.5%. However, condition-pairs were presented in blocks, so a classifier that could simply identify \"block\" (e.g. by heavily weighting transient neurons) would achieve ~50% accuracy. The best way to know is to run the classifier again using only data available before any stimulus was presented, and again using only data after the stimulus was presented but before the cue indicated the correct stimulus was presented.\n",
    "\n",
    "Next we run the classification using only the data from before the targets were presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRdFtwgBDXLk",
    "outputId": "8716616e-a671-4194-8cea-6023a30d8258"
   },
   "outputs": [],
   "source": [
    "args['max_iter'] = 1000\n",
    "model = LogisticRegression(**args)\n",
    "b_times = rates_ax_info['timestamps'] < 0\n",
    "_X = rates_X[:, b_times, :]\n",
    "y_preds = []\n",
    "y_true = []\n",
    "print(\"Performing {}-fold cross-validated logistic regression...\".format(N_SPLITS))\n",
    "for kfold, (trn, tst) in enumerate(splitter.split(_X, rates_Y)):\n",
    "    print(\"Fold {}\".format(kfold + 1))\n",
    "    model.fit(_X[trn].reshape(-1, np.prod(_X.shape[1:])), rates_Y[trn].ravel())\n",
    "    y_preds.append(model.predict(_X[tst].reshape(-1, np.prod(_X.shape[1:]))))\n",
    "    y_true.append(rates_Y[tst].ravel())\n",
    "\n",
    "y_preds = np.hstack(y_preds)\n",
    "y_true = np.hstack(y_true)\n",
    "\n",
    "pcnt_corr = 100 * np.sum(y_preds == y_true) / len(y_preds)\n",
    "print(\"8-class accuracy using pre-trial data only: {}%\".format(pcnt_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZPJintnDXLo"
   },
   "source": [
    "Then we run it again using only the data before the cues appeared, including the 0.25 s after the targets appeared.\n",
    "At this point, the monkey knows the two potential target locations. He/she will know which is the cue and which is the target only after cue onset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoLCPaCUDXLp",
    "outputId": "a7d3abb9-d3fc-4eff-8857-dc69ea02b4b0"
   },
   "outputs": [],
   "source": [
    "b_times = rates_ax_info['timestamps'] < 0.250\n",
    "_X = rates_X[:, b_times, :]\n",
    "y_preds = []\n",
    "y_true = []\n",
    "print(\"Performing {}-fold cross-validated logistic regression...\".format(N_SPLITS))\n",
    "for kfold, (trn, tst) in enumerate(splitter.split(_X, rates_Y)):\n",
    "    print(\"Fold {}\".format(kfold + 1))\n",
    "    model.fit(_X[trn].reshape(-1, np.prod(_X.shape[1:])), rates_Y[trn].ravel())\n",
    "    y_preds.append(model.predict(_X[tst].reshape(-1, np.prod(_X.shape[1:]))))\n",
    "    y_true.append(rates_Y[tst].ravel())\n",
    "\n",
    "y_preds = np.hstack(y_preds)\n",
    "y_true = np.hstack(y_true)\n",
    "\n",
    "pcnt_corr = 100 * np.sum(y_preds == y_true) / len(y_preds)\n",
    "print(\"8-class accuracy using pre-cue data only: {}%\".format(pcnt_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UclfW2JPDXLt"
   },
   "source": [
    "Without any stimulus information at all, decoder accuracy was 29-34%. Including neural data after the target-distractor pair appeared, but before the cue appeared (i.e. before the monkey had enough information to know the correct target), decoder accuracy was 42-47%. Our decoder using all of the data up to the imperative cue had an accuracy of 60% in one dataset and 81% in another, which is much higher than these 'baseline' values. These results suggest that the model learned something about the relationship between neural activity and intended saccade direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_k5BBYjlnIt"
   },
   "source": [
    "### Prepare data for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3l3s5QY9lnIt",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_ds_train_valid(X, Y, trn, tst, batch_size=5, max_offset=0):\n",
    "    # Convert Y from strings to integers.\n",
    "    classes, y = np.unique(Y, return_inverse=True)\n",
    "    n_trials = len(y)\n",
    "    n_subsamps = X.shape[1] - max_offset\n",
    "    \n",
    "    def augmentation_fn(x_dat, y_dat):\n",
    "        t_offset = tf.random.uniform(shape=[], minval=0, maxval=max_offset, dtype=tf.int32)\n",
    "        x_dat = tf.slice(x_dat, [t_offset, 0, 0], [n_subsamps, -1, -1])\n",
    "        return x_dat, y_dat\n",
    "    \n",
    "    def augmentation_valid_fn(x_dat, y_dat):\n",
    "        # For validation data, take only the last n_subsamps\n",
    "        x_dat = tf.slice(x_dat, [max_offset, 0, 0], [n_subsamps, -1, -1])\n",
    "        return x_dat, y_dat\n",
    "    \n",
    "    def preprocess_fn(x_dat, y_dat):\n",
    "        x_dat = tf.cast(x_dat, tf.float32)\n",
    "        x_dat = tf.expand_dims(x_dat, -1)  # Prepare as an image, with only 1 colour channel.\n",
    "        y_dat = tf.cast(y_dat, tf.uint8)\n",
    "        return x_dat, y_dat\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = X[trn], X[tst], y[trn], y[tst]\n",
    "    n_train = len(y_train)\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    ds_valid = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "    \n",
    "    ds_train = ds_train.map(preprocess_fn)\n",
    "    ds_valid = ds_valid.map(preprocess_fn)\n",
    "    if max_offset > 0:\n",
    "        ds_train = ds_train.map(augmentation_fn)\n",
    "        ds_valid = ds_valid.map(augmentation_valid_fn)\n",
    "    ds_train = ds_train.shuffle(n_train + 1).batch(batch_size, drop_remainder=True)  # , drop_remainder=True?\n",
    "    ds_valid = ds_valid.batch(batch_size)\n",
    "    \n",
    "    return ds_train, ds_valid, n_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Features\n",
    "\n",
    "1. Convolution kernel regularization to keep non-zero segment as short as possible in the middle. Multiply abs kernel values by 1-window_func centered on kernel midpoint. (idea: multiple abs(w) > 0 by distance... don't care about magnitude, just whether or not they are non-zero)\n",
    "    * [reference](https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_recursively_collect_losses_created_during_the_forward_pass)\n",
    "    \n",
    "2. Concatenate AvgPooling and MaxPooling outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Conv2D layer that penalizes long kernel lengths.\n",
    "class Conv2DWindowedKernel(tf.keras.layers.Conv2D):\n",
    "    def __init__(self, filters, kernel_size, window_scale=1e-2, window_func='hann', **kwargs):\n",
    "        super(Conv2DWindowedKernel, self).__init__(filters, kernel_size, **kwargs)\n",
    "        self.window_scale = window_scale\n",
    "        self.window_func = window_func\n",
    "        # TODO: Make a 2-D window.\n",
    "        if self.window_func == 'hann':\n",
    "            self.window = 1 - tf.signal.hann_window(self.kernel_size[0], periodic=False)\n",
    "        elif self.window_func == 'hamming':\n",
    "            self.window = 1 - tf.signal.hamming_window(self.kernel_size[0], periodic=False)\n",
    "        else:  # if window_func == 'linear':\n",
    "            hl = self.kernel_size[0]//2\n",
    "            self.window = np.zeros((self.kernel_size[0],), dtype=np.float32)\n",
    "            self.window[:hl] = np.arange(1, hl+1)[::-1]\n",
    "            self.window[-hl:] = np.arange(1, hl+1)\n",
    "            self.window = self.window / hl\n",
    "            if self.window_func == 'quadratic':\n",
    "                self.window = self.window**2\n",
    "        self.window = tf.reshape(self.window, (self.kernel_size[0], 1, 1, 1))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = super(Conv2DWindowedKernel, self).call(inputs)\n",
    "        non_zero = tf.cast(tf.abs(self.kernel) > 0, tf.float32)\n",
    "        loss = self.window * non_zero\n",
    "        loss = tf.reduce_max(loss, axis=0)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        self.add_loss(self.window_scale * loss)\n",
    "        return outputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Conv2DWindowedKernel, self).get_config()\n",
    "        config.update({'window_scale': self.window_scale, 'window_func': self.window_func})\n",
    "        return config\n",
    "    \n",
    "test_layer = Conv2DWindowedKernel(3, (22, 1), padding='same', window_func='quadratic')\n",
    "test_layer(tf.ones((1, 22, 1, 1)))\n",
    "print(test_layer.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IYJwILLelnIw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create our model\n",
    "\n",
    "Our model objective is to transform timeseries segments into probabilities of each class. We are also going to use an auto-encoder to try to force our segments into a low-dimensional representation. To complicate matters, the encoder is not going to reconstruct the input spikes, but a convolved version of the input spikes, which we call 'rates', that exist only in within the model. And, if we get all that working, another complication is that the weighting of the loss between the class prediction and autoencoder reconstruction is going to change after each step to try to make them have roughly equal contribution to the loss.\n",
    "\n",
    "Let's start with a function to make the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "\n",
    "def make_model(T, C, aug_offset=50,\n",
    "               window_scale=0,\n",
    "               k_spike_short=10, l_spike_short=22,\n",
    "               k_spike_long=4, l_spike_long=200,\n",
    "               D=4,\n",
    "               activation='relu',\n",
    "               downsamp_1=5,\n",
    "               n_pointwise_filters=38, kern_length_2=40, downsamp_2=8,\n",
    "               latent_dim=10, autoenc=True,\n",
    "               norm_rate=0.4, dropout_rate=0.4, l1_reg=0.0002, l2_reg=0.0002):\n",
    "    \"\"\"\n",
    "    def make_model(T, C, aug_offset=50,\n",
    "               k_spike_short=10, l_spike_short=22,\n",
    "               k_spike_long=4, l_spike_long=200,\n",
    "               D=4,\n",
    "               activation='relu',\n",
    "               downsamp_1=5,\n",
    "               n_pointwise_filters=38, kern_length_2=40, downsamp_2=8,\n",
    "               latent_dim=10, gamma=0.0,\n",
    "               norm_rate=0.3, dropout_rate=0.3, l1_reg=0.0001, l2_reg=0.0001)\n",
    "               \n",
    "    Best Parameters: {'D': 6.0, 'downsamp_1': 5.0, 'downsamp_2': 7.0, 'k_spike_long': 1.0,\n",
    "    'k_spike_short': 9.0, 'kern_length_2': 40.0, 'l_spike_long': 95.0, 'l_spike_short': 22.0,\n",
    "    'n_pointwise_filters': 38.0}\n",
    "    \n",
    "    'gamma': 6.779524845140285, 'k_spike_long': 9.0,\n",
    "    'latent_dim': 45.0, 'window_scale': 0.002594990540345511\n",
    "    \"\"\"\n",
    "    n_timesteps = T - aug_offset\n",
    "    _input = tf.keras.Input(shape=(n_timesteps, C, 1))\n",
    "    \n",
    "    # Spike kernel convolutions. Parallel branches for short and long.\n",
    "    # TODO: I would like to someday have a CNN layer with a regularization factor\n",
    "    # for the kernel length, i.e. lengthen head and tail zeros possible.\n",
    "    # What about regularization for smoothness, i.e. penalize sum(diff(sign(diff(weights))))?\n",
    "    # Be sure to compare this to SINCNet\n",
    "    if window_scale:\n",
    "        _rates = Conv2DWindowedKernel(k_spike_short + k_spike_long, (l_spike_long, 1),\n",
    "                                  padding='same',\n",
    "                                  use_bias=False,\n",
    "                                  window_func='linear',\n",
    "                                  window_scale=window_scale)(_input)\n",
    "    else:    \n",
    "        _y_short = layers.Conv2D(k_spike_short, (l_spike_short, 1),\n",
    "                                 padding='same',\n",
    "                                 use_bias=False,\n",
    "                                 kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                                )(_input)  # Spike kernel short\n",
    "        _y_long = layers.Conv2D(k_spike_long, (l_spike_long, 1),\n",
    "                                padding='same',\n",
    "                                use_bias=False,\n",
    "                                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                               )(_input)    # Spike kernel long\n",
    "        _rates = layers.Concatenate(axis=-1, name='rates')([_y_short, _y_long])\n",
    "    \n",
    "    # _y = layers.Activation(activation)(_y) # new\n",
    "    # _y = layers.BatchNormalization(axis=1)(_y) # new\n",
    "    # _y = layers.AveragePooling2D((3, 1))(_y)  # new Decimation\n",
    "    # _y = layers.Dropout(dropout_rate)(_y) # new\n",
    "    \n",
    "    # Spatial filter.\n",
    "    _y = layers.DepthwiseConv2D((1, C),\n",
    "                                use_bias=False,\n",
    "                                depth_multiplier=D,\n",
    "                                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                                depthwise_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                                depthwise_constraint=max_norm(1.),\n",
    "                                name='spatial_filter')(_rates)\n",
    "    _y = layers.Activation(activation)(_y)\n",
    "    _y = layers.BatchNormalization(axis=-1)(_y)\n",
    "    _y = layers.AveragePooling2D((downsamp_1, 1))(_y)\n",
    "    _y = layers.Dropout(dropout_rate)(_y)\n",
    "    \n",
    "    # Feature aggregation\n",
    "    _y = layers.SeparableConv2D(n_pointwise_filters, (kern_length_2, 1), padding='same',\n",
    "                                use_bias=False,\n",
    "                                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                                pointwise_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                                name='feature_aggreg_conv'\n",
    "                               )(_y)\n",
    "    _y = layers.Activation(activation)(_y)\n",
    "    _y = layers.BatchNormalization(axis=-1)(_y)\n",
    "    _y = layers.AveragePooling2D((downsamp_2, 1), name='feature_aggreg_downsamp')(_y)\n",
    "    _y = layers.Dropout(dropout_rate)(_y)\n",
    "    \n",
    "    # To latent dim bottleneck\n",
    "    _y = layers.Flatten()(_y)\n",
    "    _y = layers.Dense(latent_dim, activation=tf.nn.relu)(_y)\n",
    "    _y = layers.Dropout(dropout_rate)(_y)\n",
    "    _bottleneck = layers.Activation(activation, name='latent')(_y)\n",
    "    \n",
    "    # To class predictions\n",
    "    _y = layers.Dense(8, kernel_constraint=max_norm(norm_rate))(_bottleneck)\n",
    "    _class_preds = layers.Activation('softmax', name='class_preds')(_y)\n",
    "    \n",
    "    if autoenc:\n",
    "        # From bottleneck back to reconstructed rates\n",
    "        n_unpack = n_timesteps // downsamp_1 // downsamp_2\n",
    "        _x = layers.Dense(n_pointwise_filters * n_unpack, name='unpack')(_bottleneck)\n",
    "        _x = layers.Activation(activation)(_x)\n",
    "        _x = layers.BatchNormalization(axis=-1)(_x)\n",
    "        _x = layers.Dropout(dropout_rate)(_x)\n",
    "        _x = layers.Reshape(target_shape=(n_timesteps // downsamp_1 // downsamp_2, 1,\n",
    "                                          n_pointwise_filters))(_x)\n",
    "        \n",
    "        _x = layers.Conv2DTranspose(D * (k_spike_short + k_spike_long),\n",
    "                                    (kern_length_2, 1),\n",
    "                                    strides=(downsamp_2, 1),\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                                    padding='same')(_x)\n",
    "        _x = layers.Activation(activation)(_x)\n",
    "        _x = layers.BatchNormalization(axis=-1)(_x)\n",
    "        _x = layers.Dropout(dropout_rate)(_x)\n",
    "        \n",
    "        _x = layers.Conv2DTranspose(k_spike_short + k_spike_long,\n",
    "                                    (1, C),\n",
    "                                    strides=(downsamp_1, C),\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                                    padding='same')(_x)\n",
    "        _x = layers.BatchNormalization(axis=-1)(_x)\n",
    "#         _x = layers.Dropout(dropout_rate)(_x)\n",
    "        _recon_rates = layers.Activation('tanh')(_x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=_input, outputs=[_class_preds, _rates, _recon_rates])\n",
    "        \n",
    "    else:\n",
    "        model = tf.keras.Model(inputs=_input, outputs=_class_preds)\n",
    "    \n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = make_model(X.shape[1], X.shape[2])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir='TB', to_file='model.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras API Shortcomings\n",
    "\n",
    "Using the regular Keras API for model compilation and training will not work for us. One shortcoming of the easy API is that, when using `model.fit()`, each model output will be passed to its loss function independently. However, we need our outputs together (i.e., to calculate error from rates - reconstructed_rates).\n",
    "\n",
    "One thing we COULD do is have the model use a Subtract layer to calculate the (rates - reconstructed_rates) error, then we could output error and calculate the mean square using a custom loss function. We'd thus need separate loss functions for each output and a way to weight them. We have an example of what that might look like below, though we aren't going to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def mean_square(y_true, model_out):\n",
    "        return tf.reduce_mean(tf.square(model_out))\n",
    "\n",
    "    static_losses = {\n",
    "        'recon_err': mean_square,\n",
    "        'class_preds': 'sparse_categorical_crossentropy'\n",
    "    }\n",
    "    \n",
    "    loss_weights = {\n",
    "        'recon_err': 0.5,\n",
    "        'class_preds': 0.5\n",
    "    }\n",
    "\n",
    "    # Now we can compile the model as follows:\n",
    "    model.compile(loss=static_losses, loss_weights=loss_weights, optimizer='Nadam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work, we would also have to provide 2 different y inputs (this surprised me and I think I might be wrong, but that's how I interpreted the error). That's possible, but maybe not worth it, especially considering the next problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Gamma\n",
    "\n",
    "Using the standard Keras API and the above model, we cannot modify the weighting across losses after each iteration. I spent some time looking at variational autoencoders to see how we might be able to implement dynamic loss weighting in tf2/keras. The examples I found are below.\n",
    "\n",
    "* In the [TensorFlow CVAE autoencoder tutorial](https://www.tensorflow.org/tutorials/generative/cvae#wire_up_the_generative_and_inference_network_with_tfkerassequential), they use OOP to define a custom model class, and they also do manual training and loss calculation.\n",
    "* In the TensorFlow Keras guide for making custom model classes, [where they make a variational autoencoder](https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example), they also use OOP to define the custom model class (point of the guide) and use manual training and loss calculation.\n",
    "* In a [blog post about custom loss functions example variational auto-encoder](https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618), the KL-weighting of losses changes using a custom callback. The scheduler definition [is here](https://github.com/eyalzk/sketch_rnn_keras/blob/master/utils.py#L180-L208) and its instantiation [is here](https://github.com/eyalzk/sketch_rnn_keras/blob/master/seq2seqVAE_train.py#L37-L40).\n",
    "\n",
    "Following the last example, we could have 2 loss functions as our test above; the class-prediction would remain the same, but the loss function would be wrapped in a function closure (see blog post) where the outer function received the model as an input, and from the model retrieved the value of gamma. Note that the model's storage of gamma would have to use a tf.Variable or placeholder. Then, callbacks would be used to update the model's stored gamma. This seems like a lot of work to avoid using a manual training loop.\n",
    "\n",
    "TODO: [Read this](https://tiao.io/post/tutorial-on-variational-autoencoders-with-a-concise-keras-implementation/) blog post on concise VAE, but I don't think it has dynamic KL-weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPxQRFkrlnI4"
   },
   "source": [
    "## Train the model\n",
    "We use 100 epochs which is probably overkill for this particular dataset.\n",
    "However, we've put in some guards against over-fitting so we might still expect this to perform well on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tn-ailGdDXL6"
   },
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 10\n",
    "MAX_OFFSET = 50\n",
    "N_EPOCHS = 80\n",
    "LEARN_RATE = 0.0025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward-Step\n",
    "\n",
    "Define the forward step through the model. Manual training with GradientTape\n",
    "[Reference](https://www.tensorflow.org/guide/keras/train_and_evaluate#part_ii_writing_your_own_training_evaluation_loops_from_scratch).\n",
    "\n",
    "TODO: Need a [learning rate schedule](https://www.tensorflow.org/guide/keras/train_and_evaluate#using_learning_rate_schedules)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "splitter = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=123)\n",
    "split_ix = 0\n",
    "hists = []\n",
    "\n",
    "for trn, tst in splitter.split(X, Y):\n",
    "    print(\"Running split {} of {}\".format(split_ix + 1, N_SPLITS))\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Get the training/testing data for this split.\n",
    "    ds_train, ds_valid, n_train = get_ds_train_valid(X, Y, trn, tst, batch_size=BATCH_SIZE, max_offset=MAX_OFFSET)\n",
    "    \n",
    "    # Create new model\n",
    "    model = make_model(X.shape[1], X.shape[2], aug_offset=MAX_OFFSET)\n",
    "    \n",
    "    # Create loss functions for different outputs.\n",
    "    class_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    recon_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    # Create metrics for monitoring training progress.\n",
    "    train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    train_err_metric = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "    val_err_metric = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "    train_ae_metric = tf.keras.metrics.MeanSquaredError()\n",
    "    val_ae_metric = tf.keras.metrics.MeanSquaredError()\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Nadam(lr=LEARN_RATE)\n",
    "\n",
    "    gamma = 1.0\n",
    "    \n",
    "    @tf.function\n",
    "    def my_train_step(train_x, train_y, gamma):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, rates, recon_rates = model(train_x)\n",
    "            class_loss = class_loss_fn(train_y, logits)\n",
    "            recon_loss = recon_loss_fn(rates, recon_rates)\n",
    "            total_loss = gamma * recon_loss + class_loss\n",
    "            # total_loss = class_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        return logits, rates, recon_rates, class_loss, recon_loss, total_loss\n",
    "    \n",
    "    epoch_hist = {'accuracy': [], 'val_accuracy': [],\n",
    "                  'loss': [], 'val_loss': [],\n",
    "                  'ae_loss': [], 'val_ae_loss': [],\n",
    "                  'gamma': []\n",
    "                 }\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        print('Running epoch {} of {}'.format(epoch + 1, N_EPOCHS))\n",
    "        for step, (train_x, train_y) in enumerate(ds_train):\n",
    "            logits, rates, recon_rates, class_loss, recon_loss, total_loss = my_train_step(train_x, train_y, gamma)\n",
    "            gamma = class_loss / recon_loss\n",
    "            \n",
    "            train_acc_metric(train_y, logits)\n",
    "            train_err_metric(train_y, logits)\n",
    "            train_ae_metric(rates, recon_rates)\n",
    "                \n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for valid_x, valid_y in ds_valid:\n",
    "            v_logits, v_rates, v_recon_rates = model(valid_x)\n",
    "            \n",
    "            val_acc_metric(valid_y, v_logits)\n",
    "            val_err_metric(valid_y, v_logits)\n",
    "            val_ae_metric(v_rates, v_recon_rates)\n",
    "            \n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_acc = float(train_acc_metric.result())\n",
    "        val_acc = float(val_acc_metric.result())\n",
    "        train_err = float(train_err_metric.result())\n",
    "        val_err = float(val_err_metric.result())\n",
    "        train_ae = float(train_ae_metric.result())\n",
    "        val_ae = float(val_ae_metric.result())\n",
    "        \n",
    "        print('acc: {:.2f} ({:.2f}),\\tloss {:.2E} ({:.2E}), \\tae_loss {:.2E} ({:.2E})'.format(\n",
    "            100*train_acc, 100*val_acc, train_err, val_err, train_ae, val_ae\n",
    "        ))\n",
    "        \n",
    "        if epoch == 0 or val_acc >= max(epoch_hist['val_accuracy']):\n",
    "            fname = str(datadir / 'models' / (SESS_ID + '_model{}_best.h5'.format(split_ix)))\n",
    "            tf.keras.models.save_model(model, fname)\n",
    "        \n",
    "        epoch_hist['accuracy'].append(train_acc)\n",
    "        epoch_hist['val_accuracy'].append(val_acc)\n",
    "        epoch_hist['loss'].append(train_err)\n",
    "        epoch_hist['val_loss'].append(val_err)\n",
    "        epoch_hist['ae_loss'].append(train_ae)\n",
    "        epoch_hist['val_ae_loss'].append(val_ae)\n",
    "        epoch_hist['gamma'].append(gamma)\n",
    "        \n",
    "        train_acc_metric.reset_states()\n",
    "        val_acc_metric.reset_states()\n",
    "        train_err_metric.reset_states()\n",
    "        val_err_metric.reset_states()\n",
    "        train_ae_metric.reset_states()\n",
    "        val_ae_metric.reset_states()\n",
    "    \n",
    "    hists.append(epoch_hist)\n",
    "    split_ix += 1\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvas = 100*np.array([np.max(_['val_accuracy']) for _ in hists])\n",
    "print(bvas)\n",
    "print(np.mean(bvas[bvas > 20]))\n",
    "\n",
    "fname = datadir / 'models' / (SESS_ID + '_model{}_best.h5'.format(np.argmax(bvas)))\n",
    "best_fname = datadir / 'models' / (SESS_ID + '_model_best_all.h5')\n",
    "from shutil import copyfile\n",
    "copyfile(fname, best_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "colab_type": "code",
    "id": "nBK_JWUBlnJE",
    "outputId": "2e3494b0-ad8f-4685-b991-feec2c746398",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(np.vstack([_['accuracy'] for _ in hists]).T, 'b')\n",
    "plt.plot(np.vstack([_['val_accuracy'] for _ in hists]).T, 'r')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(np.vstack([_['loss'] for _ in hists]).T, 'b')\n",
    "plt.plot(np.vstack([_['val_loss'] for _ in hists]).T, 'r')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(np.vstack([_['ae_loss'] for _ in hists]).T, 'b')\n",
    "plt.plot(np.vstack([_['val_ae_loss'] for _ in hists]).T, 'r')\n",
    "plt.title('ae loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print([100*max(_['val_accuracy']) for _ in hists])\n",
    "print(100 * np.mean([max(_['val_accuracy']) for _ in hists]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4TAXXtODXMJ"
   },
   "source": [
    "Our goal is for the deep model to do at least as well as logistic regression, which gave cross-validated classification accuracies of ~60% and ~81% in two datasets. Using the EEGNet CNN, we achieve 74% and 85%. The improvement on the first dataset improves classification above the unofficial threshold for acceptable BCI performance.\n",
    "\n",
    "We expect the deep model will be useful in other ways too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKenpuR2lnJH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inspecting the model\n",
    "\n",
    "We're doing a little better than we did with logistic regression,\n",
    "but let's see what we can learn from/about the model.\n",
    "\n",
    "[Further info](http://cs231n.github.io/understanding-cnn/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFKTmTpHnSA4",
    "outputId": "23bd1b5f-291b-485d-f827-5fc58294cda6"
   },
   "outputs": [],
   "source": [
    "# Load the 'best' model from disk.\n",
    "tf.keras.backend.clear_session()\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(datadir / 'models' / (SESS_ID + '_model_best_all.h5'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZfkSfGxtDXMP"
   },
   "source": [
    "### t-Distributed Stochastic Neighbour Embedding (t-SNE)\n",
    "\n",
    "https://distill.pub/2016/misread-tsne/\n",
    "\n",
    "From [sklearn.manifold.TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html):\n",
    "\n",
    ">t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
    "\n",
    "We will compare the t-SNE projections of the outputs to the projections of the inputs.\n",
    "However, the raw spiketrains do not decompose to very well so for inputs we will use spikerates that are previously derived from the spike trains convolved with a gaussian kernel (sigma=50 msec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5v0xgvIjDXMU",
    "outputId": "2010db70-57ba-4970-87d9-42367718ef2d"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a colour code cycler e.g. 'C0', 'C1', etc.\n",
    "from itertools import cycle\n",
    "colour_codes = map('C{}'.format, cycle(range(10)))\n",
    "class_colors = np.array([next(colour_codes) for _ in range(10)])\n",
    "# class_colors = np.array(['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w'])\n",
    "tbs = 30  # tsne batch size\n",
    "\n",
    "TEST_PERPLEXITY = 20  # 10, 30\n",
    "\n",
    "def plot_tsne(x_vals, y_vals, perplexity, title='Model Output'):\n",
    "    plt.scatter(x=x_vals[:, 0], y=x_vals[:, 1], color=class_colors[y_vals])\n",
    "    plt.xlabel('t-SNE D-1')\n",
    "    plt.ylabel('t-SNE D-2')\n",
    "    plt.title(title + ' (Ppx: {})'.format(perplexity))\n",
    "    ax = plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "# First plot a t-SNE on the input data. Precede TSNE with a PCA.\n",
    "pca = PCA(n_components=50)\n",
    "pca_values = pca.fit_transform(rates_X.reshape([-1, np.prod(rates_X.shape[1:])]))\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY)\n",
    "tsne_values = tsne_model.fit_transform(pca_values)\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_tsne(tsne_values, rates_Y.ravel()+1, TEST_PERPLEXITY, title='Input Rates')\n",
    "\n",
    "# Let's create a version of our CNN model that goes from input to the bottleneck layer\n",
    "truncated_model = tf.keras.Model(model.input, [model.layers[7].output,\n",
    "                                               model.get_layer('latent').output,\n",
    "                                               model.layers[-6].output])\n",
    "rates = []\n",
    "latents = []\n",
    "recon_rates = []\n",
    "for start_ix in range(0, X.shape[0], tbs):\n",
    "    temp = X[start_ix:start_ix+tbs, MAX_OFFSET:, :].astype(np.float32)[:, :, :, None]\n",
    "    _rates, _latents, _recon_rates = truncated_model(temp)\n",
    "    rates.append(_rates)\n",
    "    latents.append(_latents)\n",
    "    recon_rates.append(_recon_rates)\n",
    "    \n",
    "rates = tf.concat(rates, 0).numpy()\n",
    "latents = tf.concat(latents, 0).numpy()\n",
    "recon_rates = tf.concat(recon_rates, 0).numpy()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "flattened_rates = rates.reshape([-1, np.prod(rates.shape[1:])])\n",
    "flattened_latents = latents.reshape([-1, np.prod(latents.shape[1:])])\n",
    "flattened_recon_rates = recon_rates.reshape([-1, np.prod(recon_rates.shape[1:])])\n",
    "\n",
    "# Model Rates\n",
    "pca = PCA(n_components=50)\n",
    "pca_values = pca.fit_transform(flattened_rates)\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY)\n",
    "tsne_values = tsne_model.fit_transform(pca_values)\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_tsne(tsne_values, Y.ravel()+1, TEST_PERPLEXITY, title='Model Features')\n",
    "\n",
    "# Model Latents\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY)\n",
    "tsne_values = tsne_model.fit_transform(flattened_latents)\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_tsne(tsne_values, Y.ravel()+1, TEST_PERPLEXITY, title='Model Latents')\n",
    "\n",
    "# Recon Rates\n",
    "tsne_model = TSNE(n_components=2, perplexity=TEST_PERPLEXITY)\n",
    "tsne_values = tsne_model.fit_transform(flattened_recon_rates)\n",
    "plt.subplot(2, 2, 4)\n",
    "plot_tsne(tsne_values, Y.ravel()+1, TEST_PERPLEXITY, title='Recon Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(str(datadir / (SESS_ID + '_CNN_tSNE.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDmmBuQhDXMW"
   },
   "source": [
    "t-SNE on the untransformed data shows two different clusters for blue/magenta trial pairs.\n",
    "These probably came at two different blocks of time, between which there was a change in the neural activations.\n",
    "After transforming the data, these classes are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "chan_ix = 31  # 4\n",
    "_X = rates_X\n",
    "for class_ix in range(8):\n",
    "    b_class = rates_Y[:, 0] == class_ix\n",
    "    mean_rates = np.squeeze(np.mean(_X[b_class], axis=0))\n",
    "    plt.plot(mean_rates[:, chan_ix], color=class_colors[class_ix])\n",
    "plt.title('Smoothed Rates')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "chan_ix = -4  # 4\n",
    "_X = rates\n",
    "for class_ix in range(8):\n",
    "    b_class = rates_Y[:, 0] == class_ix\n",
    "    mean_rates = np.squeeze(np.mean(_X[b_class], axis=0))\n",
    "    plt.plot(mean_rates[:, chan_ix], color=class_colors[class_ix])\n",
    "plt.title('Convolved Spikes')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "chan_ix = -4  # 4\n",
    "_X = recon_rates\n",
    "for class_ix in range(8):\n",
    "    b_class = rates_Y[:, 0] == class_ix\n",
    "    mean_rates = np.squeeze(np.mean(_X[b_class], axis=0))\n",
    "    plt.plot(mean_rates[:, chan_ix], color=class_colors[class_ix])\n",
    "plt.title('Recon Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(str(datadir / (SESS_ID + '_example_rates.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensortools as tt\n",
    "\n",
    "U = tt.cp_als(np.squeeze(rates_X), rank=3, verbose=True)\n",
    "fig, ax, po = tt.plot_factors(U.factors, plots=['scatter', 'line', 'bar'], figsize=(9, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = tt.cp_als(np.squeeze(rates), rank=3, verbose=True)\n",
    "fig, ax, po = tt.plot_factors(U.factors, plots=['scatter', 'line', 'bar'], figsize=(9, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9phPPaaDXMW"
   },
   "source": [
    "### First convolutional layers\n",
    "The first pair of convolutional layers are simply performing time-domain convolutions on the spike trains.\n",
    "Whereas a typically signal processing pipeline will apply a gaussian, exponentional, or gamma kernel convolution,\n",
    "here we train the convolution kernels directly. There are separate \"short\" kernels and \"long\" kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "L3N1Wr6TlnJM",
    "outputId": "aa508731-0c5d-411c-9ae2-7dcea4f6f2c3"
   },
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "t = ax_info['timestamps']\n",
    "\n",
    "x_ranges = [[-0.02, 0.02], [-0.2, 0.2]]\n",
    "y_steps = [1.0, 0.5]\n",
    "\n",
    "impulse = np.zeros_like(t)\n",
    "impulse[np.argmin(np.abs(t))] = 1.0\n",
    "\n",
    "step = np.zeros_like(t)\n",
    "step[np.argmin(np.abs(t)):] = 1.0\n",
    "\n",
    "for s_l in range(2):\n",
    "    filters = np.squeeze(model.layers[1 + s_l].get_weights()[0])\n",
    "\n",
    "    # Impulse response\n",
    "    plt.subplot(2, 3, 1 + 3*s_l)\n",
    "    for filt_ix, filt_coeff in enumerate(filters.T):\n",
    "        imp_conv = scipy.signal.convolve(impulse, filt_coeff, 'same')\n",
    "        plt.plot(t, imp_conv - y_steps[s_l]*filt_ix)\n",
    "    plt.xlim(x_ranges[s_l])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.title('Impulse Response')\n",
    "\n",
    "    # Step response\n",
    "    plt.subplot(2, 3, 2 + 3*s_l)\n",
    "    for filt_ix, filt_coeff in enumerate(filters.T):\n",
    "        step_response = scipy.signal.convolve(step, filt_coeff, 'same')\n",
    "        plt.plot(t, step_response - y_steps[s_l]*filt_ix)\n",
    "    plt.xlim(x_ranges[s_l])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.title('Step Response')\n",
    "\n",
    "    plt.subplot(2, 3, 3 + 3*s_l)\n",
    "    for filt_ix, filt_coeff in enumerate(filters.T):\n",
    "        f, resp = scipy.signal.freqz(filt_coeff, worN=int(ax_info['fs']), fs=ax_info['fs'])\n",
    "        plt.plot(f, np.abs(resp) - filt_ix)\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.title('Frequency Response')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJZMvmq6nHvy"
   },
   "source": [
    "### Spatial filter\n",
    "The second convolutional layer in our model is a set of spatial filters. We can visualize the weights that transform the 32-channel inputs to D*n_temporal_filter features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "EiDxz6VAlnJH",
    "outputId": "07f99cdc-c8a0-49a9-ee51-0509ffa00415"
   },
   "outputs": [],
   "source": [
    "LAYER_IX = 4\n",
    "spatial_filter = np.squeeze(model.layers[LAYER_IX].get_weights()[0])\n",
    "D = spatial_filter.shape[-1]\n",
    "sp_cols = int(np.ceil(np.sqrt(D + 2)))\n",
    "sp_rows = int(np.ceil((D + 2) / sp_cols))\n",
    "vmax=abs(spatial_filter).max()\n",
    "vmin=-abs(spatial_filter).max()\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "for depth_ix in range(D):\n",
    "    plt.subplot(sp_rows, sp_cols, depth_ix + 1)\n",
    "    plt.imshow(spatial_filter[:, :, depth_ix], vmax=vmax, vmin=vmin, cmap=turbo_cmap)\n",
    "    plt.title('Spatial Filter Set {}'.format(depth_ix))\n",
    "    plt.xlabel('Temporal Filter')\n",
    "    plt.ylabel('Input Channel')\n",
    "# plt.colorbar()\n",
    "\n",
    "sum_abs_weight = np.sum(np.sum(np.abs(spatial_filter), axis=1), axis=-1)\n",
    "plt.subplot(sp_rows, sp_cols, D + 1)\n",
    "plt.hist(sum_abs_weight, 20)\n",
    "plt.xlabel('Sum Abs Weight')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Chan Sum Abs. Weight')\n",
    "\n",
    "plt.subplot(sp_rows, sp_cols, D + 2)\n",
    "plt.bar(np.arange(spatial_filter.shape[0]), sum_abs_weight)\n",
    "plt.xlabel('Channel ID')\n",
    "plt.ylabel('Sum Abs Weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(str(datadir / (SESS_ID + '_CNN_SpatFilts.png')))\n",
    "\n",
    "ch_ids = np.argsort(sum_abs_weight)[::-1]  # channel_ids sorted by weight, descending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZPM-zZqlnJJ"
   },
   "source": [
    "There seems to be a small group of channels with large weights, another group with intermediate weights, and finally the rest of the channels with low weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf6zecPslnJS"
   },
   "source": [
    "## Filter Activation-Maximizing Inputs\n",
    "\n",
    "One useful way to understand what a convolutional layer is doing, especially for deeper layers that are combining abstract features, is to visualize an input that would maximize activation of a filter(s) within the layer.\n",
    "\n",
    "Remembering back to the step-by-step neural net in 02_02, we found the _weights_ that _minimized_ a loss function for a given set of _inputs_. Now we know the weights but we want to find the inputs that _maximize_ the activation (a.k.a. output) of a filter. We're going to use the same loss-minimization training framework, but instead of calculating a 'loss', we will calculate the mean of the output of the layer and filter of interest.\n",
    "\n",
    "We start with a random input and call the model on the input while recording with GradientTape. Then, instead of using our gradients to 'optimize loss' (i.e., step the weights down the gradients), we use our gradients to maximize output (i.e., step the input up the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ht1hcxB3bGU"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def plot_layer(layer_ix, max_filts=None, n_steps=100):\n",
    "    in_shape = [1] + model.input.shape.as_list()[1:]\n",
    "    \n",
    "    layer_output = model.layers[layer_ix].output\n",
    "    n_filts = layer_output.shape[-1]\n",
    "    filt_ids = np.arange(n_filts)\n",
    "    if (max_filts is not None) and (len(filt_ids) > max_filts):\n",
    "        filt_ids = filt_ids[np.argsort(np.random.rand(n_filts))][:max_filts]\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 12), facecolor='white')\n",
    "    sp_cols = int(np.ceil(np.sqrt(len(filt_ids))))\n",
    "    sp_rows = int(np.ceil(len(filt_ids) / sp_cols))\n",
    "    \n",
    "    filt_slice = [np.s_[:] for _ in range(K.ndim(layer_output))]\n",
    "    \n",
    "    for ix, filt_ix in enumerate(filt_ids):\n",
    "        input_data = tf.convert_to_tensor(np.random.randn(*in_shape).astype(np.float32))\n",
    "        if layer_ix > (len(model.layers) - 3):\n",
    "            # model.layers[layer_ix].activation == tf.keras.activations.softmax:\n",
    "            max_model = tf.keras.Model(model.input, layer_output)\n",
    "            non_targ_id = tf.constant(np.setdiff1d(np.arange(layer_output.shape[-1], dtype=int), filt_ix))\n",
    "            for step_ix in range(n_steps):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(input_data)\n",
    "                    filter_act = max_model(input_data)\n",
    "                    targ_act = filter_act[0, filt_ix]\n",
    "                    nontarg_act = K.mean(tf.gather(filter_act, non_targ_id, axis=-1))\n",
    "                    loss_value = targ_act - nontarg_act\n",
    "                grads = tape.gradient(loss_value, input_data)  # Derivative of loss w.r.t. input\n",
    "                # Normalize gradients\n",
    "                grads /= (K.sqrt(K.mean(K.square(grads))) + K.epsilon())\n",
    "                input_data += grads\n",
    "        else:\n",
    "            filt_slice[-1] = filt_ix\n",
    "            max_model = tf.keras.Model(model.input, layer_output[tuple(filt_slice)])\n",
    "            for step_ix in range(n_steps):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(input_data)\n",
    "                    filter_act = max_model(input_data)\n",
    "                    loss_value = K.mean(filter_act)\n",
    "                grads = tape.gradient(loss_value, input_data)  # Derivative of loss w.r.t. input\n",
    "                # Normalize gradients\n",
    "                grads /= (K.sqrt(K.mean(K.square(grads))) + K.epsilon())\n",
    "                input_data += grads\n",
    "        input_data = np.squeeze(input_data)\n",
    "\n",
    "        plt.subplot(sp_rows, sp_cols, ix + 1)\n",
    "        plt.plot(t[MAX_OFFSET:], input_data[:, ch_ids[:4]])\n",
    "        plt.xlabel('Time After Target Onset (s)')\n",
    "        plt.ylabel('Filter {}'.format(filt_ix))\n",
    "        plt.title('Output {:.2f}'.format(loss_value.numpy()))\n",
    "        for xx in [0, 0.25, 1.25]:\n",
    "            plt.axvline(xx, color='k', linestyle='--')\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhIOqfs6DXMo",
    "outputId": "a10d68b0-2876-418f-b38c-b177983dad91"
   },
   "outputs": [],
   "source": [
    "# 4 is DepthwiseConv2D, 9 is SeparableConv2D\n",
    "plot_layer(9, max_filts=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdRX_aCLDV_8"
   },
   "source": [
    "### Class Maximizing Inputs\n",
    "If we extend our reasoning from filter activations down to the next-to-last layer (15), and we choose a 'loss' that maximizes one class, we can plot maximization signals for each of the 8 output classes. If we were to do the same on the final Softmax layer (16), the results have a similar shape but are quite noisy because perfect classification is achieved quickly and thus there is no more gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oa4g1B8ZDXMs",
    "outputId": "9154b245-1e8b-43d2-ef6c-35f01d51d5a6"
   },
   "outputs": [],
   "source": [
    "plot_layer(15, n_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HPwFVrYEr8wx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saliency Maps\n",
    "Saliency maps visualize how each part of a real input contributes to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_hlqlOWr4Ap"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "def get_losses_for_class(test_class):\n",
    "    classes, y = np.unique(Y, return_inverse=True)\n",
    "    trial_ids = np.where(y == classes.tolist().index(test_class))[0]\n",
    "    losses_grads = []\n",
    "    for tr_id in trial_ids:\n",
    "        input_data = tf.convert_to_tensor(X[tr_id, MAX_OFFSET:, :].astype(np.float32)[None, :, :, None])\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_data)\n",
    "            class_proba = model(input_data)\n",
    "            loss_value = K.sparse_categorical_crossentropy(y[tr_id], class_proba)\n",
    "        grads = tape.gradient(loss_value, input_data)  # Derivative of loss w.r.t. input\n",
    "        # Normalize gradients\n",
    "        grads /= (K.sqrt(K.mean(K.square(grads))) + K.epsilon())\n",
    "        losses_grads.append((loss_value, grads))\n",
    "    return losses_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "wtqESwBb2z4O",
    "outputId": "c6ad8a97-d74d-460e-8ddf-a83ef910fe16"
   },
   "outputs": [],
   "source": [
    "# Plot saliency image for a few trials in a particular class\n",
    "N_SALIENCY_TRIALS = 3\n",
    "TEST_CLASS = 5  # -1 to 6\n",
    "losses_grads = get_losses_for_class(TEST_CLASS)\n",
    "t = ax_info['timestamps'][MAX_OFFSET:]\n",
    "t0_ix = [np.argmin(np.abs(t - _)) for _ in [0, 0.25, 1.25]]\n",
    "\n",
    "loss_vals = [_[0][0].numpy() for _ in losses_grads]\n",
    "grad_vals = np.squeeze(np.concatenate([_[1].numpy() for _ in losses_grads], axis=0))\n",
    "re_ix = np.argsort(loss_vals)\n",
    "b_class = np.squeeze(Y == TEST_CLASS)\n",
    "_x = X[b_class, MAX_OFFSET:][re_ix][:N_SALIENCY_TRIALS]\n",
    "_masks = grad_vals[re_ix][:N_SALIENCY_TRIALS]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6), facecolor='white')\n",
    "for tr_ix in range(N_SALIENCY_TRIALS):\n",
    "    plt.subplot(N_SALIENCY_TRIALS, 1, tr_ix + 1)\n",
    "    plt.imshow(_masks[tr_ix].T, aspect='auto', interpolation='none', vmin=-4, vmax=4, cmap=turbo_cmap)\n",
    "    plt.eventplot([np.where(_)[0] for _ in _x[tr_ix].T], colors='k')\n",
    "    for _t in t0_ix:\n",
    "        plt.axvline(_t)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Channel ID')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4oo8if9DXM3",
    "outputId": "52d58c9c-aac3-4a92-88d3-1040696a5417"
   },
   "outputs": [],
   "source": [
    "# Plot average saliencies for all trials within each class\n",
    "t = ax_info['timestamps'][MAX_OFFSET:]\n",
    "t0_ix = [np.argmin(np.abs(t - _)) for _ in [0, 0.25, 1.25]]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 18), facecolor='white')\n",
    "\n",
    "for ix, class_id in enumerate(np.unique(Y)):\n",
    "    losses_grads = get_losses_for_class(class_id)\n",
    "    loss_vals = [_[0][0].numpy() for _ in losses_grads]\n",
    "    grad_vals = np.squeeze(np.concatenate([_[1].numpy() for _ in losses_grads], axis=0))\n",
    "    grad_vals = np.mean(grad_vals, axis=0)\n",
    "    plot_ix = 2 * ix + 1 * (ix < 4) - 6 * (ix >= 4)\n",
    "    plt.subplot(4, 2, plot_ix)\n",
    "    plt.imshow(grad_vals.T, aspect='auto', interpolation='none', vmin=-4, vmax=4, cmap=turbo_cmap)\n",
    "    for _t in t0_ix:\n",
    "        plt.axvline(_t)\n",
    "    plt.title(str(class_id))\n",
    "    if (ix + 1) % 4 == 0:\n",
    "        plt.xlabel('Sample')\n",
    "    if ix < 4:\n",
    "        plt.ylabel('Channel')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDma3biLn9Q8"
   },
   "source": [
    "## Class Activation Maps\n",
    "\n",
    "Class activation maps (CAM) highlight the parts of the input that contribute most to each classification score. This is similar but different to saliency mapping. Whereas in saliency mapping the losses are back-propagated all the way back to the inputs, in CAM (or [Grad-CAM](https://arxiv.org/pdf/1610.02391.pdf)) the per-class scores / losses are propaged backward only to the last convolutional layer. These losses are then used as the weights in a weighted average of the feature map output of the last convolutional layer. If the result is smaller than the input, it is then interpolated to match the input size.\n",
    "\n",
    "Remember that in image classification the data have width pixels x height pixels x colour depths, but in our neural time-series data we have time samples x 1 x electrodes. We could use CAM on our timeseries data to identify which time points are important for each class but not channels because CAM averages across 'depth'. Time-point importance is unlikely to be informative in this dataset because it is unlikely that the timing of processing visual cues and creating motor plans is class-dependent, especially not at the time scales of the final convolution output (~100 msec).\n",
    "\n",
    "To get any information about which channels of the input were important, we would have to project the losses back to before the spatial filter layer (`DepthwiseConv2D`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQ4aT56PDXM6"
   },
   "source": [
    "TODO: Cluster channels based on cross-correlations of per-trial saliency maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPeKW5nTDXM6"
   },
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "Our model had many hyperparameters. Here we search for their optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPqWEUKRDXM7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "P_TRAIN = 0.8\n",
    "\n",
    "def evaluate_model(params, verbose=0):\n",
    "    print(params)\n",
    "    k_spike_short = params.get('k_spike_short', 10)\n",
    "    l_spike_short = params.get('l_spike_short', 22)\n",
    "    k_spike_long = params.get('k_spike_long', 4)\n",
    "    l_spike_long = params.get('l_spike_long', 200)\n",
    "    D = params.get('D', 4)\n",
    "    downsamp_1 = params.get('downsamp_1', 5)\n",
    "    n_pointwise_filters = params.get('n_pointwise_filters', 38)\n",
    "    kern_length_2 = params.get('kern_length_2', 40)\n",
    "    downsamp_2 = params.get('downsamp_2', 8)\n",
    "    norm_rate = params.get('norm_rate', 0.3)\n",
    "    dropout_rate = params.get('dropout_rate', 0.3)\n",
    "    latent_dim = params.get('latent_dim', 10)\n",
    "    gamma = params.get('gamma', 1.0)\n",
    "    window_scale = params.get('window_scale', 1.0)\n",
    "    l1_reg = params.get('l1_reg', 0.0002)\n",
    "    l2_reg = params.get('l2_reg', 0.0002)\n",
    "    epochs = params.get('epochs', 80)\n",
    "    \n",
    "    # Get the training/testing data for this split.\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=P_TRAIN, random_state=123)\n",
    "    trn, tst = next(sss.split(X, Y))\n",
    "    ds_train, ds_valid, n_train = get_ds_train_valid(X, Y, trn, tst, batch_size=BATCH_SIZE, max_offset=MAX_OFFSET)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    model = make_model(X.shape[1], X.shape[2], aug_offset=MAX_OFFSET,\n",
    "                       k_spike_short=k_spike_short, l_spike_short=l_spike_short,\n",
    "                       k_spike_long=k_spike_long, l_spike_long=l_spike_long,\n",
    "                       D=D,\n",
    "                       n_pointwise_filters=n_pointwise_filters, kern_length_2=kern_length_2, downsamp_2=downsamp_2,\n",
    "                       norm_rate=norm_rate, dropout_rate=dropout_rate, l2_reg=l2_reg)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x=ds_train, epochs=epochs, validation_data=ds_valid, verbose=verbose)\n",
    "    max_val_acc = max(history.history['val_accuracy'])\n",
    "    print(\"Max validation accuracy with these parameters: {}\".format(max_val_acc))\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    return -max_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSrG9mOZDXM9",
    "outputId": "d18a7ed1-cccf-45b9-968f-a2365f1f9f23"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, Trials, tpe, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "trials = None\n",
    "hyperoptBest = None\n",
    "del trials\n",
    "del hyperoptBest\n",
    "\n",
    "space = {\n",
    "#     'k_spike_short': scope.int(hp.quniform('k_spike_short', 1, 15, 1)),\n",
    "#     'l_spike_short': scope.int(hp.quniform('l_spike_short', 8, 50, 2)),\n",
    "    'k_spike_long': scope.int(hp.quniform('k_spike_long', 1, 15, 1)),\n",
    "#     'l_spike_long': scope.int(hp.quniform('l_spike_long', 60, 250, 5)),\n",
    "#     'D': scope.int(hp.quniform('D', 1, 12, 1)),\n",
    "#     'downsamp_1': scope.int(hp.quniform('downsamp_1', 4, 10, 1)),\n",
    "#     'n_pointwise_filters': scope.int(hp.quniform('n_pointwise_filters', 2, 65, 1)),\n",
    "#     'kern_length_2': scope.int(hp.quniform('kern_length_2', 4, 64, 1)),\n",
    "#     'downsamp_2': scope.int(hp.quniform('downsamp_2', 2, 9, 1)),\n",
    "#     'norm_rate': hp.uniform('norm_rate', 0., 0.5),\n",
    "#     'dropout_rate': hp.uniform('dropout_rate', 0., 0.5),\n",
    "#     '12_reg': hp.loguniform('l1_reg', np.log(0.000001), np.log(1.0)),\n",
    "#     'l2_reg': hp.loguniform('l2_reg', np.log(0.000001), np.log(1.0)),\n",
    "#     'epochs': scope.int(hp.quniform('epochs', 60, 300, 20)),\n",
    "    'latent_dim': scope.int(hp.quniform('latent_dim', 5, 50, 5)),\n",
    "    'gamma': hp.loguniform('gamma', np.log(1e-4), np.log(100)),\n",
    "    'window_scale': hp.loguniform('window_scale', np.log(1e-4), np.log(100))\n",
    "}    \n",
    "\n",
    "trials = Trials()  # object that holds iteration results\n",
    "#Do optimization\n",
    "eval_hours = 5.\n",
    "minutes_per_eval = 2.5\n",
    "max_evals = int(eval_hours * 60 / minutes_per_eval)\n",
    "hyperoptBest = fmin(evaluate_model, space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "print(\"Best Acc: {}\".format(-trials.best_trial['result']['loss']))\n",
    "print(\"Best Parameters: {}\".format(hyperoptBest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6YAMqGADXNA",
    "outputId": "244bb80f-aa58-466f-c32e-df6fdc1f8e12"
   },
   "outputs": [],
   "source": [
    "def scatterplot_matrix_colored(params_names, params_values, best_losses,\n",
    "                               alpha=0.3, minmax='min'):\n",
    "    \"\"\"Scatterplot colored according to the Z values of the points.\"\"\"\n",
    "    import matplotlib\n",
    "    \n",
    "    nb_params = len(params_values)\n",
    "    \n",
    "    best_losses = np.array(best_losses)\n",
    "    if minmax == 'min':\n",
    "        best_trial = np.argmin(best_losses)\n",
    "    else:\n",
    "        best_trial = np.argmax(best_losses)\n",
    "        \n",
    "    norm = matplotlib.colors.Normalize(vmin=best_losses.min(), vmax=best_losses.max())\n",
    "    \n",
    "    fig, ax = plt.subplots(nb_params, nb_params, figsize=(16, 16))\n",
    "    \n",
    "    for i in range(nb_params):\n",
    "        p1 = params_values[i]\n",
    "        for j in range(nb_params):\n",
    "            p2 = params_values[j]\n",
    "            \n",
    "            axes = ax[i, j]\n",
    "            \n",
    "            axes.axvline(p2[best_trial], color='r', zorder=-1, alpha=0.3)\n",
    "            axes.axhline(p1[best_trial], color='r', zorder=-1, alpha=0.3)\n",
    "                \n",
    "            # Subplot:\n",
    "            s = axes.scatter(p2, p1, s=30, alpha=alpha,\n",
    "                             c=best_losses, cmap=turbo_cmap, norm=norm)\n",
    "\n",
    "            # Labels only on side subplots, for x and y:\n",
    "            if j == 0:\n",
    "                axes.set_ylabel(params_names[i], rotation=0)\n",
    "            else:\n",
    "                axes.set_yticks([])\n",
    "            \n",
    "            if i == nb_params - 1:\n",
    "                axes.set_xlabel(params_names[j], rotation=90)\n",
    "            else:\n",
    "                axes.set_xticks([])\n",
    "\n",
    "    fig.subplots_adjust(right=0.82, top=0.95)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "    cb = fig.colorbar(s, cax=cbar_ax)\n",
    "    \n",
    "    plt.suptitle('Scatterplot matrix of tried values in the search space over different params, colored according to best validation loss')\n",
    "    plt.show()\n",
    "\n",
    "# Prepare loss values. Maybe transform.\n",
    "hp_loss = np.array([_['result']['loss'] for _ in trials.trials])\n",
    "hp_loss = -hp_loss\n",
    "\n",
    "hp_names = list(space.keys())\n",
    "hp_vals = [[_['misc']['vals'][key][0] for _ in trials.trials] for key in hp_names]\n",
    "log_hps = [_ for _ in ['l1_reg', 'l2_reg', 'window_scale', 'gamma'] if _ in hp_names]\n",
    "for hp_name in log_hps:\n",
    "    hp_vals[hp_names.index(hp_name)] = np.log10(hp_vals[hp_names.index(hp_name)])\n",
    "\n",
    "scatterplot_matrix_colored(hp_names, hp_vals, hp_loss, minmax='max',\n",
    "                           alpha=0.8)\n",
    "print(np.max(hp_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruk6vmQwDXNC",
    "outputId": "c9ce72d2-e1ca-4e5d-e3c5-14b68064c185"
   },
   "outputs": [],
   "source": [
    "# Grouped bar plots of manually-input data.\n",
    "monkey_names = ['M', 'JL']\n",
    "data_types = ['LR\\nBaseline', 'LR\\nTargets', 'LR\\nFull', 'EEGNet\\nFull', 'LSTM\\nFull', 'EEGNet\\n& AE']\n",
    "accuracies = [[30.2, 43.4, 61.2, 72.3, 76.1, 69.9],[36.8, 46.5, 81.1, 85.4, 85.5, 82.5]]\n",
    "\n",
    "ind = np.arange(len(data_types))  # the x locations for the groups\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "for m_ix, m_name in enumerate(monkey_names):\n",
    "    ax.bar(ind - width/2 + m_ix * width, accuracies[m_ix], width, label=m_name)\n",
    "\n",
    "ax.set_ylabel('Accuracies (%)')\n",
    "ax.set_ylim([0, 100])\n",
    "ax.set_title('Target Prediction by Session and Model')\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(data_types)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(str(datadir / ('Acc_Bars.png')))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "03_02_CNN_faces_houses.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

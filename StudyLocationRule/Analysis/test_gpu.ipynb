{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "from indl.display import turbo_cmap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from indl.fileio import from_neuropype_h5\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "\n",
    "if Path.cwd().stem == 'Analysis':\n",
    "    os.chdir(Path.cwd().parent.parent)\n",
    "    \n",
    "    \n",
    "data_path = Path.cwd() / 'StudyLocationRule'/ 'Data' / 'Preprocessed'\n",
    "if not (data_path).is_dir():\n",
    "    !kaggle datasets download --unzip --path {str(data_path)} cboulay/macaque-8a-spikes-rates-and-saccades\n",
    "    print(\"Finished downloading and extracting data.\")\n",
    "else:\n",
    "    print(\"Data directory found. Skipping download.\")\n",
    "    \n",
    "from misc.misc import sess_infos, load_macaque_pfc, dec_from_enc\n",
    "\n",
    "load_kwargs = {\n",
    "    'valid_outcomes': (0,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_error = {\n",
    "    'valid_outcomes': (9,),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "load_kwargs_all = {\n",
    "    'valid_outcomes': (0,9),  # Use (0, 9) to include trials with incorrect behaviour\n",
    "    'zscore': True,\n",
    "    'dprime_range': (1.0, np.inf),  # Use (-np.inf, np.inf) to include all trials.\n",
    "    'time_range': (-np.inf, 1.45),\n",
    "    'verbose': False,\n",
    "    'y_type': 'sacClass',\n",
    "    'samples_last': True    \n",
    "    #     'resample_X': 20\n",
    "}\n",
    "\n",
    "model_kwargs = dict(\n",
    "    filt=8,\n",
    "    kernLength=20,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=0,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "model_kwargs1 = dict(\n",
    "    filt=16,\n",
    "    kernLength=30,\n",
    "    ds_rate=5,\n",
    "    n_rnn=64,\n",
    "    n_rnn2=64,\n",
    "    dropoutRate=0.40,\n",
    "    activation='relu',\n",
    "    l1_reg=0.0000, l2_reg=0.001,\n",
    "    norm_rate=0.25,\n",
    "    latent_dim=64\n",
    ")\n",
    "\n",
    "N_SPLITS = 10\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "LABEL_SMOOTHING = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess_ix = 1\n",
    "sess_info = sess_infos[test_sess_ix]\n",
    "sess_id = sess_info['exp_code']\n",
    "print(f\"\\nImporting session {sess_id}\")\n",
    "X_rates, Y, ax_info = load_macaque_pfc(data_path, sess_id, x_chunk='spikerates', **load_kwargs)\n",
    "Y_class = tf.keras.utils.to_categorical(Y, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = 8\n",
    "n_timesteps, n_features = X_rates.shape[1], X_rates.shape[2]\n",
    "\n",
    "print('input_shape', n_timesteps, n_features)\n",
    "print('output', number_of_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, batch_input_shape=(BATCH_SIZE,n_timesteps, n_features), return_sequences=True))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(number_of_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Created model.\")\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Compiled model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = int(0.8*X_rates.shape[0])\n",
    "\n",
    "_y = Y_class\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((X_rates[:trn], _y[:trn]))\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices((X_rates[trn:], _y[trn:]))\n",
    "\n",
    "# cast data types to GPU-friendly types.\n",
    "ds_train = ds_train.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "ds_valid = ds_valid.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.uint8)))\n",
    "\n",
    "# TODO: augmentations (random slicing?)\n",
    "\n",
    "ds_train = ds_train.shuffle(trn + 1)\n",
    "ds_train = ds_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "ds_valid = ds_valid.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=ds_train, epochs=EPOCHS,validation_data=ds_valid, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
